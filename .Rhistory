library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(elastic, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(elastic, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(elastic, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)
cv1  <- cv.glmnet(x, y, foldid = foldid, alpha = 1)
cv.5 <- cv.glmnet(x, y, foldid = foldid, alpha = 0.5)
cv0  <- cv.glmnet(x, y, foldid = foldid, alpha = 0)
par(mfrow = c(2,2))
plot(cv1); plot(cv.5); plot(cv0)
plot(log(cv1$lambda)   , cv1$cvm , pch = 19, col = "red",
xlab = "log(Lambda)", ylab = cv1$name)
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "grey")
points(log(cv0$lambda) , cv0$cvm , pch = 19, col = "blue")
legend("topleft", legend = c("alpha= 1", "alpha= .5", "alpha 0"),
pch = 19, col = c("red","grey","blue"))
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#cv <- cv.glmnet(x, y, alpha = alphas[i])
#cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
'''
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
'''
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)
cv1  <- cv.glmnet(x, y, foldid = foldid, alpha = 1)
cv.5 <- cv.glmnet(x, y, foldid = foldid, alpha = 0.5)
cv0  <- cv.glmnet(x, y, foldid = foldid, alpha = 0)
par(mfrow = c(2,2))
plot(cv1); plot(cv.5); plot(cv0)
plot(log(cv1$lambda)   , cv1$cvm , pch = 19, col = "red",
xlab = "log(Lambda)", ylab = cv1$name)
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "grey")
points(log(cv0$lambda) , cv0$cvm , pch = 19, col = "blue")
legend("topleft", legend = c("alpha= 1", "alpha= .5", "alpha 0"),
pch = 19, col = c("red","grey","blue"))
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#cv <- cv.glmnet(x, y, alpha = alphas[i])
#cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
# Ridge regression (alpha = 0)
#ridge <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 0
#)
# Lasso regression (alpha = 1)
#lasso <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 1
#)
# Elastic (alpha = modelled)
#elastic <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = best_alpha
#)
# plot results
#par(mfrow = c(1, 3))
#plot(ridge, main = "Ridge penalty\n\n")
#plot(lasso, main = "Lasso penalty\n\n")
#plot(elastic, main = "Elastic penalty\n\n")
#vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
set.seed(2025)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
# Alphas 0-1
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i], foldid = foldid)
cv_errors[i] <- min(cv$cvm)
}
plot(alphas, cv_errors, type = "b", xlab = "Alpha", ylab = "CV Error")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#cv <- cv.glmnet(x, y, alpha = alphas[i])
#cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
# Ridge regression (alpha = 0)
#ridge <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 0
#)
# Lasso regression (alpha = 1)
#lasso <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 1
#)
# Elastic (alpha = modelled)
#elastic <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = best_alpha
#)
# plot results
#par(mfrow = c(1, 3))
#plot(ridge, main = "Ridge penalty\n\n")
#plot(lasso, main = "Lasso penalty\n\n")
#plot(elastic, main = "Elastic penalty\n\n")
#vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
