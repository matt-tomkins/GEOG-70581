# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(elastic, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i])
cv_errors[i] <- min(cv$cvm)
}
# Find best alpha
best_alpha <- alphas[which.min(cv_errors)]
print(best_alpha)
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)
cv1  <- cv.glmnet(x, y, foldid = foldid, alpha = 1)
cv.5 <- cv.glmnet(x, y, foldid = foldid, alpha = 0.5)
cv0  <- cv.glmnet(x, y, foldid = foldid, alpha = 0)
par(mfrow = c(2,2))
plot(cv1); plot(cv.5); plot(cv0)
plot(log(cv1$lambda)   , cv1$cvm , pch = 19, col = "red",
xlab = "log(Lambda)", ylab = cv1$name)
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "grey")
points(log(cv0$lambda) , cv0$cvm , pch = 19, col = "blue")
legend("topleft", legend = c("alpha= 1", "alpha= .5", "alpha 0"),
pch = 19, col = c("red","grey","blue"))
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#cv <- cv.glmnet(x, y, alpha = alphas[i])
#cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
'''
# Ridge regression (alpha = 0)
ridge <- cv.glmnet(
x = x,
y = y,
alpha = 0
)
# Lasso regression (alpha = 1)
lasso <- cv.glmnet(
x = x,
y = y,
alpha = 1
)
# Elastic (alpha = modelled)
elastic <- cv.glmnet(
x = x,
y = y,
alpha = best_alpha
)
# plot results
par(mfrow = c(1, 3))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
plot(elastic, main = "Elastic penalty\n\n")
vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
'''
library(glmnet)
library(vip)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)
cv1  <- cv.glmnet(x, y, foldid = foldid, alpha = 1)
cv.5 <- cv.glmnet(x, y, foldid = foldid, alpha = 0.5)
cv0  <- cv.glmnet(x, y, foldid = foldid, alpha = 0)
par(mfrow = c(2,2))
plot(cv1); plot(cv.5); plot(cv0)
plot(log(cv1$lambda)   , cv1$cvm , pch = 19, col = "red",
xlab = "log(Lambda)", ylab = cv1$name)
points(log(cv.5$lambda), cv.5$cvm, pch = 19, col = "grey")
points(log(cv0$lambda) , cv0$cvm , pch = 19, col = "blue")
legend("topleft", legend = c("alpha= 1", "alpha= .5", "alpha 0"),
pch = 19, col = c("red","grey","blue"))
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#cv <- cv.glmnet(x, y, alpha = alphas[i])
#cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
# Ridge regression (alpha = 0)
#ridge <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 0
#)
# Lasso regression (alpha = 1)
#lasso <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 1
#)
# Elastic (alpha = modelled)
#elastic <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = best_alpha
#)
# plot results
#par(mfrow = c(1, 3))
#plot(ridge, main = "Ridge penalty\n\n")
#plot(lasso, main = "Lasso penalty\n\n")
#plot(elastic, main = "Elastic penalty\n\n")
#vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
library(glmnet)
library(vip)
set.seed(2025)
# Independent variables
x <- as.matrix(variables)
y <- training$NO2
# Alphas 0-1
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))
# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)
for (i in seq_along(alphas)) {
cv <- cv.glmnet(x, y, alpha = alphas[i], foldid = foldid)
cv_errors[i] <- min(cv$cvm)
}
plot(alphas, cv_errors, type = "b", xlab = "Alpha", ylab = "CV Error")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#cv <- cv.glmnet(x, y, alpha = alphas[i])
#cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
# Ridge regression (alpha = 0)
#ridge <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 0
#)
# Lasso regression (alpha = 1)
#lasso <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 1
#)
# Elastic (alpha = modelled)
#elastic <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = best_alpha
#)
# plot results
#par(mfrow = c(1, 3))
#plot(ridge, main = "Ridge penalty\n\n")
#plot(lasso, main = "Lasso penalty\n\n")
#plot(elastic, main = "Elastic penalty\n\n")
#vip(lasso, num_features = 20, geom = "point")
#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)
#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))
#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}
# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)
#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)
#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables
#coef(cvfit, s = "lambda.1se")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
# Independent variables (x)
x <- as.matrix(training_df)
# Dependent variable (y)
y <- training$Mg
# A vanilla Lasso regression (alpha = 1)
lasso <- glmnet(
x = x,
y = y,
alpha = 1
)
print(lasso)
# Lasso regression (alpha = 1), with cross-validation (10-folds)
lasso_cv <- cv.glmnet(
x = x,
y = y,
alpha = 1,
n_folds = 10
)
plot(lasso_cv)
# Number of iterations
n_repeats <- 100
# Empty numeric `variable` to store lambda values
lambda_values <- numeric(n_repeats)
# Iterate n_repeat times
for (i in 1:n_repeats) {
# Perform LASSO regression (alpha = 1), with 10-fold cross-validation
lasso_cv <- cv.glmnet(
x = x,
y = y,
alpha = 1, n_folds = 10
)
# Store lambda.1se
lambda_values[i] <- lasso_cv$lambda.1se
}
# Print the median
print(median(lambda_values))
# Final LASSO model, without cross-validation
final_model <- glmnet(x, y, alpha = 1)
# Print model coefficients, using our selected value for λ
coef(final_model, s = median(lambda_values))
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
# Predict Mg values based upon LASSO model, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newdata = testing, s = median(lambda_values))
# Predict Mg values based upon LASSO model, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = testing, s = median(lambda_values))
# Predict Mg values based upon LASSO model, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = testing_df, s = median(lambda_values))
training_df <- training[factors]
testing_df <- testing[factors]
# Predict Mg values based upon LASSO model, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = testing_df, s = median(lambda_values))
# Predict Mg values based upon LASSO model, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = as.matrix(testing_df), s = median(lambda_values))
testing
# Predict Mg values based upon LASSO model and our selected λ, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = as.matrix(testing_df), s = median(lambda_values))
# Predict Mg values based upon LASSO model and our selected λ, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, s = median(lambda_values))
# Predict Mg values based upon LASSO model and our selected λ, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = as.matrix(training_df), s = median(lambda_values))
# Predict Mg values based upon LASSO model and our selected λ, saving to testing dataframe
testing$predicted_Mg <- predict(final_model, newx = x, s = median(lambda_values))
# Predict Mg values based upon LASSO model and our selected λ, saving to testing dataframe
training$predicted_Mg <- predict(final_model, newx = x, s = median(lambda_values))
training
# Predict Mg values based upon LASSO model and our selected λ using 'training' as input
predict(final_model, newx = x, s = median(lambda_values))
# As above, using 'testing' as input
predict(final_model, newx = as.matrix(testing_df), s = median(lambda_values))
# Predict Mg values based upon LASSO model and our selected λ, using 'testing' as input
testing$predicted_Mg <- predict(final_model, newx = as.matrix(testing_df), s = median(lambda_values))
# ggplot of measured vs. modelled (predicted) Mg values
mg_plot <- ggplot(data = testing, aes(x = Mg, y = predicted_Mg)) +
# Adding a linear regression ("lm"), removing standard error bars (se = FALSE)
geom_smooth(method = "lm", se = FALSE, colour="#FF953C") +
# Adds a 1:1 line for comparison
geom_abline(intercept = 0, slope = 1, lty = "dashed") +
# Adds the point data, modifying the shape, size, colour and fill
geom_point(shape = 21, colour = "white", fill = "#5695FF", size = 2.5) +
# Setting the theme and aspect ratio
theme_classic() +
theme(aspect.ratio = 1) +
# Axis limits
scale_x_continuous(limits = c(-10,60)) +
scale_y_continuous(limits = c(-10,60)) +
# Add axis labels and a title
labs(x = Measured~Mg, y = Modelled~Mg,
title = Plot~of~measured~vs.~modelled~Mg~values)
mg_plot
# ggplot of measured vs. modelled (predicted) Mg values
mg_plot <- ggplot(data = testing, aes(x = Mg, y = predicted_Mg)) +
# Adding a linear regression ("lm"), removing standard error bars (se = FALSE)
geom_smooth(method = "lm", se = FALSE, colour="#FF953C") +
# Adds a 1:1 line for comparison
geom_abline(intercept = 0, slope = 1, lty = "dashed") +
# Adds the point data, modifying the shape, size, colour and fill
geom_point(shape = 21, colour = "white", fill = "#5695FF", size = 2.5) +
# Setting the theme and aspect ratio
theme_classic() +
theme(aspect.ratio = 1) +
# Axis limits
scale_x_continuous(limits = c(0,40)) +
scale_y_continuous(limits = c(0,40)) +
# Add axis labels and a title
labs(x = Measured~Mg, y = Modelled~Mg,
title = Plot~of~measured~vs.~modelled~Mg~values)
mg_plot
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs")
