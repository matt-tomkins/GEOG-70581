# Mersey IV - Statistical analysis {#mersey_four}

## Task 5: Model building

In this next task, we will compare the information about catchment characteristics with the water quality data collected at each of the 70 monitoring stations. To begin:

> Load the csv file created at the end of Task 4 (`mersey_watersheds_ea.csv`), saving to a new variable called `watersheds_df`:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Reads completed file from csv
watersheds_df <- read.csv(here("output", "practical_2", "mersey_watersheds_ea.csv"))

```

If you have any other variables in your R environment, these can be removed using `rm()`.

This data frame should contain the following 10 water quality indicators for each watershed:

-   pH: acidity/alkalinity;
-   SSC: suspended solids concentration (mg l<sup>−1</sup>);
-   Ca: calcium (mg l<sup>−1</sup>);
-   Mg: magnesium (mg l<sup>−1</sup>);
-   NH<sub>4</sub>: ammonium (mg-N l<sup>−1</sup>);
-   NO<sub>3</sub>: nitrate (mg-N l<sup>−1</sup>);
-   NO<sub>2</sub>: nitrite (mg-N l<sup>−1</sup>);
-   TON: total oxidised nitrogen (mg-N l<sup>−1</sup>);
-   PO<sub>4</sub>: phosphate (mg-P l<sup>−1</sup>);
-   Zn: zinc (μg l<sup>−1</sup>).

It should also contain the continuous derivatives (e.g. average elevation) and categorical derivatives (e.g. land cover percentage) for each watershed.

**Note**: some of the calculated percentages may not add up to 100%. During the reclassification of the categorical datasets (`land_cover`, `soils`, `bedrock`), we focused on the macro-classes which make up the majority of the catchment and are known to have the greatest impact of river hydrochemistry (e.g. urban areas, farmland). While other land cover categories are found within each watershed (hence $total$ $percent < 100$), these typically account for only a small percentage of the total area and have a limited effect on the river environment. These categories have been excluded to simplify the analysis.

### An introduction to linear models in R

It is now time to examine the relationships between river water quality and catchment metrics. The key model outputs that are ultimately required for the assessment are:

1.  **Regression equations** relating each water quality variable (dependent variable; n = 10) to the *important* catchment characteristics (independent variables; n = 16).

2.  An assessment of **model quality**. The exact metrics / plots / tests are up to you!

> Remember, you **don't** have to run every code block shown below, but you can do so if it would help your understanding. 

> In the following instructions, I will guide you through some of the keys steps for one water quality indicator (`Mg`). You can replicate this approach and add further analysis (e.g., model evaluation) for the other nine indicators.

The simplest way to run a linear regression in R is to use the `lm()` function (i.e., a linear model), an example of which is shown below, storing in an output called `model`:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Fit a linear model (ordinary least squares regression)
model <- lm(formula = Mg ~ average_elevation, data = watersheds_df)
```

We have defined the data frame being used (`data = watersheds_df`) and the input variables from that data frame. This is achieved by including their column names, shown here:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
colnames(watersheds_df)
```

Input variables in the **formula** are separated by `~`, where the variable to the left is the dependent variable (`Mg`) and the variable to the right is an independent variable (`average_elevation`). We can, however, include **multiple** independent variables to perform multiple linear regression. This is achieved as follows, where additional independent variables are separated by `+`:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Fits a linear model
model <- lm(formula = Mg ~ average_elevation + Grassland_percent, data = watersheds_df)
```

We can then assess the model output using the `summary` function:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
summary(model)
```

For this set of independent variables, we have an R<sup>2</sup> of 0.30 (`Multiple R-squared: 0.2964`) and a model *p* value of \< 0.01 (`p-value: 7.672e-06`).

> What is your opinion of the model performance? 

The model coefficients for the independent variables are described above, where `*` denotes *p* values \< 0.05 (95% probability) and `**` denotes *p* values \< 0.01 (99% probability). As the coefficients are very small, they are presented in scientific notation. These can be converted to numeric (non-scientific) format using the following code:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
format(-2.096e-04, scientific = FALSE)
```

We can supply multiple values to the `format` function by creating a vector:

```{r, echo = TRUE, eval = FALSE, warning= FALSE, message = FALSE, fig.align = 'center'}
format(c(-2.096e-04, -8.358e-06, ...) , scientific = FALSE)
```

> When you're happy you understanding the formatting of the `lm` function, move on to the next section.

### Training vs. Testing {#train-test}

One **limitation** of the above approach is that our dataframe (`watersheds_df`) contains observations from all 70 EA monitoring stations.

When performing statistical analysis, it is common practice to split any dataset into:

-   a **training** subset, which is used to create the model(s).
-   a **testing** subset, which is used to evaluate the model(s).

Subsetting our data in this way allows models to be evaluated more rigorously. Many models perform well "in-sample" but poorly "out-of-sample" when evaluated against independent data (i.e. the testing subset). This is commonly referred to as **over-fitting**.

Training and testing subsets are usually defined randomly, with an approximate ratio of 70:30 (although this varies). However, and to ensure reproducibility, this step has been completed for you: the `watersheds_df` dataframe contains a `group` variable denoting which monitoring sites belong to the training and testing subsets.

> Run the code above to create `training` and `testing` dataframes:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Extracts training dataset, comprising 50 observations (~70%)
training <- subset(watersheds_df, Group == "Training")

# Extracts training dataset, comprising 20 observations (~30%)
testing <- subset(watersheds_df, Group == "Testing") 

```

> Before you move on to the next section, can you think of any limitations of this approach?

> Hints: How important is the training-testing ratio? How are training-testing subsets created?

### The full model

An additional **limitation** of the above approach is that we have manually defined the independent variables of interest (`average_elevation + Grassland_percent`). For exploratory analysis, however, we may not know which are the most important variables. Perhaps there is a combination of independent variables which produces a better model fit (e.g. R<sup>2</sup> \> 0.30)?

As a first step, we could create a full model including **all** the *relevant* independent variables. This would include the normalised variables (e.g. `Arable_percent`, `Heath_percent`, ...), but we'd want to exclude the categorical counts (e.g. `Arable`, `Heath`, ... influenced by catchment size), as well as any IDs or geometry variables. In general, we are only interested in testing the continuous derivatives (column names starting with `average_`) and the normalised categorical derivatives (column names ending in `_percent`).

Rather than typing out the columns of interest manually, we are going to use the `select` function from the `dplyr` package to achieve this:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Creates a vector of column names, including only those which contain "average" or "percent"
factors <- colnames(watersheds_df %>% dplyr::select(contains(c("average", "percent"))))

# Prints to console
factors
```

> Run the above code. **Note**, the formatting of `dplyr::select` may be slightly confusing but it is necessary because there is also a `select` function in the `MASS` package. Here, we are telling R to use `select` from `dplyr`.

Using this vector of column names, we are going to create new data frames (called `training_df` and `testing_df`) containing only the independent variables of interest for the `training` dataset:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
training_df <- training[factors]
testing_df <- testing[factors]
```

> Run the above code and use `head()` to inspect the results.

To run a linear regression using these data, we need to ensure our dependent variable of interest is also present in the dataframe, which can be achieved using `cbind`. Note that we have specified `variable` as the name of the column containing the `Mg` values, but this can be changed if you desire:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Column bind (cbind) the dependent (n=1) and independent variables (n=16)
model_df <- cbind(variable = training$Mg, training_df)
```

> Run the above code which creates a new dataframe (`model_df`) containing the dependent variable we want to model (`Mg`) and the full set of independent variables for our training set. 

> Next, run a new `lm()` model using all the independent variables, which can be acheived using the equation `variable ~ .`, and inspect the output.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# OLS, all independent variables
linear_model <- lm(formula = variable ~ ., data = model_df)
summary(linear_model)
```

> **Question**: How has model performance changed?

> Do you think adding all our independent variables to the model is a good approach?  

#### Problems with the full model

Including all the independent variables has improved our model performance on the `training` set.  *However*, there are some important issues we need to consider: 

-   While adding all the independent variables has increased the overall model fit (R^2^), we don't know which variables are actually **important**. Perhaps only one or two of these have a demonstrable and/or high magnitude effect on the dependent variable? In the example above, only one coefficient is marked as significant at $p < 0.05$ (`Arable_percent`). 

-   In general, we prefer models with the minimum number of parameters (independent variables). These models require fewer assumptions, less intensive data collection, can be applied more confidently to new data sets/locations, and are often easier to interpret. This principle of **model parsimony** is based upon [*Occam’s Razor*](https://www.britannica.com/topic/Occams-razor): "other things being equal, simpler explanations are generally better than more complex ones".

-   Another, and perhaps more important issue, is that we not yet assessed the performance of the model on the `testing` data set. Good performance on the training set is a promising sign, but we also need to evaluate how the model performs on unseen and independent data.

To achieve this, we can use the `predict()` function which predicts new values for `Mg` based on our model and using catchment characteristics from the `testing` set (`newdata = testing`):

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Predict Mg values based upon linear model, saving to testing dataframe
testing$predicted_Mg <- predict(linear_model, newdata = testing)
```

> Run the above code, which will create a new column in the `testing` dataframe called `predicted_Mg`, using our model as input (`linear_model`).

> To evaluate the model performance, create a scatter plot of measured `Mg` *vs.* predicted `Mg` values, using the skills developed in the [Introduction to R](@Intro_to_R). I would recommend adding a 1:1 line to the plot, as follows: `geom_abline(intercept = 0, slope = 1, lty = "dashed")`. You could also add a linear regression using the measured and modelled values, although see below for important disucssion. This can be acheived using the following: `geom_smooth(method = "lm", se = FALSE, colour="#FF953C")`.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# ggplot of measured vs. modelled (predicted) Mg values
mg_plot <- ggplot(data = testing, aes(x = Mg, y = predicted_Mg)) +
  # Adding a linear regression ("lm"), removing standard error bars (se = FALSE)
  geom_smooth(method = "lm", se = FALSE, colour="#FF953C") +
  # Adds a 1:1 line for comparison
  geom_abline(intercept = 0, slope = 1, lty = "dashed") +
  # Adds the point data, modifying the shape, size, colour and fill
  geom_point(shape = 21, colour = "white", fill = "#5695FF", size = 2.5) +
  # Setting the theme and aspect ratio
  theme_classic() +
  theme(aspect.ratio = 1) +
  # Axis limits
  scale_x_continuous(limits = c(-30,60)) +
  scale_y_continuous(limits = c(-30,60)) +
  # Add axis labels and a title
  labs(x = Measured~Mg, y = Modelled~Mg, 
       title = Plot~of~measured~vs.~modelled~Mg~values)

mg_plot

```

> Use this plot to assess the model performance on the `testing` set. Can you see evidence of systematic and/or random error?

While this plot allows for visual evaluation of the model, it is also useful to produce metrics for a quantitative evaluation, such as root-mean-square error (RMSE), normalised root-mean-square-error (nRMSE), or R^2^, which you've covered earlier in the unit.  

It is also *tempting* at this stage to produce a regression between the measured and modelled values, such as the following: 

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Regression (measured vs. modelled)
prediction_model <- lm(formula = Mg ~ predicted_Mg, data = testing)
summary(prediction_model)
```

While this looks like a sensible approach, this is actually **incorrect**  because this is evaluating the deviations from the modelled line-of-best-fit (shown in ${\color{#FF953C}{orange}}$ above), and <ins>not</ins> the 1:1 line. An easy mistake! 

To illustrate, below is a plot of some data from Alexander et al. ([2015](https://doi.org/10.1021/acs.jcim.5b00206)), a highly recommended paper on $R^2$. As you can see from the plot, there is a clear correspondence between the observed and predicted values. If we run a linear regression on these data (`lm(formula = observed ~ predicted...`), then $R^2=1$ i.e., a straight line can be drawn *through* the data points. 

This is misleading, however, because the data are far from the 1:1 line. The correct $R^2$ is actually $-1.26$, which is very different!  

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Data from Figure 2 of Alexander et al. (2015)
# Link: https://doi.org/10.1021/acs.jcim.5b00206
predicted <- c(7, 7.50190123, 8.003802731, 8.505703417, 9.001901094, 9.509505876, 10)
observed <- c(5.946969927, 6.371211597, 6.803030282, 7.227272675, 7.659090999, 8.075757822, 8.507575784)
df <- data.frame(predicted, observed)

# Regression (y ~ x)
prediction_model <- lm(formula = observed ~ predicted, data = df)
summary(prediction_model)

# 1 - RSS / TSS
sst <- sum((df$observed - mean(df$observed))^2)
sse <- sum((df$observed - df$predicted)^2)
r_squared <- 1 - (sse / sst)

# ggplot of observed-predicted
test_plot <- ggplot(data = df, aes(x = predicted, y = observed)) +
  # Adding a linear regression ("lm"), removing standard error bars (se = FALSE)
  geom_smooth(method = "lm", se = FALSE, colour="#FF953C") +
  # Adds a 1:1 line for comparison
  geom_abline(intercept = 0, slope = 1, lty = "dashed") +
  # Adds the point data, modifying the shape, size, colour and fill
  geom_point(shape = 21, colour = "white", fill = "#5695FF", size = 2.5) +
  # Setting the theme and aspect ratio
  theme_classic() +
  theme(aspect.ratio = 1) +
  # Axis limits
  scale_x_continuous(limits = c(6,10)) +
  scale_y_continuous(limits = c(6,10)) +
  # Add axis labels and a title
  labs(x = "Predicted", y = "Observed", 
       title = Alexander~et~al.~(2015))
test_plot

```

To produce the **correct** $R^2$, we need to do so manually using the following equation:

$$R^2=1-\frac{RSS}{TSS}$$
Here $RSS$ is the residual sum of squares i.e., deviations of predicted values (e.g., `predicted_Mg`) from actual empirical values (e.g., `Mg`), and can be calculated in R as follows:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Residual sum of squares i.e., the size of the residuals from the model 
rss <- sum((testing$Mg - testing$predicted_Mg)^2)
```

$TSS$ refers to the total sum of squares i.e., the dispersion of the data points from the average value, or as Alexander et al. ([2015](https://doi.org/10.1021/acs.jcim.5b00206)) summarise: "*the size of the residuals for a null model where all predictions are the same, i.e. the mean value.*"

$TSS$ involves:

1. calculating the difference between each measured value and the mean of the measured values, for example using the `mean()` function,
2. squaring the differences, 
3. calculating the total sum of the squared values. 

For example, if these were the measured values and the mean:

$$[5.3, 4.6,6.5,8.5,10.1]$$
$$mean=7.0$$
the **differences** would be as follows:
$$[-1.7,-2.4,-0.5,1.5,3.1]$$
the **squared differences** would be: 

$$[2.89, 5.76, 0.25, 2.25, 9.61]$$
resulting in:

$$TSS = 20.76$$

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Sum of Squares Total
tss <- sum((testing$Mg - mean(testing$Mg))^2)
```

> Write code to calculate $TSS$ using the guidance above, and use the $RSS$ example for assistance. If you get stuck, ask for help! 

The ratio of $RSS$ to $TSS$ accounts for the *unexplained* variation in the dependent variable that is not accounted for by the model. In turn, using $1-\frac{RSS}{TSS}$ returns our *explained* variation ($R^2$), which for the measured-modelled `Mg` data, produces:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Model performance for the testing set
# See: https://en.wikipedia.org/wiki/Coefficient_of_determination#cite_note-8
r_squared <- 1 - (rss / tss)
print(r_squared)
```

This has produced an unexpected **negative** value, similar to the example data from Alexander et al. ([2015](https://doi.org/10.1021/acs.jcim.5b00206)) above. How can we explain this?

Numerically, this has occurred because $RSS > TSS$ i.e., the error (residuals) in the model ($RSS$) are greater than the variation of the observations around their mean ($TSS$). But what does that actually mean? 

When $R^2<0$, this means that the model predictions are worse than simply using the mean of the dependent variable (`Mg`) as a predictor i.e., a horizontal line at the `Mg` mean would have lower residuals than our predictions. Not good! 

If `testing` R^2^ is lower than `training` R^2^, we would describe our models as **overfit** and/or **poorly generalised** i.e., the model performs well on the `training` set, but poorly on the `testing` set. In most cases, there may be some overfitting, but not extreme (e.g., `training` $R^2=0.7$, `testing` $R^2=0.4$). In our example above, we have extreme overfitting ($R^2<0$)

One solution to prevent overfitting is to reduce model complexity i.e., the number of independent variables. While this will generally reduce `training` performance, this can lead to improved performance on independent data (`testing`). Moreover, by reducing the number of independent variables, we can focus on the important processes (model parsimony), can make fewer assumptions, and can more easily apply our model to new situations.

> How can we acheive this?

### Variable selection strategies

Determining which variables to include/exclude from a model is a very difficult problem, which has resulted in many different variable selection strategies. Common approaches include expert opinion and/or theory, stepwise regression, implemented in `MASS`, partial least squares (PLS) regression, implemented in `PLS`, as well as elastic net methods and ridge regression, both implemented in `glmnet`. You may want to explore some of these approaches if you use regression analysis for your dissertation.

We are going to use a very popular and well-respected approach, known as Least Absolute Shrinkage and Selection Operator (**LASSO**), implemented in `glmnet`. Published by Robert Tibshirani in [1996](https://www.jstor.org/stable/2346178), the original work has >65,000 citations! There are lots of [online resources](https://www.youtube.com/watch?v=NGf0voTMlcs&t=196s&ab_channel=StatQuestwithJoshStarmer) available to learn more about LASSO, including the original paper. 

<p align="center">
  <a name="lasso_abstract"></a>
  <img src="figures/Practical-2/lasso.png" width="60%">
</p>

### LASSO in theory 

As explored in the lecture, LASSO is slightly different to ordinary least squares (OLS) regression, which produces a model which minimises the sum of the squared residuals, as shown [below](@fig:ols).

<br/> 

<p align="center">
  <a name="figure_ols"></a>
  <img src="figures/Practical-2/ols-schematic.png" width="60%">
</p>

In **ordinary least square regression**, the line of best fit minimizes the sum of the squared (vertical) residuals between the data points and the line itself

<br/> 

While this approach produces the best fit to the `training` data, this can often result in overfitting, and poor performance on `testing` data, as demonstrated with our analysis of `Mg`.  

Instead, LASSO minimises the sum of the squared residuals, but also adds an additional **penalty** term known as **lambda** (λ), which is applied to the slope coefficient of the model ($λ×slope$). While OLS minimises the sum of the squared residuals, LASSO returns the model with the minimal overall error (sum of the squared residuals + $λ×slope$).

The aim of this approach is to add a small amount of **bias** to our model. While this generally results in a worse fit for the `training` data, this often leads to improved performance on the `testing` data. 

<br/> 

<p align="center">
  <a name="figure_ols"></a>
  <img src="figures/Practical-2/lasso-schematic.png" width="60%">
</p>

In **LASSO**, an additional penalty term (λ) is used to reduce overfitting

<br/> 

This approach is known as *regularisation*, which in effect is reducing the sensitivity of our model to the `training` data. A model that is highly tuned to the `training` data often won't perform well in other situations. In contrast, models containing fewer variables and/or additional bias ($λ×slope$) are often more **generalisable**. 

One key aspect we haven't discussed in depth is **lambda** (λ). This refers to the **size of the penalty** we apply to the slope coefficient i.e., how much should we penalise steeper slope coefficients in our model? Note that the shallower the slope of the model, the less sensitive it is to the `training` data.

- If λ = 0, then no penalty would be applied, and the results of LASSO would be identical to OLS.

- As we increase λ, the slope of the model will get closer and closer to 0 (i.e., the model is very insensitive to the `training` data)

**How then should we choose a suitable lambda for λ?**

### Cross-validation for λ

A common approach to choose λ (and other model parameters) is known as **cross-validation**. There are many different methods that can be used here, such as *k*-fold cross-validation or leave-*one*-out cross-validation (LOOCV), but the underlying approach is similar and is summarised in the figure below. 

<p align="center">
  <a name="figure_cross_validation"></a>
  <img src="figures/Practical-2/cross-validation-schematic.png" width="60%">
</p>

In *k*-fold **cross-validation**, the training set is split into *k*-folds, with one fold left out for each model run. The model performance on this fold is used to evaluate performance, as with the `testing` set.

<br/> 

We've already split our data into [training-testing subsets](@train-test). Cross-validation repeats the process, splitting the training data into different groups (or sets or folds). For example, **10-fold** cross-validation would split the training data into ten folds. The model would then be trained using a dataset comprising nine of the ten folds, with the final fold left out and used for model evaluation. This process would then be repeated, leaving a different fold out for each model run.  When all combinations have been tested, the final model might be the average of the *k* model runs or the best performing one. 

One disadvantage of cross-validation is that the training data will be split **randomly** into different folds. If you run your analysis again, the data within each fold will be different and your model output might also differ! 

One way to account for this is to repeat cross-validation many times (e.g., $×100$ or $×1000$), which generally produces more stable coefficients. This is the approach we'll use in the following sections.

> Before we move on the use LASSO in R, make sure you understand the approaches outlined above. If you can answer the following, move on!

> How does LASSO differ from OLS? 

> What is λ? 

> How does cross-validation work?

### LASSO in practice 

In R, we can run a LASSO regression via `glmnet`, which is described in full [here](https://glmnet.stanford.edu/articles/glmnet.html). The main function, which confusingly is also named `glmnet()`, takes in dependent and independent variables in a similar way to `lm()`, but as vectors or matrixes, rather than data frames, as shown below.

One key input of `glmnet()` is the `alpha` parameter. For our analysis, we are going to keep $alpha = 1$, which denotes that we are performing LASSO regression. The package `glmnet()` is designed to enable a more complex approach known as *elastic net regression*, published by Zou and Hastie ([2005](https://doi.org/10.1111/j.1467-9868.2005.00503.x)). This method incorporates both LASSO and a similar technique known as *ridge regression*. While elastic net is an excellent approach, combining the strengths of both LASSO and ridge, it is much more complex. 

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Independent variables (x)
x <- as.matrix(training_df)

# Dependent variable (y)
y <- training$Mg

# A vanilla Lasso regression (alpha = 1)
lasso <- glmnet(
  x = x,
  y = y,  
  alpha = 1
)

```

> Run the above the code and print the regression results using `print(lasso)`. This output shows a range of potential models, which vary in terms of the number of non-zero coefficients (Df) (i.e., the number of independent variables in the model), the percentage of deviance explained (%dev), and the value of λ i.e., the parameter that controls the strength of the penalty. 

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Print results of glmnet
print(lasso)

```

There are lots of models here, with differing values for λ, so how do we pick one? This is where **cross-validation** comes in, splitting the data into *k*-folds, and repeating the process until we find the best value for λ. 

> Run the code below to run LASSO with cross-validation, using 10-folds. 

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Lasso regression (alpha = 1), with 10-fold cross-validation
lasso_cv <- cv.glmnet(
  x = x,
  y = y,
  alpha = 1, 
  n_folds = 10
)

# Plot results
plot(lasso_cv)

```

You can use `plot()` to visualise the results of cross-validation, which shows how the model performance varies with the number of non-zero coefficients and the value for λ. **Note**, your plot might look different to the one above, but don't panic... Marked with dashed lines are two candidate models that the user might want to select as the final model:

- $λ_{min}$ (`lambda.min`) i.e., the model with the minimum mean cross-validated error (y-axis)
- $λ_{1se}$ (`lambda.1se`) i.e., the model with the largest λ within 1 standard error of $λ_{min}$. 

$λ_{min}$ is therefore the "best" model, based upon the mean cross-validated error. However, a user might prefer to select $λ_{1se}$. As this has a larger penalty value (λ), this will guard better against overfitting, while typically resulting in fewer independent variables. 

This is an improvement, because we now have just two candidate models to select from, either $λ_{min}$ (minimum error, but potential for overfitting), or $λ_{1se}$ (larger error, but more parsimonious). However, there is still an issue to be solved.

> Run `cv.glmnet()` again with the same settings and keep plotting the results. What do you notice? 

If this has worked as expected, your plots and final models should have changed each time. Sometimes this difference might be quite subtle, but occasionally you will obtain a very different result. As discussed above, this is due to the **random splitting** of the data during cross-validation. Our relatively small dataset is quite *sensitive* to this split, whereas we might expect to see less sensitive behaviour for datasets consisting of hundreds or thousands of measurements, or where there are high magnitude or unambigious links between the dependent and independent variables.

To stabilise our results, we will repeat cross-validation 100 times, as shown below. This uses a `for` loop to iterate the chosen number of times, perform cross-validation, and then store the $λ_{1se}$ value for each iteration. I am using $λ_{1se}$ for model parsimony here, but you can explore the effects of using $λ_{min}$ for the assessment. For example, if using $λ_{1se}$ returns no important predictors (except for the intercept), then the model is **underfit**, and $λ_{min}$ might be preferable.

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Number of iterations
n_repeats <- 100

# Empty numeric `variable` to store lambda values 
lambda_values <- numeric(n_repeats)

# Iterate n_repeat times
for (i in 1:n_repeats) {
  
  # Perform LASSO regression (alpha = 1), with 10-fold cross-validation
  lasso_cv <- cv.glmnet(
    x = x,
    y = y,
    alpha = 1, n_folds = 10
  )
  
  # Store lambda.1se
  lambda_values[i] <- lasso_cv$lambda.1se
}

# Median value
m_value <- median(lambda_values)

```

> Run the code above and inspect the results. For example, you could plot of a histogram of the λ values using `hist()` and print out the `median()` λ. 

> Thinking back to earlier in the course, why would using the median λ be a better choice than mean?

The median λ based on my repeated (n=100) 10-fold cross validation is as follows:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Print the median
print(m_value)

```

Your λ value should be identical or very similar. If so, it can be used to produce a final model for this dependent variable:

```{r, echo = TRUE, eval = FALSE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Final LASSO model, without cross-validation
final_model <- glmnet(x, y, alpha = 1)

# Print model coefficients, using our selected value for λ
coef(final_model, s = median(lambda_values))

```

> Run the above code to run the LASSO regression and print out the model coefficients.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Final LASSO model, without cross-validation
final_model <- glmnet(x, y, alpha = 1)

# Print model coefficients, using our selected value for λ
coef(final_model, s = median(lambda_values))

```

Our model coefficients are as follows:

- `intercept` = 9.62080435
- `Arable_percent` = 0.29694731
- `Gleyed_percent` = 0.05816263

Coefficients are important because they are used in **regression equations**, which can then be used for prediction. 

The general format for a regression equation is as follows:

$$
y = a + (b_1 \cdot x_1) + (b_2 \cdot x_2) + (b_n \cdot x_n)
$$
where `a` is the constant (intercept) value, and `b` is the coefficient of x. 

For our `Mg` model above, we can define our regression equation (presented using sensible data precision) as:

$$
Mg = 9.62 + 0.30 \cdot Arable \: percent + 0.06 \cdot Gleyed \: percent
$$
**Well done!** You have now calculated a regression which links the dependent variable (Mg) to the important independent variables, in this case the percentage of arable land cover and gleyed soils. 

> For your assessment, we would like you to **explain** the regression results, linking to hydrological processes and literature. For example, why are $Arable \: percent$  and $Gleyed \: percent$ present in the model?

> Think about specific sources of pollution, transport pathways, types of flow...

## Task 6: Model evaluation

Having created a statistical model, it is necessary to evaluate its performance. Comparison plots of **measured vs. modelled values** are one common way to assess model quality, alongside other metrics such as root-mean-square error (RMSE), normalised root-mean-square-error (nRMSE), Q-Q plots, or histograms of model residuals. You may want to explore some of these for the assessment. 

To predict values based on our LASSO model for the `testing` set, we can use the `predict()` function, taking the model variable (`final_model`) as the input, and using our selected λ value:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', cache = TRUE}

# Predict Mg values based upon LASSO model and our selected λ, using 'testing' as input
testing$predicted_Mg <- predict(final_model, newx = as.matrix(testing_df), s = median(lambda_values))

```

> Run the above code block to predict Mg concentrations in the `testing` dataset, using the LASSO model produced from the `training` dataset. Note that you can also run this process for the `training set` as well e.g., `predict(final_model, newx = x, s = median(lambda_values))`. This allows you to calculate metrics (e.g., R^2^) for both the training and testing set. 

Other metrics include RMSE or nRMSE (plus others introduced earlier in the course) using your own code or additional packages (e.g. `Metrics`);

$$
 RMSE = \sqrt{mean(measured\:values - modelled\:values)^2}
$$

Plots of measured vs. modelled values (as well as Q-Q plots and histograms) can be created in ggplot2. Here is an example:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# ggplot of measured vs. modelled (predicted) Mg values
mg_plot <- ggplot(data = testing, aes(x = Mg, y = predicted_Mg)) +
  # Adding a linear regression ("lm"), removing standard error bars (se = FALSE)
  geom_smooth(method = "lm", se = FALSE, colour="#FF953C") +
  # Adds a 1:1 line for comparison
  geom_abline(intercept = 0, slope = 1, lty = "dashed") +
  # Adds the point data, modifying the shape, size, colour and fill
  geom_point(shape = 21, colour = "white", fill = "#5695FF", size = 2.5) +
  # Setting the theme and aspect ratio
  theme_classic() +
  theme(aspect.ratio = 1) +
  # Axis limits
  scale_x_continuous(limits = c(0,40)) +
  scale_y_continuous(limits = c(0,40)) +
  # Add axis labels and a title
  labs(x = Measured~Mg, y = Modelled~Mg, 
       title = Plot~of~measured~vs.~modelled~Mg~values)

mg_plot

```

> Does the regression line match the 1:1 line? Is there any evidence of under- or over-prediction? Are there any outliers? What **types** of errors can you identify?  

You could also assess this relationship statistically, but be careful to calculate $R^2$ correctly ($1-\frac{RSS}{TSS}$), rather than `lm(formula = Mg ~ predicted_Mg...`. 

> How well does our Mg model perform on the testing dataset, based on the above graphs/statistics? Is out-of-sample performance comparable to in-sample performance?

<br/> 

**To finish the practical** and to prepare for the assessment:

> Replicating the above approaches, calculate regression equations using LASSO for all 10 water quality indicators (NO<sub>2</sub>, pH, SSC, Ca, Mg, NH<sub>4</sub>, NO<sub>3</sub>, TON, PO<sub>4</sub>, Zn). 

> Evaluate the performance of the models. The exact approach is up to you, but I would encourage you to be **ambitious**. Some common approaches includes $R^2$ for the `training` and the `testing` sets, Q-Q plots, histograms of residuals, error metrics (e.g., $RMSE$, $NRMSE$). 

<br/> 

**Advice**:

- when constructing your models, think carefully about both over- and under-fitting. For example, in the `Mg` example above, I used $λ_{1se}$ to favour model parsimony. However, models can be underfit (e.g., no important predictors, except for the intercept), so in some cases $λ_{min}$ might be preferable.

- when evaluating model performance, don't worry if performance is not "good", however you define this. Predicting water quality is challenging and we don't always fully understand the links between catchment characteristics and river hydrochemistry. Moreover, if the models *were* all "good" (e.g., high $R^2$, low $RMSE$), you would have very little to discuss in your report! 

