[["index.html", "Chapter 1 Introduction 1.1 Practical outline 1.2 Objectives 1.3 Schedule", " Hydrological analysis in R Chapter 1 Introduction 1.1 Practical outline In this set of practicals, well be using R, RStudio and WhiteboxTools to perform hydrological analysis. In Chapters 2 and 3, well install and get to grips with the required programs. In Practical 1 (Chapters 4 - 6), which uses data for the Eskdale watershed, well introduce you to some of the key approaches, including DEM pre-processing, calculating flow parameters and comparing flow algorithms. In Practical 2 (Chapters 7 - 11), well utilise these approaches to investigate the water quality of the Mersey Basin. This forms the basis for your assessment. Note: the relevant chapters will be made available as we progress through the semester, hence why Practicals 2 and the assessment information are not currently visible. Practical 2 will be released prior to the Week 10 lecture. 1.2 Objectives After completing these practicals, you will: have gained experience with new tools (e.g. R, RStudio, WhiteboxTools); become familiar with hydrological approaches used to simulate overland flow from digital elevation models (DEMs); utilised statistical analysis to investigate the relationships between water quality indicators and catchment characteristics; improved your understanding of the hydrological processes influencing water quality. 1.3 Schedule These practicals will be completed across Weeks 8 to 12 as follows: Week 8: Lecture on Hydrological and Geomorphological models (~1 hr) Complete Chapters 1 - 3: An introduction to R (~2 hr) If theres time available, begin Chapters 4 - 6: Eskdale practical. Weeks 9: Water Quality Modelling introduction lecture (~1 hr) Complete Chapters 4 - 6: Eskdale practical (~2-3 hr) Weeks 10 - 12: Complete Chapters 7 - 11: Mersey practical (~9 hr) The assessment, based upon Practical 2 (Mersey), is due 06/01/2022 (14:00). "],["Installation.html", "Chapter 2 Installation 2.1 R 2.2 R Studio 2.3 Course materials", " Chapter 2 Installation During this practical you will be using WhiteboxTools, an open-source GIS software package for hydro-geomorphic applications. While it contains much of the functionality found in a standard GIS software package, it also includes novel functionality specific to hydrological analysis. The tools are also computationally efficient (i.e. fast!), taking advantage of parallel processing and other advancements in computing power. Finally, the tools are developed for cross-platform use, so can be accessed through code (e.g. R, Python) or other GUIs (e.g. QGIS, ArcGIS) if required. To run WhiteboxTools functions (WBT), well be using R as a front-end interface. This will allow us to access and run all of the relevant hydrological functions from WBT, while utilising the statistical and visualisation capabilities of R. 2.1 R To Install R on a personal computer: Open an internet browser and go to https://www.r-project.org/. Click the download R link in the middle of the page under Getting Started. Select a CRAN location (a mirror site) and click the corresponding link e.g. the site provided by Imperial College London. Click on the Download R for Windows link, or the relevant link for your operating system (macOS, Linux). Click on base and then Download R [Version Number] for Windows. The current version is 4.1.1. Once downloaded, double click the application file (.exe) and follow the installation instructions. If youre working on a university-managed computer or accessing the course via Citrix, both R and RStudio (described below) can be accessed through the Software Centre. 2.2 R Studio While R is now installed, it is good practice to use an integrated development environment (IDE) to access and run R functions. The most well known IDE for R is RStudio, which includes a console, higher quality graphics, as well as tools for plotting, history, debugging and workspace management. Note: R may be used without RStudio, but RStudio cannot be used without R. To Install RStudio on a personal computer: Go to https://www.rstudio.com/products/rstudio/download/ and click on Download RStudio for Windows. Installers for macOS and other operating systems are available below. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.3 Course materials All the data required for this sets of practicals are available on Blackboard under Course Content  Week 8 to 12. Download the folder, save it to an appropriate location and extract the contents (unzip). The directory structure is as follows: GEOG70581 Data Practical_1 Practical_2 Images Practical_1 Practical_2 Output Practical_1 Practical_2 All the required data files for Practicals 1 and 2 are stored in the associated sub-directory within Data. As we move through the practicals, well be creating a number of additional GIS files and we encourage you to save these within the corresponding sub-directory within Output. Finally, maps and figures will be required for the assessment. These can be exported directly from R and stored in Images. "],["Intro_to_R.html", "Chapter 3 A (brief) introduction to R 3.1 Overview 3.2 Loading R 3.3 Variables 3.4 Data structures 3.5 Scripts 3.6 Loading packages 3.7 Loading data 3.8 Plotting 3.9 Formative task", " Chapter 3 A (brief) introduction to R 3.1 Overview The following chapter provides a basic introduction to coding in R. If you already have experience with coding, perhaps from Spatial Ecology or Understanding GIS, then the following content should be familiar to you. For those of you with little or no experience with code, the content builds from the simple (using R as a calculator) to the complex (loading packages and data, plotting) and will equip you with the skills needed to complete Practicals 1 (Eskdale) and 2 (Mersey) You should aim to work through this Chapter by the end of Week 8, making sure you understand the concepts and code before moving on to the next sub-section. To test your understanding, there is a short formative task (non-assessed) at the end of this chapter here. This covers many of the techniques covered here, as well as unfamiliar ones, so you may have to do some independent research to complete it. Ill post the solution on the Monday after the class (22/11/2021) so you can check your working. 3.1.1 What is R? R was originally focused on statistical data analysis, but has evolved into a general-purpose high level programming language. High level languages are typically less complicated and more user friendly than low level languages (e.g. C, C++, Java), but this comes at the expense of being slightly slower to execute. Importantly, R is free and open-source and its core functionality can be extended using packages. These are available through the Comprehensive R Archive Network (CRAN) and allow the user to perform a range of tasks (e.g. reading and writing files, rendering images), without the requirement to delve into the underlying source code. While there are a myriad of free online resources available to help you learn R, including R for Data Science and YaRrr! The Pirates Guide to R, as well as paid options from Data Camp, this chapter and the subsequent practicals will equip you with all the skills required to complete the course. 3.1.2 Why code? For those of you familiar with desktop GIS (e.g. ArcGIS, QGIS) or other statistical programs (e.g. Excel, SPSS), the transition to using code can be a frightening experience! However, there are a several reasons why it is worth persevering with. First, and with career progression in mind, the ability to code is becoming increasingly important, particularly for those of you who want to progress down GIS or environmental career paths. Getting to grips with R and understanding the fundamentals of coding will equip you to learn new skills and new coding languages which may be required by a future employer (e.g. Python, MATLAB, Julia, Ruby, ). Second, performing your analysis using code ensures that your outputs are reproducible. If you provide someone else with the original data files and the corresponding code i.e. a list of operations carried out by the computer (e.g. read data, modify, plot, perform statistical test, save), they should be able to reproduce your outputs exactly. Third, code is repeatable. In standard GIS software (for example), most functions allow you to perform a single task on a single data file e.g. clipping a digital elevation model to the geometry of a shapefile. By comparison, if that same functionality can be implemented in code, it can applied as easily to 10 data files, or 1000, or 10,000. Finally, code is easily modifiable. Code can be tweaked, adapted, or completely rewritten to produce the desired outputs. In R, a wide range of functions are already available in packages, but you can write your own functions or combine existing functions to suit your analysis. This functionality is often not the case in other software, which are generally more complicated to modify and typically provide Black Box solutions, where we can inspect the inputs and outputs, but typically have limited understanding of the steps in between. Black Box programming 3.1.3 A quick note on the practicals As we progress through this course, make sure to read the content carefully. However, you do not have to run every bit of code in this document, although have a go where you feel it would help your understanding. If I explicitly want you to do something, I will write an instruction that looks like this: This is an instruction that tells you something to either think about, or do. 3.1.4 Dealing with errors Before we load R and take our first steps towards coding excellence, it is worth noting that errors are an inevitable aspect of coding. Irrespective of how good you are, you will spend more time fixing your code (i.e. debugging) than writing it! Luckily, when R encounters an error, it will try and help you out. For example, shown below is some simple R code. Here, we are tying to create a new variable called a, which is the product (+) of variable b and the number 1: a &lt;- b + 1 ## Error in eval(expr, envir, enclos): object &#39;b&#39; not found In this example, the code fails and prints the error message object 'b' not found because the variable b does not exist! We could solve this by defining b beforehand as follows: b &lt;- 3 a &lt;- b + 1 a ## [1] 4 These error messages are important and contain helpful information - do not ignore them! If youve struggled to resolve the problem, you should then Google it (normally by just typing R followed by the error message into Google), e.g. R Error in eval(expr, envir, enclos): object 'b' not found. Another useful source of information is StackOverflow which is a public collection of coding questions and answers. If youve come across a challenging error, theres a high probability that someone else has also encountered the same error, and there may be a solution already available. Understanding, interpreting and fixing error messages is a key programming skill, so read the error messages carefully and use the above resources (Google, StackOverflow). If you need any additional help, staff and TAs will be happy to help during the practicals. 3.2 Loading R Now that we understand what R is, why its useful and what to do when something goes wrong, lets begin. Open RStudio. The RStudio user-interface should resemble the following, although the exact arrangement of the windows may differ from that below: This contains the following primary windows, which are described below: In the console window, commands can be typed and results returned. For example, typing the following command into the console: 1 + 2 produces: ## [1] 3 Copy and paste the above code into the console and press Enter to run. Does it produce the correct result? In the Workspace window, you can see all the objects that you have created in the current R session (Environment tab; currently empty) as well as a record of the commands youve used (History tab; 1 + 2). At its most basic, R can operate as a powerful calculator. We can add +, subtract -, multiply * and divide /, take the exponent ^, calculate the square root sqrt() or the logarithm of a number log10(), or melt our computers by using the factorial() function. Operators can also be combined to produce more complicated mathematical functions. 3.3 Variables One of the most important things to know about in R is the variable or object. In this set of practicals, we use the term variable, which is more common across different programming languages. Variables are containers that you can store values in, and then refer to again later. R creates variables for you automatically, so all that is required is to name it and then use the assignment operator &lt;- to assign a value to it. As a general rule (and because of complicated reasons), use &lt;- for assignment, as shown below, and dont use =: b &lt;- 3 Run the above code. This assigns the value of 3 to a variable named b: You should now see that under the Workspace window, the variable b has been added to the Environment tab: The variable b can now be called again by the user. Run the following in the console. Does it work as expected? b + 5 There are number of benefits to using variables. For example, take the following: a &lt;- 5 b &lt;- 10 result &lt;- (sqrt(a) + log(b))^a + 2*b result # Typing the name of a variable (&#39;result&#39;) prints it in the Console Window ## [1] 1945.905 This is a relatively complicated mathematical formula! It uses a square root sqrt() and logarithm log() function, as well as multiplication * and powers ^. We could replicate this result by substituting each value of a for 5 and each value of b for 10, but this would be time consuming and prone to error, and would be very frustrating if we wanted to re-calculate the result for a = 6 and b = 11! Using variables allows us to use and modify values multiple times within the code. Modify the code above to test for a = 6 and b = 11. Compare your answer with a colleague in class or on Zoom. Did you get the same result? Using variables is also important for repeatability. Lets say we want to calculate the sum of the squares from 1 to 5 i.e. \\(1^2 + 2^2 + 3^2 + 4^2 +5^2\\). We could write this numerically in R as follows: 1^2 + 2^2 + 3^2 + 4^2 + 5^2 This produces the current result: ## [1] 55 However, what if wanted to perform this calculation for all the numbers from 1 to 50 i.e. \\(1^2 + 2^2 + 3^2 + ... + 50^2\\)? What if we wanted to cube each value (i.e. \\(n^3\\)), rather than squaring? Either change would require a great deal of manual editing, with lots of potential for error. By comparison, using variables allows us to loop (or iterate) through a series of calculations. The code below creates a variable called result and loops through a sequence of numbers using seq() to perform the above calculation, where the variable i is updated each iteration. # Initialises a variable called result, with a value of 0 result &lt;- 0 # Loops through a sequence from 1 to 5 in increments of 1 (i.e. 1, 2, 3, 4, 5) for(i in seq(from = 1, to = 5, by = 1)){ # Squares each number (i) and adds to result result &lt;- result + i^2 } # Prints variable &#39;result&#39; to the Console Window result ## [1] 55 Run the above code in the console. Try to modify the code to perform the calculation for 1 - 50; this should equal \\(42925\\) At this stage, dont worry about understanding the syntax of the code above. The important thing to remember is that using variables allows to us to perform more complex and repeatable analyses. 3.3.1 Variable names When defining a variable name, you cant use spaces or characters that are not a-z, A-Z or 0-9. As a general rule, variables in R should be written in snake case (as opposed to upper case or lower case) where all words are in lower case and are separated by underscores (_). All of these are valid variable names in snake case: snake_case a_really_long_snake_case_name snake s Other popular naming conventions include camelCase, UPPER_CASE or hyphen-case (among many others). Pick one and use it consistently. 3.3.2 Variable types Each variable will have a particular type, which specifies what kind of a value it can hold. Whilst this is handled automatically for you by R, it is important that you understand what the different types are. Here are some common examples: # Character i.e. a string of letters, numbers and/or characters a &lt;- &quot;Bill&quot; # Numeric i.e. a number with decimals b &lt;- 3.567 # Integer c &lt;- 3 # Logical i.e. TRUE or FALSE d &lt;- TRUE Run the above code. If successful, your workspace should now include the variables a, b, c, d. Some data types are compatible e.g. # Numeric + integer b + c ## [1] 6.567 But others are not! # Numeric + character b + a ## Error in b + a: non-numeric argument to binary operator Can you understand the error message above? Why does b + a produces an error? 3.4 Data structures In the previous examples, each variable has consisted of a single element. This could be a name (bill), a decimal number (3.567), an integer (3), and so on. However, R can store data in a range of different structures, some of which you will encounter as we progress through the course. 3.4.1 Vectors One of the simplest structures is the vector, which contains multiple elements of the same type. These are typically created using the command c(): # Creating a vector (v) of the numbers 1 - 5 v &lt;- c(1, 2, 3, 4, 5) v ## [1] 1 2 3 4 5 As a vector must have elements of the same type, R will try and coerce elements to the same type: # Creating a mixed vector (m) containing numeric (1,2), character (&quot;Bill&quot;, &quot;Ben&quot;) and logical elements (TRUE) m &lt;- c(1, 2, &quot;Bill&quot;, &quot;Ben&quot;, TRUE) m ## [1] &quot;1&quot; &quot;2&quot; &quot;Bill&quot; &quot;Ben&quot; &quot;TRUE&quot; In the above example, the numeric and logical elements have been coerced to characters, as signified by these elements being enclosed by quotation marks \" \". We can also create vectors programmatically, rather than manually defining each element. For example, we can generate sequences using seq() or repetitive sequences using rep(): # Creating a sequence from 1 to 5, in increments of 1 s &lt;- seq(from = 1, to = 5, by = 1) s ## [1] 1 2 3 4 5 # Creating a vector of length 5, consisting of the value 10 r &lt;- rep(10, times = 5) r ## [1] 10 10 10 10 10 3.4.2 Data frames Other common R data structures include matrices, lists and data frames. The data frame is the most commonly used structure for tabular data and will be familiar to those of you with experience in Microsoft Excel. Here is an example data frame, which is created by combining existing vectors of the same length: # Create numeric, character and logical vectors name &lt;- c(&quot;John&quot;, &quot;Eric&quot;, &quot;Michael&quot;, &quot;Graham&quot;, &quot;Terry&quot;, &quot;Terry&quot;) height &lt;- c(1.96, 1.85, 1.78, 1.88, 1.75, 1.73) nationality &lt;- c(&quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;American&quot;, &quot;British&quot;) bereft_of_life &lt;- c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE) # Combines into a data frame df &lt;- data.frame(name, height, nationality, bereft_of_life) df ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 5 Terry 1.75 American FALSE ## 6 Terry 1.73 British TRUE Run the above code in your R console Well be working with data frames throughout the practicals, so it will be useful to know that we can access data frame elements in a variety of different ways. We can access individual columns of a data frame using the $ operator, for example: # Extract the &quot;name&quot; column df$name ## [1] &quot;John&quot; &quot;Eric&quot; &quot;Michael&quot; &quot;Graham&quot; &quot;Terry&quot; &quot;Terry&quot; We can also extract using indexing, which requires us to provide the row and columns indexes in the following format: df[row.index, column.index] # Extract the element at row 3, column 4 df[3,4] ## [1] FALSE # Extract rows 1 to 4, all columns df[1:4,] ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE # Extracts all rows, the second and third columns df[,2:3] ## height nationality ## 1 1.96 British ## 2 1.85 British ## 3 1.78 British ## 4 1.88 British ## 5 1.75 American ## 6 1.73 British We can also extract rows and columns based on the values within the data frame, for example using the subset() function. This can be used to select and exclude variables and observations as follows: # Extract data frame rows where height is more than or equal to (&gt;=) 1.8 m. subset(df, height &gt;= 1.8) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 4 Graham 1.88 British TRUE # Extract data frames rows where nationality is not equal to (!=) American subset(df, nationality != &quot;American&quot;) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 6 Terry 1.73 British TRUE # Extract data frames rows where bereft_of_life is equal to (==) FALSE subset(df, bereft_of_life == FALSE) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 5 Terry 1.75 American FALSE Using the above code as an example, extract all the rows where name is equal to Terry. Assign the result to a new variable. You can calculate the number of rows using the function nrow(). Did you get the following answer? ## [1] 2 The sections above have given you a crash course in some of the fundamentals of R. Learning to code (or learning a new code syntax for those of your familiar with Python, MATLAB, Java, ) is similar to learning a new language. Dont worry if it seems complicated! You can always refer back to these instructions if you get stuck, or ask for guidance. To finish this Chapter, well provide a brief overview of some other key skills, including writing R scripts, loading data and packages, and plotting. This should prepare you for the formative task This is not an exhaustive list of R functions, and well cover other functionality in Practicals 1 and 2 (e.g. loading GIS raster and vector data, creating maps and figures, performing statistical tests). However, the R functionality outlined here will cover all the steps required for the assessment and should equip you with the core skills required to progress further with R in the future. 3.5 Scripts So far weve been typing commands into the console. This works as expected; we can print results, store variables in the workspace and create plots in the plots window. However, it is often more convenient to store our code in a script, which is simply a file containing a set of commands and comments. For example, a single R script could contain commands to load data, perform statistical analysis, and output the results to a new file. This can be run in a single keystroke, which is much simpler than typing each command in one after another. To create a new R script: Navigate to File, New File and R Script. This should open a new window, which is an untitled R script: To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. learning_R) One of the easiest ways to run an entire script is to use Ctrl + Shift + Enter. However, you can also run aspects of a script by clicking on the individual line you want to run and using Ctrl + Enter. The same command applies if you highlight (select) multiple lines of code. 3.5.1 Comments When writing code, it is good practice to include comments which describe what the code does. As your code becomes more complex and as time passes, it is very easy to forget what your code does! Commenting your code is especially important if your code is being used by other people or even by yourself weeks/months/years in the future; save yourself time and effort by describing your code fully as you write it. Comments can be written in R using the # symbol. Any line of code which begins with # is ignored by R when the script is run and are used solely to improve the readability of the code. To add a comment, insert the # symbol and write some text. Below is some uncommented code. Copy and paste into your script and add some comments to describe what is taking place. a &lt;- seq(10, 100, 10) b &lt;- a/10 c &lt;- b*a plot(c, a) The above code is our first encounter with plot() which (as you may have guessed), is used to plot variables in base R.1 The new plot youve created should now appear in the plots window and should resemble the following figure. There are lots of more exciting ways to plot data in R, some of which are described below. As a general rule, well-presented code would typically comprise: 1/3 code; 1/3 empty space; 1/3 comments. This is known as the Rule of Thirds. 3.6 Loading packages Now that we have our script set up, we need to understand is how to install and load packages in the R environment. To begin, remove all the previous code from your script learning_R.R and save. For the remainder of this chapter, we are going to use just one package as an example, but more will be required for Practicals 1 and 2, as detailed here. The package we are going to use is ggplot2, which is a widely used visualization package for R. If theres a figure, map or graphic you want to make, its likely that ggplot2 can do it for you. Its now even used by the BBC! BBC graphics created using ggplot2. See here for details. We can install the package using the helpfully named function install.packages() as follows: # Function to install packages. Name of package enclosed by quotation marks. install.packages(&quot;ggplot2&quot;) This function installs the package to your computer, but before we can use it, it needs to be activated within your current R session. This can be achieved by adding it to the library() as follows: # Function to add package to the library (no quotation marks) library(ggplot2) Copy and paste the two code snippets above into your script and run to install and load ggplot2. This is the standard approach to installing and loading packages in R. However, one of the weaknesses of this approach is that R will try and install and load each package every time the script is run. This is unnecessary; the package is already loaded! Try to re-install ggplot2 using the code above. The following pop-up should appear: Press No. The following message will appear in the console: Warning in install.packages : package ggplot2 is in use and will not be installed We can address this issue by using a user-defined function (called check.packages) to check whether a package is already installed. If it is installed, R will skip the installation. If it is not installed, R will install as normal. This function is similar to the functions weve been using before, such as sqrt() or seq(), which are available by default in base R; the difference is its been written from scratch! # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) Copy and paste the above code at the start of your R run script, removing the previous install.packages() and library() commands. Re-run the code. In future, you can add new packages by simply adding the package name to the packages vector. This works in exactly the same way as described previously, with the vector created using the c() command. For example, we could install packages as follows: packages &lt;- c(\"ggplot2\", \"dyplr\", \"raster\", \"sf\"). A full list of the available packages is here (&gt;18,000 and counting). Finally, packages should typically be installed and loaded at the start of any R script. If your code requires a package to run and it has not yet been installed, the code will fail. Installing and loading all the required packages at the start of the script avoids this problem. 3.7 Loading data In the final two sections of this chapter, we will load some data from a external file and plot using ggplot2. In the previous examples, weve been creating variables, vectors and data frames from scratch, but most data you will work with will be saved in another file format. This could be a comma-separated file (.csv), tab-delimited (.txt) or in an Excel format (.xls), or could be a spatial data type, such as a raster (.tif, .asc, .img) or vector (.shp). Being able to load and manipulate different data types is a key programming skill. Well use one of the most well-used formats as an example; the comma-separated file (.csv). This consists of rows and columns of data, where columns are delineated by commas. For example, here is an example of some comma-separated data: name,age,height,year_of_birth,instrument stewart,69,1.87,1952,drums andy,78,1.70,1942,guitar gordon,69,1.81,1951,vocals When read by R, it is interpreted as follows, splitting columns based on the occurrence of commas, and using the first row as column names: name age height year_of_birth instrument stewart 69 1.87 1952 drums andy 78 1.70 1942 guitar gordon 69 1.81 1951 vocals To load a .csv file into R, we can use the command read.csv(). The most basic way to load a file is to write the absolute file path. This is the location on your computer where the file is saved. For example, a file saved on your C drive, within your Documents folder, and within a folder called Manchester would be formatted as: C:\\Documents\\Manchester To determine the absolute file path, navigate in your file explorer (e.g. Windows Explorer, finder on macOS) to the data sub-directory within GEOG70581: For the final two sections of this Chapter (and the formative task), well using a data file called flow_data.csv which contains the average daily river flow (m3 per second) from the National River Flow Archive and the UK Centre for Ecology &amp; Hydrology. The data are from monitoring station 90003 on the River Nevis, which drains Ben Nevis, the highest mountain in the UK, and cover the period from the 1st October 1982 to the 30th September 2019. You can explore the Glen Nevis area below: The file path to your file might read as follows (if your name was Bill): C:\\Users\\Bill\\GEOG-70581\\data\\flow_data.csv To load this file into R: Copy and paste this file path into your script, enclosing the file path within quotation marks. Importantly, R uses forward slashes within file paths /, whereas the file path in your file explorer probably uses backslashes \\, so these will need to be swapped. # Load data df &lt;- read.csv(&quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot;) If this has worked successfully, a new variable df will have been loaded into the workspace: As you may have noticed, this is not the most elegant way to load data, as it requires us to copy, paste and edit the full file path each time. This would quickly become tiring if we wanted to load multiple data sets. In Practical 1, well guide you through a solution using R projects and the here package but as ever, it is important to understand the basics before moving on to more sophisticated solutions. We can use a variety of commands to inspect this new data set, including: head(df) colnames(df) str(df) Run the above code. What do each of the commands do? head(), colnames(), str() What data types do we have in each column? See here for a reminder. What data structure are we working with? As outlined previously, we can access individual columns of a data frame using the $ operator and could start to do some simple analysis using base R functions, such as: # What is the average daily flow? mean(df$flow) # What is the maximum daily flow? max(df$flow) # What is the minimum daily flow? min(df$flow) Question: What is the range of daily flow measurements? As outlined previously, we can also subset our data based on the values within the data frame, based on columns names ($) or using indexing. To simplify our subsequent analysis, were just going to work with data collected in 2019, although well ask you to extract a different year for the formative task. The data frame rows which correspond to 2019 are from the 1st January [13242] to the 30th September [13514]. Extract these rows and all columns using indexing; see here for a reminder if you get stuck. You may also have noticed from the str() command that our column of dates (day/month/year) were stored as characters e.g. $ date: chr \"01/10/1982\". Working with dates and times is a complicated task (see this Tom Scott video for a light introduction to the problem), so to simplify plotting, well convert these characters e.g. \"01/10/1982\" into a date format. This can be achieved using the following code, which overwrites the date column in the data frame: df$date &lt;- and uses the function as.Date() to define a date format. In this case, our numbers are listed in day, month and then year, which is formatted in R as \"%d/%m/%Y\": # Converts dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) Once youve extracted the flow data corresponding to 2019, run the above code. 3.8 Plotting In the final task, were going to plot these data using ggplot2. One of the challenges of ggplot2 is its steep learning curve, but well walk through this example to show how we can progress from the raw data to high quality figures. These approaches will be useful for your assessment. To finish this chapter, copy and paste the following code elements into your R script in turn and run them. Make sure to read and understand what each step is doing. To begin, were going to create an empty ggplot canvas i.e. the blank canvas upon which we can add ggplot elements, such as axes, legends, data, scale bars etc. # Initialising an empty canvas ggplot() Next, we specify which data set we are going to plot. Note: ggplot2 works almost exclusively with data frames: # Which data frame do we want to use? ggplot(data = df) With the data frame defined, we now need to specify which columns of date we wish to plot. The code below uses the function mapping=aes()to do so, where the date variable is assigned to the x-axis x=date and the flow rate variable is assigned to the y-axis y=flow using their column names: # Which data frame columns do we want to plot? ggplot(data = df, mapping=aes(x=date, y=flow)) However, our data are still not visible because we need to select a geom type, which is a complicated way of saying the type of geometric object we want to plot. We could plot our data as points, lines or polygons; we could choose a histogram, bar plot, or box and whiskers plot; there are many to choose from! In this example, we are going to plot our data as points, which is done using the geom_point() command. As you can see from the code below, the structure differs slightly from normal code, in that ggplot elements are separated by the + operator. This allows the user to add each ggplot element on a separate line of code, thereby improving readability. # What type of geom do we want? ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() In the same way, we could also plot the same data as a line, using geom_line(). In this example, both geoms are using the same data frame as inputs x=date, y=flow. This is because the data frame and columns have been specified in the initial call to ggplot here: data = df, mapping=aes(x=date, y=flow). All subsequent geom objects (e.g. points, lines, bar plots) will these inputs, unless an alternative is specified. # Adding a line geom ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() + geom_line() The above code covers the basics of ggplot, but we can start to improve the design of our plots. For example, we can choose one of the available themes. We could re-order our geoms to show the data points above the line, while adding some colour using HEX codes. We could change the aspect ratio to produce a square plot and maybe add some axis labels: # Specifying data frame and columns ggplot(data = df, mapping=aes(x=date, y=flow)) + # Specifying colours geom_line(colour = &quot;#56ACFF&quot;) + geom_point(colour = &quot;#767676&quot;) + # Adding a ggplot theme theme_classic() + # Setting the aspect ratio theme(aspect.ratio = 1) + # Updating axis labels (including superscript elements) labs(x = &quot;Date (January - October 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) Using the above plot, we could start to identify periods of above or below average flow. As a final step, we will probably want to export our figure so that it can be used elsewhere e.g. in the assessment. To do so, we simply assign our ggplot object to a variable using the assignment operator: &lt;- as normal: # Assign our ggplot object to the variable &quot;g&quot; g &lt;- ggplot(data = df, mapping=aes(x=date, y=flow)) + ... Finally, we can save our ggplot object using the ggsave() function, as shown below. This takes in the name of the plot (g), as well as the intended name, extension and file path e.g. \"C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png\". In this example, we are saving our output within the images sub-directory of GEOG-70581. We have also defined the name of the file flow_data_2019 and specified its file type: .png (portable network graphics file). We can also modify other settings, such as the dpi which determines the plot resolution. A list of ggsave options can be found here. # Saves to a png ggsave(plot = g, &quot;C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png&quot;, dpi = 150) ## Saving 7 x 5 in image Run the above code to assign your plot to the variable g and export it to your images folder, making sure to update the file path! We have now exported our plot and should be able to see it in the images sub-directory of GEOG-70581. Importantly, and for future plots, you dont need to run the commands in the sequence shown above e.g. first creating a canvas ggplot(), then defining a data frame ggplot(data = df), and so on. This sequence was used to show you how we can add elements to the ggplot object, building from the blank canvas to exporting the final figure. 3.9 Formative task Congratulations! You have now completed the guided portion of Chapter 3: the (not-so) brief introduction to R. While this may have been challenging, we have covered most of the core skills which you will need. You should now have an appreciation of the the different data types and structures that are available in R, experience running code in the console and within scripts, and the ability to install packages, load data and create plots. Well use many of these techniques in Practicals 1 and 2. To finish the practical, please complete the formative task outlined below. Create an R Script which does the following: Installs the required packages. Loads the flow data for Glen Nevis. Extracts all rows corresponding to the years 2000 to 2019, making sure to convert to date format. Finds the average daily river flow (m3 per second) for each month and plot the result. Questions: on average, which month had the highest average daily flow and which had the lowest? Hints and Tips: For general guidance on using ggplot2, see here. To extract the month from each record, you may want to use the months() function from base R, while aggregate() (described here) can be used to compute summary statistics (i.e. the mean) based on groups. 3.9.1 Formative solution A code solution to the formative task will be uploaded here on the Monday after the class (22/11/2021). # Solution to the Chapter 3 formative task # Author: matt-tomkins # Date of upload: 22-11-2021 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) ## ggplot2 ## TRUE # Load data from csv (personal file path - update to your own) df &lt;- read.csv(&quot;C:/Users/44797/Desktop/Repo/GEOG-70581/data/flow_data.csv&quot;) # Convert dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) # Extracts post-2000 river flow df_post2000 &lt;- subset(df, date &gt;= as.Date(&quot;2000-01-01&quot;)) # Create a new column, corresponding to the month of measurement, using the months() function df_post2000$month &lt;- months(df_post2000$date) # Summarise by month, using the aggregate() function and &#39;mean&#39; summary &lt;- aggregate(flow ~ month, df_post2000, mean) # Determines plotting order by converting the month column to a &#39;factor&#39; # The factor levels are determined by the base R month.name function summary$month &lt;- factor(summary$month, levels = month.name) # Plotting using ggplot2 g &lt;- ggplot(data = summary, mapping=aes(x=month, y=flow)) + # Stat = &quot;identity&quot; is used when the values of x and y are known geom_bar(fill = &quot;#3EBBFB&quot;, stat=&quot;identity&quot;) + # Sets the theme theme_classic() + # Add x and y-axis labels labs(x = &quot;Month (2000 - 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) + # Tight fitting y-axis scale_y_continuous(expand = c(0,0)) + # Adjusts angle of x-axis labels theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # Displays figure g # Prints highest and lowest average daily flows print(paste0(&quot;The month with the highest average daily flow is &quot;, summary$month[summary$flow == max(summary$flow)])) ## [1] &quot;The month with the highest average daily flow is January&quot; print(paste0(&quot;The month with the lowest average daily flow is &quot;, summary$month[summary$flow == min(summary$flow)])) ## [1] &quot;The month with the lowest average daily flow is July&quot; Base R is used to refer to the functions that are part of the core packages deployed with R, such as plot(), sqrt() or seq(). As we install and utilise user-created packages, we are moving away from base R. "],["FirstPractical.html", "Chapter 4 Eskdale I - Introduction 4.1 Overview", " Chapter 4 Eskdale I - Introduction It should be evident from the lectures that digital elevation models (DEMs) have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions that (1) we can trace the path that water will take from any point in an elevation grid to a basin outlet, and (2) the discharge of water from a particular location is directly related to its catchment area. This is a fairly reasonable assumption in most drainage basins. 4.1 Overview In this lab exercise, we will use a DEM of the Upper Eskdale catchment to model flow-related phenomena, and will have gained experience in: DEM pre-processing; Calculating flow parameters (e.g. pointers and contributing areas); Comparing flow algorithms. Make sure to save your R script regularly! Upper Eskdale catchment 4.1.1 Intended learning outcomes In Chapter 5, well download the data and install the required programs and packages, and will combine these in Chapter 6 to: gain experience with WhiteboxTools, a specialist GIS specifically designed for digital terrain analysis (DTA); become familiar with some of the most commonly used flow algorithms used to simulate overland flow from digital elevation models (DEMs); consider the utility of DTA for routing applications; 4.1.2 Assessment This practical is formative and as such will not count towards your final mark for GEOG 70581, but you are encouraged to complete the practical to the best of your ability as the skills developed here will be used throughout the assessed Practical 2. "],["Eskdale_set_up.html", "Chapter 5 Eskdale II - Set-up 5.1 Install programs 5.2 Download data 5.3 Open RStudio 5.4 Initialise an R project 5.5 Creating an R script 5.6 Install WhiteboxTools 5.7 Loading packages", " Chapter 5 Eskdale II - Set-up 5.1 Install programs If you havent installed R and RStudio, please refer to the instructions here before continuing. 5.2 Download data If youve not already done so, the data for this practical can be downloaded here. The directory structure is outlined in Chapter 2. 5.3 Open RStudio To begin, open RStudio. 5.4 Initialise an R project In the previous chapter, we typed commands into the console and used R scripts to store commands and comments. For both Practicals 1 (Eskdale) and 2 (Mersey), we are going to expand on these by also utilising R projects. Projects make life simpler by allowing us to access file locations (e.g. for loading and writing data) using relative file paths, rather than using the clunky and prone-to-breaking absolute file paths. Relative file paths work by setting the working directory, which is simply the the default location where R will look for files you want to load and where it will put any files you save. We can do this manually by using the setwd() function, but a simpler approach is to use an R project, which sets the working directory for your automatically. Rather than the complicated absolute file paths: &quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot; which indicates that the file is located in data, which is within GEOG-70581, which is within Documents, which is within Bill, which is within Users, which is on the C drive, we can set the working directory to GEOG-70581 (for example) and then access the file using the here package: here(&quot;data&quot;, &quot;flow_data.csv&quot;) This approach not only makes it easier to access files, but also improves reproducibility. Using projects ensures that our work is self-contained (the project directory contains all the data, scripts and outputs) and portable. Transferring the project directory to another user, to a different sub-folder on your computer, or a different drive, would not cause any code to fail. By comparison, scripts featuring absolute file paths or a manually defined working directory would likely fail in any of the above scenarios. To initialise a new R project: Go to File, New Project, and select Existing Directory. Using the Browse icon, set the project working directory to the folder GEOG70581, which you downloaded and unzipped here. Select Create new project If this has been successful, your console should have been updated to include the path to your project working directory as follows: 5.5 Creating an R script Now that we have created our project for GEOG70581, we are going to use scripts to store the code for each practical. Well create a script for Practical 1 and a separate script for Practical 2. Important: a script can be understood as a set of instructions. As a result, the order of the code is critical. A useful metaphor may be to think of a script as a recipe. For example, a simple cake recipe might require butter, sugar, eggs, flour, milk, vanilla extract and lemon. These could be combined as follows: Pre-heat oven. Whisk butter and sugar. Crack the eggs in one at a time and whisk. Add the lemon zest, vanilla, flour and milk and whisk. Bake in the oven for 30 mins. In code form, it might resemble the following, with a vector including the ingredients and different functions used to combine the ingredients (whisk), turn on the oven (pre_heat) and bake (bake_mixture): # Vector of ingredients ingredients &lt;- c(butter, sugar, eggs, flour, milk, vanilla, lemon) # Pre-heats oven for 20 minutes pre_heat(20) # Create a mixture of butter and sugar using the whisk function, mixture &lt;- whisk(ingredients$butter, ingredients$sugar) # Iterates (loops) over eggs variable for (i in length(ingredients$eggs)){ # Extracts each egg, one at a time current_egg &lt;- ingredients$eggs[i] # Adds current egg to mixture mixture &lt;- whisk(mixture, current_egg) } # Add remaining ingredients mixture &lt;- whisk(mixture, ingredients$flour, ingredients$milk, ingredients$vanilla, ingredients$lemon) # If oven temperature is 180 degrees of more if(oven_temperature &gt;= 180){ # Bake mixture cake &lt;- bake_mixture(30) } From the text and code example above, it should be clear that our recipe (and code) would not work correctly if we tried to bake in the oven before pre-heating it, or added the ingredients in the wrong order. In exactly the same way, it is important that your code is formatted in a logical order (e.g. Function 1, Function 2, Function 3, ). Ensure that your future code reflects the recipe above, where individual functions utilise variables created previously, and dont rely on variables created further down in the code. To create an R script for Practical 1: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-1-Eskdale) This should now resemble the following: 5.6 Install WhiteboxTools For Practical 1, we are going to use the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); Most of these packages can be installed as normal using the install.packages() and library() functions. The exception to this is whitebox, because (for complicated reasons) it is not currently available on the Comprehensive R Archive Network (CRAN). However, it is available on R-Forge. While CRAN is the official package repository for R and generally features release versions of packages, R-Forge provides candidate, beta or work-in-progress packages or packages which dont meet the criteria for inclusion on CRAN. We can install R-forge packages as normal using the install.packages() function, but have to specify the repository (web location) where it is stored, in this case: http://R-Forge.R-project.org. Paste the following code into your R script and run to install whitebox # Installs whitebox from R-Forge install.packages(&quot;whitebox&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) If successful, the console window should update as the installation progresses and should finish by printing the directory where the package is saved e.g. The downloaded source packages are in C:\\file-path-here\\downloaded_packages. One additional step, which is different from normal R package installation, is to use the function whitebox::wbt_init(), which initialises WhiteboxTools: # Initialise WBT whitebox::wbt_init() Performing one-time download of WhiteboxTools binary from https://jblindsay.github.io/ghrg/WhiteboxTools/WhiteboxTools_win_amd64.zip (This could take a few minutes, please be patient...) WhiteboxTools binary is located at: C:/Users/44797/OneDrive/Documents/R/win-library/4.0/whitebox/WBT/whitebox_tools.exe You can now start using whitebox &gt; library(whitebox) &gt; wbt_version() This code checks if a suitable WhiteboxTools executable is present (.exe) and installs if missing. The WhiteboxTools executable contains all the code used for geospatial analysis, which we then access using R via the whitebox R package. This is a slightly different configuration from normal R packages but enables WhiteboxTools to be used cross-platform i.e. WhiteboxTools functionality stored in the executable can be assessed through R, Python, ArcGIS or QGIS etc, depending on the user needs. Run the above code to initialise WhiteboxTools, which should perform a one-time download of the WhiteboxTools binary (executable) When complete, WhiteboxTools has now been successfully installed and initialised and is almost ready to be used. However, to avoid re-installing and re-initialisng WBT every time we run our script, we can either delete the code or comment-out # each line. Remember that R ignores any line of comment that begins with #: # Code to install whitebox from R-forge and initialise # install.packages(&quot;whitebox&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) # whitebox::wbt_init() Either delete or comment-out the previous commands (install.packages(), wbt_init). Ctrl + Shift + C is a useful shortcut for adding or removing comments from multiple lines of highlighted code. 5.7 Loading packages To finish the set-up for Practical 1, we are going to install the remaining required R packages (described above) and load them into the R library. Weve already practiced installing and loading the ggplot2 package in Chapter 2 so you are ready for the following: Referring back to the instructions here, install and load ggplot2, here, raster, sf, ggspatial and whitebox. Hint 1 - weve already installed whitebox, so this can be excluded if youre using install.packages() and the library() commands. Hint 2 - the user-defined function check.packages might be useful here. If the package has been installed and loaded correctly, the console should print the following: ggplot2 here raster sf whitebox ggspatial TRUE TRUE TRUE TRUE TRUE TRUE where the logical value TRUE equates to a successful installation (and vice versa for FALSE). "],["Eskdale_flow_algorithms.html", "Chapter 6 Eskdale III - Flow routing 6.1 DEM pre-processing: flow enforcement 6.2 Flow Parameters: Pointers 6.3 Flow Algorithms 6.4 Practical solution", " Chapter 6 Eskdale III - Flow routing Overland and near-surface water flow can be modelled using DEMs if we assume that surface topography is the sole factor which influences the distribution of water. One very simple model routes all water from a particular grid cell in a DEM to a single neighbouring cell (i.e. water is not partitioned between multiple neighbours). This D8 (8 direction) method sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell. Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). Using this method, water flow is allowed in one of eight possible directions (       ), assuming that water will travel along the steepest downslope path. In turn, the method is sometimes referred to as the steepest descent method. Based on the \\(3 * 3\\) cell neighbourhood shown in Figure 1, flow would be directed from the centre cell (8 m elevation) to the southwest cell (4 m elevation). 6.1 DEM pre-processing: flow enforcement One common issue encountered when performing hydrological analyses is the presence of sinks, which interrupt the drainage network. When sinks are encountered, flow direction is undefined when a grid cell, or group of grid cells, is lower than all neighbouring cells (see Figure 2) When sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. These features are referred to as pits if they are a single cell in size, and depressions if they consist of groups of cells. Figure 2: Schematic of a sink in a DEM. In this \\(5 * 5\\) matrix, water is routed into the top right cell (9 m) and then is routed to the lowest elevation cell at each step (9 m  8 m  7 m  6 m  2 m) using the D8 method (blue cells). However, the drainage network is interrupted by a sink at the 2 m cell (orange), as all neighbouring cells are of higher elevation. Sinks can often be artefacts of the data and should be removed during DEM pre-processing. Pre-processing involves altering the elevations of the DEM in a way that enforces continuous flow-paths. However, it is important to realise that sometimes these digital depressions reflect actual features in the landscape, and should be preserved during flow modelling. This is a particular issue for hydrological analysis of karst environments, where water can be routed into dolines and fractures (see Figure 3). However, for our work, we will assume that all depressions in DEMs are artefacts and are justified in being removed. Figure 3: An example of a true DEM sink. A sinkhole extending into the limestone karst of Twisleton Scar, Yorkshire Dales, with Ingleborough behind (723 m). How should we model water flow in environments with sinkholes, fractures, dolines and extensive sub-surface water flow? Several methods have been developed for removing depressions from DEMs. These methods vary greatly in terms of their sophistication and impact on the DEM. The two most common depression removal methods (Figure 4) are: depression filling, which raises cells within a depression to the elevation of the outlet cell; depression breaching, which digs a trench from a depressions bottom to some point downslope. Figure 4: Schematic of depression filling and breaching in a DEM. Using the same values from Figure 2, original values are modified to allow water to escape the sink (orange). Depression filling has raised the value of the sink (2 m  4 m), while depression breaching has lowered the value of a neighbouring cell (3 m  1 m). In this simplified example, the outputs of these two distinct approaches are identical but care should be taken when working with real world data as they will often produce different results. Not all interruptions to flow routing are caused by depression cells. Often, DEMs contain extensive flat regions (areas of equal elevation). Flat areas interrupt flow routing in the same way as depressions. Cells within a flat region do not have downslope neighbours, and therefore, flow routing is impossible on flat sites without pre-processing. Correction of flow direction on flat sites typically involves finding an outlet cell, forcing flow from cells adjacent to the outlet to the outlet, and continuing backwards in an iterative manner (e.g., Jenson and Domingue (1988)). 6.1.1 Data description The DEM we are working with is centred on the Upper Eskdale catchment; an upland valley which drains the highest mountain in England (Scafell Pike; 978 m), as shown below: Figure 5: Upper Eskdale panorama, viewed from Harter Fell [Location: 54.386907, -3.205004, Elevation: 649 m]. The catchment ranges in elevation from 978 m (Scafell Pike) to ~160 m at the catchment outlet (white circle), and is ringed by numerous summits with elevations in excess of 800 m (white triangles). The catchment has an area of ~15.7 km2 and all water which falls with the catchment ultimately drains to the Irish Sea via the River Esk. You can explore the catchment below: The DEM we are working with was downloaded from EDINA Digimap, has a cell size of 10 m and uses the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Unfortunately, we dont have time in this course to delve into the exciting world of map projections, although these are covered excellently by Dr. Jonny Huck in the Semester 2 course Understanding GIS. However, it is important to know that different map projections have different uses and work more/less effectively in different regions. As we are working within the UK, it makes sense to use a map projection which is tailored to the UK (e.g. BNG) as this minimises different types of map distortion (length, shape, area). When loaded into R using the raster package and plotted using ggplot2 and ggspatial, our DEM can be visualised as follows, where the outlet point (white circle) and summits (black triangles) match those shown in Figure 5. To accentuate areas of relief, the DEM has been combined with a semi-transparent hillshade layer, which is shown below: 6.1.2 Breaching and filling To evaluate the effects of depression breaching and filling, were going to use the whitebox package and the wbt_fill_depressions and wbt_breach_depressions functions. One of the advantages of using whitebox is that functions can be applied to spatial data without having to load files into the R environment (and assign them to variables), which can be an issue when working with large files (e.g. high spatial resolution rasters) or when working on computers with limited available memory. We can call whitebox functions by simply naming the function, specifying the names of the input and output layers, and deciding on parameters (if required). For example, the code to fill depressions, might resemble the following: # Fill depressions wbt_fill_depressions(&quot;name of input file goes here&quot;, &quot;name of output file goes here&quot;) Given the structure of our file directory, this could be written as: # Fill depressions wbt_fill_depressions(&quot;C:/Users/Bill/GEOG-70581/data/practical_1/dem_10m.tif&quot;, &quot;C:/Users/Bill/GEOG-70581/output/practical_1/dem_10m_fill.tif&quot;) Here, weve using absolute file paths to set the input DEM as dem_10m.tif (stored in data/practical_1) and have specified the output DEM as dem_10m_fill.tif. The function wbt_fill_depressions will fill depressions in the input DEM and save the new DEM as dem_10m_fill.tif in output/practical_1. However, because were using an R project we can simplify this further. As well be using the DEM (dem_10m.tif) on multiple occasions, we can store its file path using the here package in the variable dem as follows: # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem Copy and paste the above code into your Practical-1-Eskdale script and run. This should print the absolute file path to dem_10m.tif. With this file path stored, we can run the fill depressions function as follows, using the here package to specify the name and intended location of the output file: # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) Run the above code, which should produce a new raster file in the output/practical_1 folder. Repeat this process to also breach depressions, making sure to update the output file name. The difference between the original DEM and the processed DEM(s) may be very subtle and you probably wont be able to observe the differences directly. As such, it may be useful to identify the specific areas of the raster which have been modified. This can be achieved by using the subtract function to work out the difference between the original DEM and the processed DEM, as follows: # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file Run the above code. To visualise this new raster file, were going to load it into the R environment using the raster and here packages and plot with ggplot2 and ggspatial. The raster can be loaded as follows: # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) It can be plotted as follows: # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g Most of the above should be familiar to you, with a few exceptions: Under scale_fill_continuous(), weve set limits for the fill symbology. This now ranges between the maximum value, calculated using max(values(dem)), and the minimum value that is greater than 0, calculated using minimum_value &lt;- min(values(dem)[which(values(dem) &gt; 0)]). The latter is useful because most of the DEM was unmodified by wbt_fill_depressions so most raster cells contain a value of 0. The layer spatial() is a non-standard geom type and is available through the ggspatial package. This simplifies the process of plotting raster data in ggplot2, and this approach should be replicated for future plotting. Run the above code elements to reproduce the plot above. You can remove limits from the scale_fill_continuous() argument to plot the actual distribution of DEM values. Question: to what degree has the original DEM been modified by the wbt_fill_depressions function? Consider the spatial extent and magnitude of the observed differences. Note: For this catchment, the difference between DEM breaching and filling is relatively minor. Well use the latter (dem_10m_fill.tif) for all subsequent calculations. As outlined above, the areas which are most affected by the depression filling algorithm are partly determined by local relief i.e. relatives differences in elevation. To help visualise this, lets create and plot a slope image for the area, where the values in the raster cells represent the gradient of the slope, rather than their raw elevation. This can be achieved using the wbt_slope function. Using the examples above, run wbt_slope, using the original DEM as the input, saving the output file with a logical name (e.g. dem_10m_slope.tif) and to the correct sub-directory (output/practical_1). Load the data into R and plot using ggplot2. Your figure should resemble the following: Compare the slope and difference images. Hint: you can toggle between plots using the left and right arrows in the plot window. Based on your visual inspection the images, which parts of the landscape appear to have been affected by the depression filling algorithm? Why do you think that this is the case? Before we move on to further analysis, it is worth thinking carefully about the symbology we are using for our raster layers. This is important, because not all colour ramps are created equal! Colour ramps differ in terms of their perceptual uniformity and their accessibility for colour-blind individuals. Save your script before continuing. 6.1.3 Choosing a colour ramp Perceptual uniformity refers to how well a colour ramp represents the underlying data. Some (e.g. rainbow; Hawkins (2015)) can introduce artificial boundaries (Alcalde et al. (2017)) which influence how we assess and then use that data (see here. In the schematic below, there are numerous artificial boundaries in the rainbow colour ramp (e.g. dark blue  light blue, light blue  green, green  yellow, yellow  orange), whereas the monochrome colour ramp smoothly transitions from black  white. The viridis colour ramp we have been using so far is perceptually uniform, but others are not. Inspect Figure 6 below. These show the same slope raster, but one is visualised using rainbow, while the other uses a monochrome (perceptually-uniform) colour ramp. Does this influence how you interpret the data? Figure 6: Perceptual uniformity with rainbow and monochrome colour ramps. In a similar fashion, not all colour ramps are suitable for individuals with various forms of colour-blindness (see Levine (2009) and Geissbuehler and Lasser (2013)), who may have difficulty distinguishing reds from greens, or yellows from blues. Luckily, there are now a range of perceptually uniform and colour-blind friendly colour ramps (e.g. Crameri (2018)), including Scientific Colour Maps 6.0. These can be downloaded and used across a range of GIS programs to suit a range of data types. For now, utilise the colour ramps available in R (i.e. viridis) but bear these issues in mind for future map creation. 6.2 Flow Parameters: Pointers Now that we have a DEM that has had all interruptions to flow removed, we can continue to calculate the data needed to define the drainage network i.e. the topological network describing how each cell in a DEM is connected to all others. One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. Whitebox Tools stores D8 pointers as binary numbers in base-2, as follows: Well explore the reason for this in Practical 2. In our earlier example of the D8 flow algorithm (see Figure 1), the flow direction was to the SW. This has a binary value of \\(2^4 = 16\\). Therefore, in the pointer file, the highlighted dark blue cell would be assigned a value of 16. Several sub-programs in Whitebox Tools use pointer files to calculate other terrain data, including the algorithms used to calculate catchment area (see below). However, sub-programs rarely require a pointer file as an input, as WBT will automatically generate a pointer layer from the input DEM when required, before discarding the file afterwards. While this processing step is not readily apparent to the user, it is important to realise that it occurs, and it is often good practice to generate a pointer file manually. This can be achieved using the wbt_d8_pointer function. Using the understanding gained above, run the wbt_d8_pointer function, using the correct input files and output file name and directory. Load the raster into R and plot. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. While this is an acceptable plot, it is actually slightly misleading, because the scale bar is continuous, despite the fact that our underlying data are discrete i.e. there are only eight possible values, which are listed below: \\(2^0 = 1\\) \\(2^1 = 2\\) \\(2^2 = 4\\) \\(2^3 = 8\\) \\(2^4 = 16\\) \\(2^5 = 32\\) \\(2^6 = 64\\) \\(2^7 = 128\\) Here is the same data but using a discrete scale: which is described here: Question: Can you think of any issues with symbolising pointer data? Save your script before continuing. 6.3 Flow Algorithms Once you have determined how each grid cell in a DEM is connected to its neighbours, it is possible to use this pointer information to calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. Measures of catchment area are often used for stream network extraction and other terrain indices. There are three primary measures of catchment area: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment. Unlike the NUE and UCA, SCA is not affected by the grid resolution of the DEM. To calculate the upslope catchment area for each raster cell, we can use the wbt_d8_flow_accumulation function. Run the above function, using the filled DEM as the input file and the output type (out_type) as specific contributing area (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. dem_10m_flow_accumulation.tif) and to the correct sub-directory (output/practical_1). Load the raster to R and plot, and your output should resemble the following: # Calculates accumulation file wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) Inspect the map output. Question: Which parts of the landscape have low/high values of SCA, respectively? 6.3.1 Comparing flow algorithms So far we have used the most simplistic flow algorithm (D8) to model specific catchment area (SCA). However, there are many flow algorithms available (e.g. Fairfield and Leymarie (1991), Seibert and McGlynn (2007), Grabs et al. (2010)) and each algorithm results in slightly different spatial patterns of catchment area. As flow routing is fundamental to the modelling of surface water hydrology, it is important to familiarise yourself with several commonly used methods, although it can be difficult to decide when or where it is appropriate to use a particular routing algorithm. In this section of the practical, we will compare the output of three flow routing algorithms including: D8: see OCallaghan and Mark (1984) FD8: see Freeman (1991), Quinn et al. (1991) and its modification by Quinn et al. (1995) D-infinity: see Tarboton (1997); Flow algorithms differ in the way in which they: calculate flow direction; model flow divergence, i.e., partition flow between neighbouring downslope cells; handle streams. The D8 algorithm is incapable of modelling flow divergence (or dispersion) because the flow from a particular grid cell is directed to a single downslope neighbour. FD8 (i.e. fractional D8; Freeman (1991), Quinn et al. (1991)) divides flow to each downslope neighbour using a partitioning scheme that is based on the slope angle to neighbouring cells, while a later modification by Quinn et al. (1995) increases the degree of flow convergence from the catchment divide to the channel head. Finally, D-infinity divide flows between a maximum of two downslope neighbours based on the direction of maximum downward slope. Of these, FD8-Quinn (Quinn et al. (1995)) is the only algorithm of the three that explicitly recognises that flow on hillslopes should be divergent while flow along valley bottoms should be convergent. In the final part of the practical, assess the specific catchment areas (SCA) derived from each of the flow algorithms discussed above. 6.3.2 FD8 catchment area To calculate the catchment area using the FD8 algorithm (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)), use the wbt_fd8_flow_accumulation function. Use the wbt_fd8_flow_accumulation function, making sure to select specific contributing area and remembering to log-transform the values. The exponent parameter should be set to 1.1 (exponent = 1.1), as suggested by Freeman (1991). Save with an appropriate name (e.g. dem_10m_flow_accumulation_fd8.tif). Load the raster in R and plot. 6.3.3 D-infinity catchment area To calculate the catchment area using the D-infinity algorithm of Tarboton (1997): Use wbt_d_inf_flow_accumulation. Use the same input parameters as above (i.e. SCA, log-transformed values). Save with an appropriate name (e.g. dem_10m_flow_accumulation_d_inf.tif). Load the raster in R and plot. 6.3.4 Comparing the outputs The output of the algorithms should resemble the following: Figure 7: Flow algorithm comparison. Results from the D8, FD8 and D-Infinity algorithms. Plot created using ggplot2 and the patchwork package (useful for combining plots). When water flows over a hillslope, you would expect there to be some degree of divergence depending on the topography (i.e. some hillslopes are convergent and some are divergent). However, once water becomes channelised, you would no longer expect flow to be divergent. Once water is in a channel it stays there until it reaches a lake or ocean. Branching in channels is rather rare. So lets assume that the pattern of SCA should display some degree of dispersion on hillslopes and no dispersion at all along valley bottoms (i.e. in streams). Flow divergence or dispersion is apparent in SCA images by a fuzzy quality, almost like the SCA has been smudged. You can zoom into individual plots using the Zoom button in the Plot Window or edit the x- and y-axis limits of your plots by adding the following to your ggplot2 call: # Sets x- and y-axis limits using coord_sf() + coord_sf(ylim=c(504063,506727), xlim = c(320705, 322255)) The above code uses British National Grid coordinates (m), rather than decimal degrees. To finish Practical 1, save your script and complete the following: Compare the D8, FD8 and D-Inf outputs. Either cycle between the plots using the plot window or combine them using the patchwork package. This would need to be installed and loaded to the library. Which algorithms do a good job modelling the pattern of contributing area on hillslopes? Can you see signs of flow divergence along streams in any of the SCA images? Which algorithms perform best for modelling the pattern of SCA in fluvial (stream) environments? Do any of the algorithms do a good job for both hillslopes and streams? Congratulations! You have now completed the Practical 1 (Eskdale; Chapters 4 - 6). You now have considerable experience running whitebox functions as well as visualising spatial data using ggplot2. 6.4 Practical solution As with the introduction to R (Chapter 2 - 3), I will provide a code solution to the Eskdale practical (Chapters 4 - 6), outlining the major analytical steps. This will be released prior to the Week 10 class (scheduled for 3/12/2021). # Solution to Practical 1 (Eskdale) # Author: matt-tomkins # Date of upload: xx-xx-xxxx "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
