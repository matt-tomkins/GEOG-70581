[["index.html", "Chapter 1 Introduction 1.1 Practical outline 1.2 Objectives 1.3 Schedule", " Hydrological analysis in R Chapter 1 Introduction 1.1 Practical outline In this set of practicals, we’ll be using R, RStudio and WhiteboxTools to perform hydrological analysis. In Chapters 2 and 3, we’ll install and get to grips with the required programs. In Practical 1 (Chapters 4 - 6), which uses data for the Eskdale watershed, we’ll introduce you to some of the key approaches, including DEM pre-processing, calculating flow parameters and comparing flow algorithms. In Practical 2 (Chapters 7 - 9), we’ll utilise these approaches to investigate the water quality of the Mersey Basin. This forms the basis for your assessment. Note: the relevant chapters will be made available as we progress through the semester. Prior to the semester, only the introductory page will be visible. 1.2 Objectives After completing these practicals, you will: have gained experience with new tools (e.g. R, RStudio, WhiteboxTools); become familiar with hydrological approaches used to simulate overland flow from digital elevation models (DEMs); utilised statistical analysis to investigate the relationships between water quality indicators and catchment characteristics; improved your understanding of the hydrological processes influencing water quality. 1.3 Schedule These practicals will be completed across Weeks 8 to 12 as follows: Week 8: Introductory lecture (~1 hr) Complete Chapters 1 - 3: An introduction to R (~2 hr) Weeks 9: Lecture on Hydrological and Geomorphological models (~1 hr) Complete Chapters 4 - 6: Eskdale practical (~2 hr, plus independent work) Weeks 10 - 12: Lecture on Water Quality Modelling (~1 hr) Complete Chapters 7 - 10: Mersey practical (~6 hr, plus independent work) The assessment, based upon Practical 2 (Mersey), is detailed in full on Canvas here. "],["Installation.html", "Chapter 2 Installation 2.1 R 2.2 R Studio 2.3 Course materials", " Chapter 2 Installation During this practical you will be using WhiteboxTools, an open-source GIS software package for hydro-geomorphic applications. While it contains much of the functionality found in a standard GIS software package, it also includes novel functionality specific to hydrological analysis. The tools are also computationally efficient (i.e. fast!), taking advantage of parallel processing and other advancements in computing power. Finally, the tools are developed for cross-platform use, so can be accessed through code (e.g. R, Python) or other GUIs (e.g. QGIS, ArcGIS) if required. To run WhiteboxTools functions (WBT), we’ll be using R as a front-end interface. This will allow us to access and run all of the relevant hydrological functions from WBT, while utilising the statistical and visualisation capabilities of R. 2.1 R If you’re working on a university-managed computer (in HBS 2.2, HBS 2.88 or Mansfield Cooper 2.01), both R and RStudio (described below) should already be installed or can be accessed through the Software Centre. To Install R on a personal computer: Open an internet browser and go to https://www.r-project.org/. Click the “download R” link in the middle of the page under “Getting Started.” Select a CRAN location (a mirror site) and click the corresponding link e.g. the site provided by Imperial College London. Click on the “Download R for Windows” link, or the relevant link for your operating system (macOS, Linux). Click on ‘base’ and then Download R [Version Number] for Windows. The current version is 4.5.1. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.2 R Studio While R is now installed, it is good practice to use an integrated development environment (IDE) to access and run R functions. The most well known IDE for R is RStudio, which includes a console, higher quality graphics, as well as tools for plotting, history, debugging and workspace management. Note: R may be used without RStudio, but RStudio cannot be used without R. To Install RStudio on a personal computer: Go to https://www.rstudio.com/products/rstudio/download/ and click on Download RStudio for Windows. Installers for macOS and other operating systems are available below. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.3 Course materials All the data required for this sets of practicals are available on Canvas here. Download the folder, save it to an appropriate location and extract the contents (unzip). The directory structure is as follows: GEOG70581 Data Practical_1 Practical_2 Images Practical_1 Practical_2 Output Practical_1 Practical_2 All the required data files for Practicals 1 and 2 are stored in the associated sub-directory within Data. As we move through the practicals, we’ll be creating a number of additional GIS files and we encourage you to save these within the corresponding sub-directory within Output. Finally, maps and figures will be required for the assessment. These can be exported directly from R and stored in Images. "],["Intro_to_R.html", "Chapter 3 A (brief) introduction to R 3.1 Overview 3.2 Loading R 3.3 Objects 3.4 Data structures 3.5 Scripts 3.6 Loading packages 3.7 Loading data 3.8 Plotting 3.9 Formative task", " Chapter 3 A (brief) introduction to R 3.1 Overview The following chapter provides a basic introduction to coding in R. If you already have experience with coding, perhaps from Understanding GIS, then the following content should be familiar to you. For those of you with little or no experience with code, the content builds from the simple (using R as a calculator) to the complex (loading packages and data, plotting) and will equip you with the skills needed to complete Practicals 1 (Eskdale) and 2 (Mersey) You should aim to work through this Chapter by the end of Week 8, making sure you understand the concepts and code before moving on to the next sub-section. To test your understanding, there is a short formative task (non-assessed) at the end of this chapter. This covers many of the techniques covered here, as well as unfamiliar ones, so you may have to do some independent research to complete it. I’ll post the solution after the class so you can check your working. 3.1.1 What is R? R was originally developed in the 1990s for statistical data analysis. While it is still optimized for statistical analysis and data visualization, it has been utilised for a much wider range of applications. As a high-level programming language, it is less complicated and more user friendly than lower-level languages (e.g. C, C++, Java, Rust), but this comes at the expense of being slightly slower to execute. Importantly, R is free and open-source and its core functionality can be extended using packages. These are available through the Comprehensive R Archive Network (CRAN) and allow the user to perform a range of tasks (e.g. reading and writing files, rendering images), without the requirement to delve into the underlying source code. While there are a myriad of free online resources available to help you learn R, including R for Data Science and YaRrr! The Pirate’s Guide to R, as well as paid options from Data Camp, this chapter and the subsequent practicals will equip you with all the skills required to complete the course. 3.1.2 Why code? For those of you familiar with desktop GIS (e.g. ArcGIS, QGIS) or other statistical programs (e.g. Excel, SPSS), the transition to using code can be a frightening experience! However, there are a several reasons why it is worth persevering with. First, and with career progression in mind, the ability to code is becoming increasingly important, particularly for those of you who want to progress down GIS or environmental career paths. Getting to grips with R and understanding the fundamentals of coding will equip you to learn new skills and new coding languages which may be required by a future employer (e.g. Python, MATLAB, Julia, Ruby, …). Second, performing your analysis using code ensures that your outputs are reproducible. If you provide someone else with the original data files and the corresponding code i.e. a list of operations carried out by the computer (e.g. read data, modify, plot, perform statistical test, save), they should be able to reproduce your outputs exactly. Third, code is repeatable. In standard GIS software (for example), most functions allow you to perform a single task on a single data file e.g. clipping a digital elevation model to the geometry of a shapefile. By comparison, if that same functionality can be implemented in code, it can be applied as easily to 10 data files, or 1000, or 10,000. Finally, code is easily modifiable. Code can be tweaked, adapted, or completely rewritten to produce the desired outputs. In R, a wide range of functions are already available in packages, but you can write your own functions or combine existing functions to suit your analysis. This functionality is often not the case in other software, which are generally more complicated to modify and typically provide Black Box solutions, where we can inspect the inputs and outputs, but typically have limited understanding of the steps in between. Black Box programming 3.1.3 A quick note on the practicals As we progress through this course, make sure to read the content carefully. However, you do not have to run every bit of code in this document, although have a go where you feel it would help your understanding. If I explicitly want you to do something, I will write an instruction that looks like this: This is an instruction that tells you something to either think about, or do. 3.1.4 Dealing with errors Before we load R and take our first steps towards coding excellence, it is worth noting that errors are an inevitable aspect of coding. Irrespective of how good you are, you will spend more time fixing your code (i.e. debugging) than writing it! Luckily, when R encounters an error, it will try and help you out. For example, shown below is some simple R code. Here, we are tying to create a new object called a, which is the product (+) of object b and the numeric value 1: a &lt;- b + 1 ## Error: object &#39;b&#39; not found In this example, the code fails and prints the error message object 'b' not found because the object b does not exist! We could solve this by defining b beforehand as follows: b &lt;- 3 a &lt;- b + 1 a ## [1] 4 These error messages are important and contain helpful information - do not ignore them! If you’ve struggled to resolve the problem, you should then Google it (normally by just typing “R” followed by the error message into Google), e.g. R Error in eval(expr, envir, enclos): object 'b' not found. Another useful source of information is StackOverflow which is a public collection of coding questions and answers. If you’ve come across a challenging error, there’s a high probability that someone else has also encountered the same error, and there may be a solution already available. Understanding, interpreting and fixing error messages is a key programming skill, so read the error messages carefully and use the above resources (Google, StackOverflow). If you need any additional help, staff and TAs will be happy to help during the practicals. 3.2 Loading R Now that we understand what R is, why it’s useful and what to do when something goes wrong, let’s begin. Open RStudio. The RStudio user-interface should resemble the following, although the exact arrangement of the windows may differ from that below: This contains the following primary windows, which are described below: In the console window, commands can be typed and results returned. For example, typing the following command into the console: 1 + 2 produces: ## [1] 3 Copy and paste the above code into the console and press Enter to run. Does it produce the correct result? In the Workspace window, you can see all the objects that you have created in the current R session (Environment tab; currently empty) as well as a record of the commands you’ve used (History tab; 1 + 2). At it’s most basic, R can operate as a powerful calculator. We can add +, subtract -, multiply * and divide /, take the exponent ^, calculate the square root sqrt() or the logarithm of a number log10(), or melt our computers by using the factorial() function. Operators can also be combined to produce more complicated mathematical functions. 3.3 Objects One of the most important things to know about in R is the variable or object. In this set of practicals, we use the term “object”, although variable is also used commonly across different programming languages. Objects are containers that you can store values in, and then refer to again later. R creates objects for you automatically, so all that is required is to name it and then use the assignment operator &lt;- to assign a value to it. As a general rule (and for historical and other complicated reasons), use &lt;- for assignment, as shown below, and don’t use =: b &lt;- 3 Run the above code. This assigns the value of “3” to an object named “b”: You should now see that under the Workspace window, the object b has been added to the Environment tab: The object b can now be called again by the user. Run the following in the console. Does it work as expected? b + 5 There are number of benefits to using objects. For example, take the following: a &lt;- 5 b &lt;- 10 result &lt;- (sqrt(a) + log(b))^a + 2*b result # Typing the name of an object (&#39;result&#39;) prints it in the Console Window ## [1] 1945.905 This is a relatively complicated mathematical formula! It uses a square root sqrt() and logarithm log() function, as well as multiplication * and powers ^. We could replicate this result by substituting each value of a for 5 and each value of b for 10, but this would be time consuming and prone to error, and would be very frustrating if we wanted to re-calculate the result for a = 6 and b = 11! Using objects allows us to use and modify values multiple times within the code. Modify the code above to test for a = 6 and b = 11. Compare your answer with a friend in class. Did you get the same result? Using objects is also important for repeatability. Let’s say we want to calculate the sum of the squares from 1 to 5 i.e. \\(1^2 + 2^2 + 3^2 + 4^2 +5^2\\). We could write this numerically in R as follows: 1^2 + 2^2 + 3^2 + 4^2 + 5^2 This produces the correct result: ## [1] 55 However, what if wanted to perform this calculation for all the numbers from 1 to 50 i.e. \\(1^2 + 2^2 + 3^2 + ... + 50^2\\)? What if we wanted to cube each value (i.e. \\(n^3\\)), rather than squaring? Either change would require a great deal of manual editing, with lots of potential for error. By comparison, using objects allows us to loop (or iterate) through a series of calculations. The code below creates a object called result and loops through a sequence of numbers using seq() to perform the above calculation, where the object i is updated each iteration. # Initialises an object called result, with a value of 0 result &lt;- 0 # Loops through a sequence from 1 to 5 in increments of 1 (i.e. 1, 2, 3, 4, 5) for(i in seq(from = 1, to = 5, by = 1)){ # Squares each number (i) and adds to result result &lt;- result + i^2 } # Prints object &#39;result&#39; to the Console Window result ## [1] 55 Run the above code in the console. Try to modify the code to perform the calculation for 1 - 50; this should equal \\(42925\\) At this stage, don’t worry about understanding the syntax of the code above. The important thing to remember is that using objects allows to us to perform more complex and repeatable analyses. 3.3.1 Object names When defining a object name, you can’t use spaces or characters that are not a-z, A-Z or 0-9. As a general rule, objects in R should be written in “snake case” (as opposed to “upper case” or “lower case”) where all words are in lower case and are separated by underscores (_). All of these are valid object names in snake case: snake_case a_really_long_snake_case_name snake s Other popular naming conventions include camelCase, UPPER_CASE or hyphen-case (among many others). Pick one and use it consistently. 3.3.2 Object types Each object will have a particular type, which specifies what kind of a value it can hold. Whilst this is handled automatically for you by R, it is important that you understand what the different types are. Here are some common examples: # Character i.e. a string of letters, numbers and/or characters a &lt;- &quot;Bill&quot; # Numeric i.e. a number with decimals b &lt;- 3.567 # Integer c &lt;- 3 # Logical i.e. TRUE or FALSE d &lt;- TRUE Run the above code. If successful, your workspace should now include the objects a, b, c, d. Some data types are compatible e.g.  # Numeric + integer b + c ## [1] 6.567 But others are not! # Numeric + character b + a ## Error in b + a: non-numeric argument to binary operator Can you understand the error message above? Why does b + a produces an error? 3.4 Data structures In the previous examples, each object has consisted of a single element. This could be a name (bill), a decimal number (3.567), an integer (3), and so on. However, R can store data in a range of different structures, some of which you will encounter as we progress through the course. 3.4.1 Vectors One of the simplest structures is the vector, which contains multiple elements of the same type. These are typically created using the command c(): # Creating a vector (v) of the numbers 1 - 5 v &lt;- c(1, 2, 3, 4, 5) v ## [1] 1 2 3 4 5 As a vector must have elements of the same type, R will try and coerce elements to the same type: # Creating a mixed vector (m) containing numeric (1,2), character (&quot;Bill&quot;, &quot;Ben&quot;) and logical elements (TRUE) m &lt;- c(1, 2, &quot;Bill&quot;, &quot;Ben&quot;, TRUE) m ## [1] &quot;1&quot; &quot;2&quot; &quot;Bill&quot; &quot;Ben&quot; &quot;TRUE&quot; In the above example, the numeric and logical elements have been coerced to characters, as signified by these elements being enclosed by quotation marks \" \". We can also create vectors programmatically, rather than manually defining each element. For example, we can generate sequences using seq() or repetitive sequences using rep(): # Creating a sequence from 1 to 5, in increments of 1 s &lt;- seq(from = 1, to = 5, by = 1) s ## [1] 1 2 3 4 5 # Creating a vector of length 5, consisting of the value 10 r &lt;- rep(10, times = 5) r ## [1] 10 10 10 10 10 3.4.2 Data frames Other common R data structures include matrices, lists and data frames. The data frame is the most commonly used structure for tabular data and will be familiar to you, as this similiar in structure to a Microsoft Excel spreadsheet (containing rows and columns of data). Here is an example data frame, which is created by combining four existing vectors of the same length (name, height, nationality, bereft_of_life): # Create numeric, character and logical vectors name &lt;- c(&quot;John&quot;, &quot;Eric&quot;, &quot;Michael&quot;, &quot;Graham&quot;, &quot;Terry&quot;, &quot;Terry&quot;) height &lt;- c(1.96, 1.85, 1.78, 1.88, 1.75, 1.73) nationality &lt;- c(&quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;American&quot;, &quot;British&quot;) bereft_of_life &lt;- c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE) # Combines into a data frame df &lt;- data.frame(name, height, nationality, bereft_of_life) df ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 5 Terry 1.75 American FALSE ## 6 Terry 1.73 British TRUE Run the above code in your R console We’ll be working with data frames throughout the practicals, so it will be useful to know that we can access data frame elements in a variety of different ways. We can access individual columns of a data frame using the $ operator, for example: # Extract the &quot;name&quot; column df$name ## [1] &quot;John&quot; &quot;Eric&quot; &quot;Michael&quot; &quot;Graham&quot; &quot;Terry&quot; &quot;Terry&quot; We can also extract using indexing, which requires us to provide the row and columns indexes in the following format: df[row.index, column.index] # Extract the element at row 3, column 4 df[3,4] ## [1] FALSE # Extract rows 1 to 4, all columns df[1:4,] ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE # Extracts all rows, the second and third columns df[,2:3] ## height nationality ## 1 1.96 British ## 2 1.85 British ## 3 1.78 British ## 4 1.88 British ## 5 1.75 American ## 6 1.73 British We can also extract rows and columns based on the values within the data frame, for example using the subset() function. This can be used to select and exclude observations as follows: # Extract data frame rows where height is more than or equal to (&gt;=) 1.8 m. subset(df, height &gt;= 1.8) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 4 Graham 1.88 British TRUE # Extract data frame rows where nationality is not equal to (!=) American subset(df, nationality != &quot;American&quot;) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 6 Terry 1.73 British TRUE # Extract data frame rows where bereft_of_life is equal to (==) FALSE subset(df, bereft_of_life == FALSE) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 5 Terry 1.75 American FALSE Using the above code as an example, extract all the rows where name is equal to Terry. Assign the result to a new object. You can calculate the number of rows using the function nrow(). Did you get the following answer? ## [1] 2 The sections above have given you a crash course in some of the fundamentals of R. Learning to code (or learning a new code syntax for those of your familiar with Python, MATLAB, Java, …) is similar to learning a new language. Don’t worry if it seems complicated! You can always refer back to these instructions if you get stuck, or ask for guidance. To finish this Chapter, we’ll provide a brief overview of some other key skills, including writing R scripts, loading data and packages, and plotting. This should prepare you for the formative task. This is not an exhaustive list of R functions, and we’ll cover other functionality in Practicals 1 and 2 (e.g. loading GIS raster and vector data, creating maps and figures, performing statistical tests). However, the R functionality outlined here will cover all the steps required for the assessment and should equip you with the core skills required to progress further with R in the future e.g. as the basis for your dissertation. 3.5 Scripts So far we’ve been typing commands into the console. This works as expected; we can print results, store objects in the workspace and create plots in the plots window. However, it is often more convenient to store our code in a script, which is simply a file containing a set of commands and comments. For example, a single R script could contain commands to load data, perform statistical analysis, and output the results to a new file. This can be run in a single keystroke, which is much simpler than typing each command in one after another. To create a new R script: Navigate to File, New File and R Script. This should open a new window, which is an untitled R script: To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. learning_R) One of the easiest ways to run an entire script is to use Ctrl + Shift + Enter. However, you can also run aspects of a script by clicking on the individual line you want to run and using Ctrl + Enter. The same command applies if you highlight (select) multiple lines of code. 3.5.1 Comments When writing code, it is good practice to include comments which describe what the code does. As your code becomes more complex and as time passes, it is very easy to forget what your code does! Commenting your code is especially important if your code is being used by other people or even by yourself weeks/months/years in the future; save yourself time and effort by describing your code fully as you write it. Comments can be written in R using the # symbol. Any line of code which begins with # is ignored by R when the script is run and are used solely to improve the readability of the code. To add a comment, insert the # symbol and write some text. Below is some uncommented code. Copy and paste into your script and add some comments to describe what is taking place. a &lt;- seq(10, 100, 10) b &lt;- a/10 c &lt;- b*a plot(c, a) The above code is our first encounter with plot() which (as you may have guessed), is used to plot objects in base R 1. The new plot you’ve created should now appear in the plots window and should resemble the following figure. There are lots of more exciting ways to plot data in R, some of which are described below. As a general rule, well-presented code would typically comprise: 1/3 code; 1/3 empty space; 1/3 comments. This is known as the Rule of Thirds. 3.6 Loading packages Now that we have our script set up, we need to understand is how to install and load packages in the R environment. To begin, remove all the previous code from your script learning_R.R and save. For the remainder of this chapter, we are going to use just one package as an example, but more will be required for Practicals 1 and 2. The package we are going to use is ggplot2, which is a widely used visualization package for R. If there’s a figure, map or graphic you want to make, it’s likely that ggplot2 can do it for you. It’s now even used by the BBC! BBC graphics created using ggplot2. See here for details. We can install the package using the helpfully named function install.packages() as follows: # Function to install packages. Name of package enclosed by quotation marks. install.packages(&quot;ggplot2&quot;) This function installs the package to your computer, but before we can use it, it needs to be activated within your current R session. This can be achieved by adding it to the library() as follows: # Function to add package to the library (no quotation marks) library(ggplot2) Copy and paste the two code snippets above into your script and run to install and load ggplot2. This is the standard approach to installing and loading packages in R. However, one of the weaknesses of this approach is that R will try and install and load each package every time the script is run. This is unnecessary; the package is already loaded! Try to re-install ggplot2 using the code above. The following pop-up should appear: Press No. The following message will appear in the console: Warning in install.packages : package ‘ggplot2’ is in use and will not be installed We can address this issue by using a user-defined function (called check.packages) to check whether a package is already installed. If it is installed, R will skip the installation. If it is not installed, R will install as normal. This function is similar to the functions we’ve been using before, such as sqrt() or seq(), which are available by default in base R; the difference is it’s been written from scratch! # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) Copy and paste the above code at the start of your R run script, removing the previous install.packages() and library() commands. Re-run the code. In future, you can add new packages by simply adding the package name to the packages vector. This works in exactly the same way as described previously, with the vector created using the c() command. For example, we could install packages as follows: packages &lt;- c(\"ggplot2\", \"dyplr\", \"raster\", \"sf\"). A full list of the available packages is here (&gt;18,000 and counting…). Finally, packages should typically be installed and loaded at the start of any R script. If your code requires a package to run and it has not yet been installed, the code will fail. Installing and loading all the required packages at the start of the script avoids this problem. 3.7 Loading data In the final two sections of this chapter, we will load some data from a external file and plot using ggplot2. In the previous examples, we’ve been creating objects, vectors and data frames from scratch, but most data you will work with will be saved in another file format. This could be a comma-separated file (.csv), tab-delimited (.txt) or in an Excel format (.xls), or could be a spatial data type, such as a raster (.tif, .asc, .img) or vector (.shp). Being able to load and manipulate different data types is a key programming skill. We’ll use one of the most well-used formats as an example; the comma-separated file (.csv). This consists of rows and columns of data, where columns are delineated by commas. For example, here is an example of some comma-separated data: name,age,height,year_of_birth,instrument stewart,70,1.87,1952,drums andy,79,1.70,1942,guitar gordon,70,1.81,1951,vocals When read by R, it is interpreted as follows, splitting columns based on the occurrence of commas, and using the first row as column names: name age height year_of_birth instrument stewart 70 1.87 1952 drums andy 79 1.70 1942 guitar gordon 70 1.81 1951 vocals To load a .csv file into R, we can use the command read.csv(). The most basic way to load a file is to write the absolute file path. This is the location on your computer where the file is saved. For example, a file saved on your C drive, within your Documents folder, and within a folder called Manchester would be formatted as: C:\\Documents\\Manchester To determine the absolute file path, navigate in your file explorer (e.g. Windows Explorer, finder on macOS) to the data sub-directory within GEOG70581: For the final two sections of this Chapter (and the formative task), we’ll using a data file called flow_data.csv which contains the average daily river flow (m3 per second) from the National River Flow Archive and the UK Centre for Ecology &amp; Hydrology. The data are from monitoring station 90003 on the River Nevis, which drains Ben Nevis, the highest mountain in the UK, and cover the period from the 1st October 1982 to the 30th September 2019. You can explore the Glen Nevis catchment below: The file path to your file might read as follows (if your name was Bill): C:\\Users\\Bill\\GEOG-70581\\data\\flow_data.csv To load this file into R: Copy and paste this file path into your script, enclosing the file path within quotation marks. Importantly, R uses forward slashes within file paths /, whereas the file path in your file explorer probably uses backslashes \\, so these will need to be swapped. # Loading data (ensure file path is updated) df &lt;- read.csv(&quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot;) If this has worked successfully, a new object df will have been loaded into the workspace: As you may have noticed, this is not the most elegant way to load data, as it requires us to copy, paste and edit the full file path each time. This would quickly become tiring if we wanted to load multiple data sets. In Practical 1, we’ll guide you through a solution using R projects and the here package but as ever, it is important to understand the basics before moving on to more sophisticated solutions. We can use a variety of commands to inspect this new data set, including: head(df) colnames(df) str(df) Run the above code. What do each of the commands do? head(), colnames(), str() What data types do we have in each column? See here for a reminder. What data structure are we working with? As outlined previously, we can access individual columns of a data frame using the $ operator and could start to do some simple analysis using base R functions, such as: # What is the average daily flow? mean(df$flow) # What is the maximum daily flow? max(df$flow) # What is the minimum daily flow? min(df$flow) Question: What is the range of daily flow measurements? As outlined previously, we can also subset our data based on the values within the data frame, based on columns names ($) or using indexing. To simplify our subsequent analysis, we’re just going to work with data collected in 2019, although we’ll ask you to extract a different year for the formative task. The data frame rows which correspond to 2019 are from the 1st January [13242] to the 30th September [13514]. Extract these rows and all columns using indexing; see here for a reminder if you get stuck. You may also have noticed from the str() command that our column of dates (day/month/year) were stored as characters e.g. $ date: chr \"01/10/1982\". Working with dates and times is a complicated task (see this Tom Scott video for a light introduction to the problem), so to simplify plotting, we’ll convert these characters e.g. \"01/10/1982\" into a date format. This can be achieved using the following code, which overwrites the date column in the data frame: df$date &lt;- and uses the function as.Date() to define a date format. In this case, our numbers are listed in day, month and then year, which is formatted in R as \"%d/%m/%Y\": # Converts dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) Once you’ve extracted the flow data corresponding to 2019, run the above code. 3.8 Plotting In the final task, we’re going to plot these data using ggplot2. One of the challenges of ggplot2 is its steep learning curve, but we’ll walk through this example to show how we can progress from the raw data to high quality figures. These approaches will be useful for your assessment. To finish this chapter, copy and paste the following code elements into your R script in turn and run them. Make sure to read and understand what each step is doing. To begin, we’re going to create an empty ggplot canvas i.e. the blank canvas upon which we can add ggplot elements, such as axes, legends, data, scale bars etc. # Initialising an empty canvas ggplot() Next, we specify which data set we are going to plot. Note: ggplot2 works almost exclusively with data frames: # Which data frame do we want to use? ggplot(data = df) With the data frame defined, we now need to specify which columns of date we wish to plot. The code below uses the function mapping=aes()to do so, where the date variable (within the df object) is assigned to the x-axis x=date and the flow rate variable is assigned to the y-axis y=flow using their column names: # Which data frame columns do we want to plot? ggplot(data = df, mapping=aes(x=date, y=flow)) However, our data are still not visible because we need to select a geom type, which is a complicated way of saying the type of geometric object we want to plot. We could plot our data as points, lines or polygons; we could choose a histogram, bar plot, or box and whiskers plot; there are many to choose from! In this example, we are going to plot our data as points, which is done using the geom_point() command. As you can see from the code below, the structure differs slightly from normal code, in that ggplot elements are separated by the + operator. This allows the user to add each ggplot element on a separate line of code, thereby improving readability. # What type of geom do we want? ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() In the same way, we could also plot the same data as a line, using geom_line(). In this example, both geoms are using the same data frame as inputs x=date, y=flow. This is because the data frame and columns have been specified in the initial call to ggplot here: data = df, mapping=aes(x=date, y=flow). All subsequent geom objects (e.g. points, lines, bar plots…) will use these inputs, unless an alternative is specified. # Adding a line geom ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() + geom_line() The above code covers the basics of ggplot, but we can start to improve the design of our plots. For example, we can choose one of the available themes. We could re-order our geoms to show the data points above the line, while adding some colour using HEX codes. We could change the aspect ratio to produce a square plot and maybe add some axis labels: # Specifying data frame and columns ggplot(data = df, mapping=aes(x=date, y=flow)) + # Specifying colours geom_line(colour = &quot;#56ACFF&quot;) + geom_point(colour = &quot;#767676&quot;) + # Adding a ggplot theme theme_classic() + # Setting the aspect ratio theme(aspect.ratio = 1) + # Updating axis labels (including superscript elements) labs(x = &quot;Date (January - October 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) Using the above plot, we could start to identify periods of above or below average flow. Adding a trend line could also be useful to identify patterns in the data. Add a trendline to your plot using the geom_smooth() argument. You can experiment with different smoothing methods e.g. method = “lm”, “glm”, “gam” or “loess”. As a final step, we will probably want to export our figure so that it can be used elsewhere e.g. in the assessment. To do so, we simply assign our ggplot object to an object using the assignment operator: &lt;- as normal: # Assign our ggplot object to the object &quot;g&quot; g &lt;- ggplot(data = df, mapping=aes(x=date, y=flow)) + ... Finally, we can save our ggplot object using the ggsave() function, as shown below. This takes in the name of the plot (g), as well as the intended name, extension and file path e.g. \"C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png\". In this example, we are saving our output within the images sub-directory of GEOG-70581. We have also defined the name of the file flow_data_2019 and specified its file type: .png (portable network graphics file). We can also modify other settings, such as the dpi which determines the plot resolution. A list of ggsave options can be found here. # Saves to a png ggsave(plot = g, &quot;C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png&quot;, dpi = 150) ## Saving 7 x 5 in image Run the above code to assign your plot to the object g and export it to your images folder, making sure to update the file path! We have now exported our plot and should be able to see it in the images sub-directory of GEOG-70581. Importantly, and for future plots, you don’t need to run the commands in the sequence shown above e.g. first creating a canvas ggplot(), then defining a data frame ggplot(data = df), and so on. This sequence was used to show you how we can add elements to the ggplot object, building from the blank canvas to exporting the final figure. 3.9 Formative task Congratulations! You have now completed the guided portion of Chapter 3: the (not-so) brief introduction to R. While this may have been challenging, we have covered most of the core skills which you will need. You should now have an appreciation of the different data types and structures that are available in R, experience running code in the console and within scripts, and the ability to install packages, load data and create plots. We’ll use many of these techniques in Practicals 1 and 2. To finish the practical, please complete the formative task outlined below. Create an R Script which does the following: Installs the required packages. Loads the flow data for Glen Nevis. Extracts all rows corresponding to the years 2000 to 2019, making sure to convert to date format. Finds the average daily river flow (m3 per second) for each month and plots the result. Questions: on average, which month had the highest average daily flow and which had the lowest? Hints and Tips: For general guidance on using ggplot2, see here. To extract the month from each record, you may want to use the months() function from base R, while aggregate() (described here) can be used to compute summary statistics (i.e. the mean) based on groups. 3.9.1 Formative solution A code solution to the formative task will be uploaded here after the class. Don’t worry if your solution is slightly different from mine! Note If you’re working on a personal computer with Chinese language settings, you may need to run Sys.setlocale(\"LC_TIME\", \"English\") at the start of your code to fix language-related errors the in the as.Date() function. # Solution to the Chapter 3 formative task # Author: matt-tomkins # Date of upload: 21-11-2025 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) ## ggplot2 ## TRUE # Load data from csv (personal file path - update to your own) df &lt;- read.csv(&quot;C:/Users/44797/Desktop/Repo/GEOG-70581/data/flow_data.csv&quot;) # Convert dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) # Extracts post-2000 river flow df_post2000 &lt;- subset(df, date &gt;= as.Date(&quot;2000-01-01&quot;)) # Create a new column, corresponding to the month of measurement, using the months() function df_post2000$month &lt;- months(df_post2000$date) # Summarise by month, using the aggregate() function and &#39;mean&#39; summary &lt;- aggregate(flow ~ month, df_post2000, mean) # Determines plotting order by converting the month column to a &#39;factor&#39; # The factor levels are determined by the R month.name function summary$month &lt;- factor(summary$month, levels = month.name) # Plotting using ggplot2 g &lt;- ggplot(data = summary, mapping=aes(x=month, y=flow)) + # Stat = &quot;identity&quot; is used when the values of x and y are known geom_bar(fill = &quot;#3EBBFB&quot;, stat=&quot;identity&quot;) + # Sets the theme theme_classic() + # Add x and y-axis labels labs(x = &quot;Month (2000 - 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) + # Tight fitting y-axis scale_y_continuous(expand = c(0,0)) + # Adjusts angle of x-axis labels theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # Displays figure g # Prints highest and lowest average daily flows print(paste0(&quot;The month with the highest average daily flow is &quot;, summary$month[summary$flow == max(summary$flow)])) ## [1] &quot;The month with the highest average daily flow is January&quot; print(paste0(&quot;The month with the lowest average daily flow is &quot;, summary$month[summary$flow == min(summary$flow)])) ## [1] &quot;The month with the lowest average daily flow is July&quot; Base R is used to refer to the functions that are part of the core packages deployed with R, such as plot(), sqrt() or seq(). As we install and utilise user-created packages, we are moving away from base R.↩︎ "],["FirstPractical.html", "Chapter 4 Eskdale I - Introduction 4.1 Overview", " Chapter 4 Eskdale I - Introduction It should be evident from the lectures that digital elevation models (DEMs) have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions that (1) we can trace the path that water will take from any point in an elevation grid to a basin outlet, and (2) the discharge of water from a particular location is directly related to its catchment area. This is a fairly reasonable assumption in most drainage basins. 4.1 Overview In this lab exercise, we will use a DEM of the Upper Eskdale catchment to model flow-related phenomena, and will have gained experience in: DEM pre-processing; Calculating flow parameters (e.g. pointers and contributing areas); Comparing flow algorithms. Make sure to save your R script regularly! Upper Eskdale catchment 4.1.1 Intended learning outcomes In Chapter 5, we’ll download the data and install the required programs and packages, and will combine these in Chapter 6 to: gain experience with WhiteboxTools, a specialist GIS specifically designed for digital terrain analysis (DTA); become familiar with some of the most commonly used flow algorithms used to simulate overland flow from digital elevation models (DEMs); consider the utility of DTA for routing applications; 4.1.2 Assessment This practical is formative and as such will not count towards your final mark for GEOG 70581. However, you are encouraged to complete the practical to the best of your ability as the skills developed here will be used throughout the assessed Practical 2. As with the Introduction to R practical, I will upload a code solution after the class so you can check your working. "],["Eskdale_set_up.html", "Chapter 5 Eskdale II - Set-up 5.1 Install programs 5.2 Download data 5.3 Open RStudio 5.4 Initialise an R project 5.5 Creating an R script 5.6 Install WhiteboxTools 5.7 Loading packages", " Chapter 5 Eskdale II - Set-up 5.1 Install programs If you haven’t installed R and RStudio, please refer to the instructions here before continuing. 5.2 Download data If you’ve not already done so, the data for this practical can be downloaded here. The directory structure is outlined in Chapter 2. 5.3 Open RStudio To begin, open RStudio. 5.4 Initialise an R project In the previous chapter, we typed commands into the console and used R scripts to store commands and comments. For both Practicals 1 (Eskdale) and 2 (Mersey), we are going to expand on these by also utilising R projects. Projects make life simpler by allowing us to access file locations (e.g. for loading and writing data) using relative file paths, rather than using the clunky and prone-to-breaking absolute file paths. Relative file paths work by setting the working directory, which is simply the the default location where R will look for files you want to load and where it will put any files you save. We can do this manually by using the setwd() function, but a simpler approach is to use an R project, which sets the working directory for your automatically. Rather than the complicated absolute file paths: &quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot; which indicates that the file is located in data, which is within GEOG-70581, which is within Documents, which is within Bill, which is within Users, which is on the C drive, we can set the working directory to GEOG-70581 (for example) and then access the file using the here package: here(&quot;data&quot;, &quot;flow_data.csv&quot;) This approach not only makes it easier to access files, but also improves reproducibility. Using projects ensures that our work is self-contained (the project directory contains all the data, scripts and outputs) and portable. Transferring the project directory to another user, to a different sub-folder on your computer, or a different drive, would not cause any code to fail. By comparison, scripts featuring absolute file paths or a manually defined working directory would likely fail in any of the above scenarios. To initialise a new R project: Go to File, New Project, and select Existing Directory. Using the Browse icon, set the project working directory to the folder GEOG70581, which you downloaded and unzipped here. Select Create new project If this has been successful, your console should have been updated to include the path to your project working directory as follows: 5.5 Creating an R script Now that we have created our project for GEOG70581, we are going to use scripts to store the code for each practical. We’ll create a script for Practical 1 and a separate script for Practical 2. Important: a script can be understood as a set of instructions. As a result, the order of the code is critical. A useful metaphor may be to think of a script as a recipe. For example, a simple cake recipe might require butter, sugar, eggs, flour, milk, vanilla extract and lemon. These could be combined as follows: Pre-heat oven. Whisk butter and sugar. Crack the eggs in one at a time and whisk. Add the lemon zest, vanilla, flour and milk and whisk. Bake in the oven for 30 mins. In code form, it might resemble the following, with a vector including the ingredients and different functions used to combine the ingredients (whisk), turn on the oven (pre_heat) and bake (bake_mixture): # Vector of ingredients ingredients &lt;- c(butter, sugar, eggs, flour, milk, vanilla, lemon) # Pre-heats oven for 20 minutes pre_heat(20) # Create a mixture of butter and sugar using the whisk function, mixture &lt;- whisk(ingredients$butter, ingredients$sugar) # Iterates (loops) over eggs variable for (i in length(ingredients$eggs)){ # Extracts each egg, one at a time current_egg &lt;- ingredients$eggs[i] # Adds current egg to mixture mixture &lt;- whisk(mixture, current_egg) } # Add remaining ingredients mixture &lt;- whisk(mixture, ingredients$flour, ingredients$milk, ingredients$vanilla, ingredients$lemon) # If oven temperature is 180 degrees of more if(oven_temperature &gt;= 180){ # Bake for 30 minutes cake &lt;- bake(mixture, time = 30) } From the text and code example above, it should be clear that our recipe (and code) would not work correctly if we tried to bake in the oven before pre-heating it, or added the ingredients in the wrong order. In exactly the same way, it is important that your code is formatted in a logical order (e.g. Function 1, Function 2, Function 3, …). Ensure that your future code reflects the recipe above, where individual functions utilise variables created previously, and don’t rely on variables created further down in the code. To create an R script for Practical 1: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-1-Eskdale) This should now resemble the following: 5.6 Install WhiteboxTools For Practical 1, we are going to use the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); These packages can be installed as normal using the install.packages() and library() functions. In previous versions of this course, whitebox was not available (because of complicated reasons) on the Comprehensive R Archive Network (CRAN). Instead, it had to be installed using R-Forge. While CRAN is the official package repository for R and generally features “release” versions of packages, R-Forge provides candidate, beta or “work-in-progress” packages or packages which don’t meet the criteria for inclusion on CRAN. However, as of last year, whitebox is now back on CRAN so we can install as normal using the install.packages() function. It is worth noting, however, that you can install packages from R-Forge by simply specifying the repository (web location) where the package is stored, in this case: http://R-Forge.R-project.org. # Installing a package via R-Forge install.packages(&quot;insert package name here&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) Note You don’t need to run the above code - this is just an example of how to install via an alternative repository. One additional step, which is different from “normal” R package installation, is to use the function whitebox::install_whitebox(), which initialises WhiteboxTools: # Initialise WBT whitebox::install_whitebox() This should print the following to your console: Performing one-time download of WhiteboxTools binary from https://jblindsay.github.io/ghrg/WhiteboxTools/WhiteboxTools_win_amd64.zip (This could take a few minutes, please be patient...) WhiteboxTools binary is located at: C:/Users/44797/OneDrive/Documents/R/win-library/4.0/whitebox/WBT/whitebox_tools.exe You can now start using whitebox &gt; library(whitebox) &gt; wbt_version() This code checks if a suitable WhiteboxTools executable is present (.exe) and installs if missing. The WhiteboxTools executable contains all the code used for geospatial analysis, which we then access using R via the whitebox R package. This is a slightly different configuration from normal R packages but enables WhiteboxTools to be used cross-platform i.e. WhiteboxTools functionality stored in the executable can be assessed through R, Python, ArcGIS or QGIS etc, depending on the user needs. Run the below code to install the whitebox package, initialise WhiteboxTools (which should perform a one-time download of the WhiteboxTools executable) and check it is installed correctly. # Install the whitebox R package install.packages(&quot;whitebox&quot;) # Initialise WhiteboxTools whitebox::install_whitebox() # Add the whitebox R package to library library(whitebox) # Check the version is correct wbt_version() When complete, WhiteboxTools has now been successfully installed and initialised and is almost ready to be used. However, to avoid re-installing and re-initialisng WBT every time we run our script, we can either delete the code or comment-out # each line. Remember that R ignores any line of code that begins with #: # Code to install WhiteboxTools # whitebox::install_whitebox() Either delete or comment-out the previous command (install_whitebox()). Ctrl + Shift + C is a useful shortcut for adding or removing comments from multiple lines of highlighted code. 5.7 Loading packages To finish the set-up for Practical 1, we are going to install the remaining required R packages (described above) and load them into the R library. We’ve already practiced installing and loading the ggplot2 package in Chapter 2 so you are ready for the following: Referring back to the instructions here, install and load ggplot2, here, raster, sf, ggspatial and whitebox. Hint 1 - we’ve already installed whitebox, so this can be excluded if you’re using install.packages() and the library() commands. Hint 2 - the user-defined function check.packages might be useful here. If the package has been installed and loaded correctly, the console should print the following: ggplot2 here raster sf whitebox ggspatial TRUE TRUE TRUE TRUE TRUE TRUE where the logical value TRUE equates to a successful installation (and vice versa for FALSE). "],["Eskdale_flow_algorithms.html", "Chapter 6 Eskdale III - Flow routing 6.1 DEM pre-processing: flow enforcement 6.2 Flow Parameters: Pointers 6.3 Flow Algorithms 6.4 Practical solution", " Chapter 6 Eskdale III - Flow routing Overland and near-surface water flow can be modelled using DEMs if we assume that surface topography is the sole factor which influences the distribution of water. One very simple model routes all water from a particular grid cell in a DEM to a single neighbouring cell (i.e. water is not partitioned between multiple neighbours). This ‘D8’ (8 direction) method sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell. Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). Using this method, water flow is allowed in one of eight possible directions (↑ ↗ → ↘ ↓ ↙ ← ↖), assuming that water will travel along the steepest downslope path. In turn, the method is sometimes referred to as the steepest descent method. Based on the \\(3 * 3\\) cell neighbourhood shown in Figure 1, flow would be directed from the centre cell (8 m elevation) to the southwest cell (4 m elevation). Reminder: you don’t have to run every bit of code in this document. Indented text (such as this) is an instruction that tells you something to either think about, or do. 6.1 DEM pre-processing: flow enforcement One common issue encountered when performing hydrological analyses is the presence of sinks, which interrupt the drainage network. When sinks are encountered, flow direction is undefined when a grid cell, or group of grid cells, is lower than all neighbouring cells (see Figure 2). When sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. These features are referred to as pits if they are a single cell in size, and depressions if they consist of groups of cells. Figure 2: Schematic of a sink in a DEM. In this \\(5 * 5\\) matrix, water is routed into the top right cell (9 m) and then is routed to the lowest elevation cell at each step (9 m → 8 m → 7 m → 6 m → 2 m) using the D8 method (blue cells). However, the drainage network is interrupted by a sink at the 2 m cell (orange), as all neighbouring cells are of higher elevation. Sinks can often be artefacts of the data and should be removed during DEM pre-processing. Pre-processing involves altering the elevations of the DEM in a way that enforces continuous flow-paths. However, it is important to realise that sometimes these ‘digital depressions’ reflect actual features in the landscape, and should be preserved during flow modelling. This is a particular issue for hydrological analysis of karst environments, where water can be routed into dolines and fractures (see Figure 3). However, for our work, we will assume that all depressions in DEMs are artefacts and are justified in being removed. Figure 3: An example of a “true” DEM sink. A sinkhole extending into the limestone karst of Twisleton Scar, Yorkshire Dales, with Ingleborough behind (723 m). How should we model water flow in environments with sinkholes, fractures, dolines and extensive sub-surface water flow? Several methods have been developed for removing depressions from DEMs. These methods vary greatly in terms of their sophistication and impact on the DEM. The two most common depression removal methods (Figure 4) are: depression filling, which raises cells within a depression to the elevation of the outlet cell; depression breaching, which digs a trench from a depression’s bottom to some point downslope. Figure 4: Schematic of depression filling and breaching in a DEM. Using the same values from Figure 2, original values are modified to allow water to escape the sink (orange). Depression filling has raised the value of the sink (2 m → 4 m), while depression breaching has lowered the value of a neighbouring cell (3 m → 1 m). In this simplified example, the outputs of these two distinct approaches are identical but care should be taken when working with real world data as they will often produce different results. Not all interruptions to flow routing are caused by depression cells. Often, DEMs contain extensive flat regions (areas of equal elevation). Flat areas interrupt flow routing in the same way as depressions. Cells within a flat region do not have downslope neighbours, and therefore, flow routing is impossible on flat sites without pre-processing. Correction of flow direction on flat sites typically involves finding an outlet cell, forcing flow from cells adjacent to the outlet to the outlet, and continuing backwards in an iterative manner (e.g., Jenson and Domingue (1988)). 6.1.1 Data description The DEM we are working with is centred on the Upper Eskdale catchment; an upland valley which drains the highest mountain in England (Scafell Pike; 978 m), as shown below: Figure 5: Upper Eskdale panorama, viewed from Harter Fell [Location: 54.386907, -3.205004, Elevation: 649 m]. The catchment ranges in elevation from 978 m (Scafell Pike) to ~160 m at the catchment outlet (white circle), and is ringed by numerous summits with elevations in excess of 800 m (white triangles). The catchment has an area of ~15.7 km2 and all water which falls with the catchment ultimately drains to the Irish Sea via the River Esk. You can explore the catchment below: The DEM we are working with was downloaded from EDINA Digimap, has a cell size of 10 m and uses the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Unfortunately, we don’t have time in this course to delve into the exciting world of map projections, although these are covered in great detail in Understanding GIS. However, it is important to know that different map projections have different uses and work more/less effectively in different regions. As we are working within the UK, it makes sense to use a map projection which is tailored to the UK (e.g. BNG) as this minimises different types of map distortion (length, shape, area). When loaded into R using the raster package and plotted using ggplot2 and ggspatial, our DEM can be visualised as follows, where the outlet point (white circle) and summits (black triangles) match those shown in Figure 5. To accentuate areas of relief, the DEM has been combined with a semi-transparent hillshade layer, which is shown below: 6.1.2 Breaching and filling To evaluate the effects of depression breaching and filling, we’re going to use the whitebox package and the wbt_fill_depressions and wbt_breach_depressions functions. One of the advantages of using whitebox is that functions can be applied to spatial data without having to load files into the R environment (and assign them to objects), which can be an issue when working with large files (e.g. high spatial resolution rasters) or when working on computers with limited available memory. We can call whitebox functions by simply naming the function, specifying the names of the input and output layers, and deciding on parameters (if required). For example, the code to fill depressions, might resemble the following: # Fill depressions wbt_fill_depressions(&quot;name of input file goes here&quot;, &quot;name of output file goes here&quot;) Given the structure of our file directory, this could be written as: # Fill depressions wbt_fill_depressions(&quot;C:/Users/Bill/GEOG-70581/data/practical_1/dem_10m.tif&quot;, &quot;C:/Users/Bill/GEOG-70581/output/practical_1/dem_10m_fill.tif&quot;) Here, we’ve used absolute file paths to set the input DEM as dem_10m.tif, stored in data/practical_1. The function wbt_fill_depressions will fill depressions in the input DEM and save the new DEM as dem_10m_fill.tif, stored in output/practical_1. However, because we’re using an R project we can simplify this further. As we’ll be using the DEM (dem_10m.tif) on multiple occasions, we can store its file path using the here package in the variable dem as follows: # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem Copy and paste the above code into your Practical-1-Eskdale script and run. This should print the absolute file path to dem_10m.tif. With this file path stored, we can run the fill depressions function as follows, using the here package to specify the name and intended location of the output file: # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) Run the above code, which should produce a new raster file in the output/practical_1 folder. Repeat this process to also breach depressions, making sure to update the output file name. The difference between the original DEM and the processed DEM(s) may be very subtle and you probably won’t be able to observe the differences directly. As such, it may be useful to identify the specific areas of the raster which have been modified. This can be achieved by using the subtract function to work out the difference between the original DEM and the processed DEM, as follows: # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file Run the above code. To visualise this new raster file, we’re going to load it into the R environment using the raster and here packages and plot with ggplot2 and ggspatial. The raster can be loaded as follows: # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) It can be plotted as follows: # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g Most of the above should be familiar to you, with a few exceptions: Under scale_fill_continuous(), we’ve set limits for the fill symbology. This now ranges between the maximum value, calculated using max(values(dem)), and the minimum value that is greater than 0, calculated using minimum_value &lt;- min(values(dem)[which(values(dem) &gt; 0)]). The latter is useful because most of the DEM was unmodified by wbt_fill_depressions so most raster cells contain a value of 0. The layer spatial() is a non-standard geom type and is available through the ggspatial package. This simplifies the process of plotting raster data in ggplot2, and this approach should be replicated for future plotting. Run the above code elements to reproduce the plot above. You can remove limits from the scale_fill_continuous() argument to plot the actual distribution of DEM values. Question: to what degree has the original DEM been modified by the wbt_fill_depressions function? Consider the spatial extent and magnitude of the observed differences. Note: For this catchment, the difference between DEM breaching and filling is relatively minor. We’ll use the latter (dem_10m_fill.tif) for all subsequent calculations. As outlined above, the areas which are most affected by the depression filling algorithm are partly determined by local relief i.e. relatives differences in elevation. To help visualise this, let’s create and plot a slope image for the area, where the values in the raster cells represent the gradient of the slope, rather than their raw elevation. This can be achieved using the wbt_slope function. Using the examples above, run wbt_slope, using the original DEM as the input, saving the output file with a logical name (e.g. dem_10m_slope.tif) and to the correct sub-directory (output/practical_1). Load the data into R and plot using ggplot2. Your figure should resemble the following: Compare the ‘slope’ and ‘difference’ images. Hint: you can toggle between plots using the left and right arrows in the plot window. Based on your visual inspection the images, which parts of the landscape appear to have been affected by the depression filling algorithm? Why do you think that this is the case? Before we move on to further analysis, it is worth thinking carefully about the symbology we are using for our raster layers. This is important, because not all colour ramps are created equal! Colour ramps differ in terms of their perceptual uniformity and their accessibility for colour-blind individuals. Save your script before continuing. 6.1.3 Choosing a colour ramp Perceptual uniformity refers to how well a colour ramp represents the underlying data. Some (e.g. rainbow; Hawkins (2015)) can introduce artificial boundaries (Alcalde et al. (2017)) which influence how we assess and then use that data (see here). In the schematic below, there are numerous artificial boundaries in the rainbow colour ramp (e.g. dark blue → light blue, light blue → green, green → yellow, yellow → orange), whereas the monochrome colour ramp smoothly transitions from black → white. The viridis colour ramp we have been using so far is perceptually uniform, but others are not. Inspect Figure 6 below. These show the same slope raster, but one is visualised using rainbow, while the other uses a monochrome (perceptually-uniform) colour ramp. Does this influence how you interpret the data? Figure 6: Perceptual uniformity with rainbow and monochrome colour ramps. In a similar fashion, not all colour ramps are suitable for individuals with various forms of colour-blindness (see Levine (2009) and Geissbuehler and Lasser (2013)), who may have difficulty distinguishing reds from greens, or yellows from blues. Luckily, there are now a range of perceptually uniform and colour-blind friendly colour ramps (e.g. Crameri (2018)), including Scientific Colour Maps 6.0. These can be downloaded and used across a range of GIS programs to suit a range of data types. For now, utilise the colour ramps available in R (i.e. viridis) but bear these issues in mind for future map creation. 6.2 Flow Parameters: Pointers Now that we have a DEM that has had all interruptions to flow removed, we can continue to calculate the data needed to define the drainage network i.e. the topological network describing how each cell in a DEM is connected to all others. One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. Whitebox Tools stores D8 pointers as binary numbers in base-2, as follows: We’ll explore the reason for this in Practical 2. In our earlier example of the D8 flow algorithm (see Figure 1), the flow direction was to the SW. This has a binary value of \\(2^4 = 16\\). Therefore, in the pointer file, the highlighted dark blue cell would be assigned a value of 16. Several sub-programs in Whitebox Tools use pointer files to calculate other terrain data, including the algorithms used to calculate catchment area (see below). However, sub-programs rarely require a pointer file as an input, as WBT will automatically generate a pointer layer from the input DEM when required, before discarding the file afterwards. While this processing step is not readily apparent to the user, it is important to realise that it occurs, and it is often good practice to generate a pointer file manually. This can be achieved using the wbt_d8_pointer function. Using the understanding gained above, run the wbt_d8_pointer function, using the correct input files and output file name and directory. Load the raster into R and plot. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. While this is an acceptable plot, it is actually slightly misleading, because the scale bar is continuous, despite the fact that our underlying data are discrete i.e. there are only eight possible values, which are listed below: \\(2^0 = 1\\) \\(2^1 = 2\\) \\(2^2 = 4\\) \\(2^3 = 8\\) \\(2^4 = 16\\) \\(2^5 = 32\\) \\(2^6 = 64\\) \\(2^7 = 128\\) Here is the same data but using a discrete scale: # Manually defined colours based on &quot;RdYlBu&quot; colour palette colours &lt;- c(&quot;1&quot; = &quot;#D73027&quot;, &quot;2&quot; = &quot;#F46D43&quot;, &quot;4&quot; = &quot;#FDAE61&quot;, &quot;8&quot; = &quot;#FEE090&quot;, &quot;16&quot; = &quot;#E0F3F8&quot;, &quot;32&quot; = &quot;#ABD9E9&quot;, &quot;64&quot; = &quot;#74ADD1&quot;, &quot;128&quot; = &quot;#4575B4&quot;) # Plotting p &lt;- ggplot() + layer_spatial(eskdale_pointer, aes(fill = factor(stat(band1)))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_manual(values = colours, na.value = NA) p which is described here: Question: Can you think of any issues with symbolising pointer data? Save your script before continuing. 6.3 Flow Algorithms Once you have determined how each grid cell in a DEM is connected to its neighbours, it is possible to use this pointer information to calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. Measures of catchment area are often used for stream network extraction and other terrain indices. There are three primary measures of catchment area: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment. Unlike the NUE and UCA, SCA is not affected by the grid resolution of the DEM. To calculate the upslope catchment area for each raster cell, we can use the wbt_d8_flow_accumulation function. Run the above function, using the filled DEM as the input file and the output type (out_type) as ‘specific contributing area’ (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. dem_10m_flow_accumulation.tif) and to the correct sub-directory (output/practical_1). Load the raster to R and plot, and your output should resemble the following: Inspect the map output. Question: Which parts of the landscape have low/high values of SCA, respectively? 6.3.1 Comparing flow algorithms So far we have used the most simplistic flow algorithm (D8) to model specific catchment area (SCA). However, there are many flow algorithms available (e.g. Fairfield and Leymarie (1991), Seibert and McGlynn (2007), Grabs et al. (2010)) and each algorithm results in slightly different spatial patterns of water flow. As flow routing is fundamental to the modelling of surface water hydrology, it is important to familiarise yourself with several commonly used methods, although it can be difficult to decide when or where it is appropriate to use a particular routing algorithm. In this section of the practical, we will compare the output of three flow routing algorithms including: D8: see O’Callaghan and Mark (1984); FD8: see Freeman (1991); D-infinity: see Tarboton (1997); Flow algorithms differ in the way in which they: calculate flow direction; model flow divergence, i.e., partition flow between neighbouring downslope cells; handle streams. The D8 algorithm is incapable of modelling flow divergence (or dispersion) because the flow from a particular grid cell is directed to a single downslope neighbour. FD8 (i.e. fractional D8; Freeman (1991), Quinn et al. (1991)) divides flow to each downslope neighbour using a partitioning scheme that is based on the slope angle to neighbouring cells. There are later modifications to this algorithm by Quinn et al. (1991) and Quinn et al. (1995) which increase the degree of flow convergence from the catchment divide to the channel head. You could evaluate the effects of these in Whitebox tools using wbt_quinn_flow_accumulation(). Finally, D-infinity divide flows between a maximum of two downslope neighbours based on the direction of maximum downward slope. Of these, FD8 (Freeman (1991)), and its later modifications (e.g., Quinn et al. (1995)), is the only algorithm of the three that explicitly recognises that flow on hillslopes should be divergent while flow along valley bottoms should be convergent. It is worth noting that there are many other single-flow direction (SLF) and multiple-flow direction (MFD) algorithms that can be used, see Li et al. (2020). In the final part of the practical, we will assess the specific catchment areas (SCA) derived from each of the flow algorithms discussed above. 6.3.2 FD8 catchment area To calculate the catchment area using the FD8 algorithm (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)), use the wbt_fd8_flow_accumulation function. Use the wbt_fd8_flow_accumulation function, making sure to select ‘specific contributing area’ and remembering to log-transform the values. The exponent parameter should be set to 1.1 (exponent = 1.1), as suggested by Freeman (1991). Save with an appropriate name (e.g. dem_10m_flow_accumulation_fd8.tif). Load the raster in R and plot. 6.3.3 D-infinity catchment area To calculate the catchment area using the D-infinity algorithm of Tarboton (1997): Use wbt_d_inf_flow_accumulation. Use the same input parameters as above (i.e. SCA, log-transformed values). Save with an appropriate name (e.g. dem_10m_flow_accumulation_d_inf.tif). Load the raster in R and plot. 6.3.4 Comparing the outputs The output of the algorithms should resemble the following: Figure 7: Flow algorithm comparison. Results from the D8, FD8 and D-Infinity algorithms. Plot created using ggplot2 and the patchwork package (useful for combining plots). When water flows over a hillslope, you would expect there to be some degree of divergence depending on the topography (i.e. some hillslopes are convergent and some are divergent). However, once water becomes channelised, you would no longer expect flow to be divergent. Once water is in a channel it stays there until it reaches a lake or ocean. Branching in channels is rather rare. So let’s assume that the pattern of SCA should display some degree of dispersion on hillslopes and no dispersion at all along valley bottoms (i.e. in streams). Flow divergence or dispersion is apparent in SCA images by a ‘fuzzy’ quality, almost like the SCA has been smudged. You can zoom into individual plots using the Zoom button in the Plot Window or edit the x- and y-axis limits of your plots by adding the following to your ggplot2 call: # Sets x- and y-axis limits using coord_sf() + coord_sf(ylim=c(504063,506727), xlim = c(320705, 322255)) The above code uses British National Grid coordinates (m), rather than decimal degrees. To finish Practical 1, save your script and complete the following: Compare the D8, FD8 and D-Inf outputs. Either cycle between the plots using the plot window or combine them using the patchwork package. This would need to be installed and loaded to the library. Which algorithms do a good job modelling the pattern of contributing area on hillslopes? Can you see signs of flow divergence along streams in any of the SCA images? Which algorithms perform best for modelling the pattern of SCA in fluvial (stream) environments? Do any of the algorithms do a good job for both hillslopes and streams? Congratulations! You have now completed the Practical 1 (Eskdale; Chapters 4 - 6). You now have considerable experience running whitebox functions as well as visualising spatial data using ggplot2. 6.4 Practical solution As with the introduction to R (Chapter 2 - 3), I will provide a code solution to the Eskdale practical (Chapters 4 - 6), outlining the major analytical steps. This will be released prior to the Week 10 class. # Solution to Practical 1 (Eskdale) # Author: matt-tomkins # Date of upload: 28-11-2025 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) ## ggplot2 sf here raster whitebox ggspatial patchwork ## TRUE TRUE TRUE TRUE TRUE TRUE TRUE # Initialise whitebox tools executable # whitebox::install_whitebox() # Check whitebox is in the library and the version # library(whitebox) # wbt_version() # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) ## fill_depressions - Elapsed Time: NA [did not run] wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) ## breach_depressions - Elapsed Time (excluding I/O): 0.51s # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) ## d8_pointer - Elapsed Time (excluding I/O): 0.5s # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) ## d8_flow_accumulation - Elapsed Time (excluding I/O): 0.71s # Calculates FD8 accumulation file (SCA), with log-transformed values and an exponent of 1.1 wbt_fd8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_fd8.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;, exponent = 1.1) ## fd8_flow_accumulation - Elapsed Time (excluding I/O): 0.76s # Calculates D-Infinity accumulation file (SCA), with log-transformed values wbt_d_inf_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_d_inf.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) ## d_inf_flow_accumulation - Elapsed Time (excluding I/O): 0.51s # Loads accumulation files using raster and here packages eskdale_d8 &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;)) eskdale_fd8 &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_fd8.tif&quot;)) eskdale_d_inf &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_d_inf.tif&quot;)) # Plots D8 using ggplot2 p_d8 &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_d8, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;D8&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Removes legend, sets title size theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) # Plots FD8 using ggplot2 p_fd8 &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_fd8, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;FD8&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Sets legend location and axis labeling theme(legend.position = &quot;bottom&quot;) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.title = element_text(hjust = 0.5)) # Plots D-Infinity using ggplot2 p_dinf &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_d_inf, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;D-Infinity&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Sets fill symbology theme(legend.position = &quot;top&quot;) + # Removes legend and sets axis labeling theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) # Combines using the patchwork library combined_plot &lt;- p_d8 + p_fd8 + p_dinf # Displays figure combined_plot References "],["SecondPractical.html", "Chapter 7 Mersey I - Introduction 7.1 Overview", " Chapter 7 Mersey I - Introduction Within a Geographical Information System (GIS) framework, readily available spatial datasets, such as land use and geology, have been used to explore the controls on river water quality. Understanding the relationships between catchment characteristics (or metrics) and river water quality provides a base for determining how future changes in both land use and climate will impact river water quality. Therefore, it is important to determine the processes that regulate river water quality in landscapes under increasing pressure from human population, whether from urbanisation or more intensive food production. The Mersey Basin is one of the UK’s most environmentally varied regions, with rich rural landscapes and urban-industrial centres. Industrial prosperity in the region over the last few hundred years resulted in severe pollution of the many rivers in the basin. Although there has been significant improvements in water quality in recent years (see The Mersey Basin Campaign), the rivers in the Mersey Basin are still subjected to a number of sewage and industrial inputs, and agricultural runoff. You can explore the basin below: 7.1 Overview In this practical you will explore the controls on river water quality in the Mersey Basin and develop empirical models of river water quality across the region. 7.1.1 Topics covered in this practical Terrain analysis, including catchment delineation and derivation of catchment characteristics; Writing and execution of scripts; Multiple linear regression to create models of catchment hydrochemistry; Methods for model evaluation. 7.1.2 Intended Learning Outcomes Gain practical experience of modelling environmental phenomena using a Geographical Information Systems (GIS) framework; Gain practical experience of hydrological modelling using GIS; Gain practical experience of manipulating and analysing raster datasets; Gain practical experience of using Map Algebra functions and writing scripts; Gain practical experience of using multivariate analysis; Understand some of the key controls on river water quality; Be able to critically evaluate model outputs. 7.1.3 Assessment This practical forms the basis for Assessment 2, full details of which are provided on Canvas here. "],["Mersey_set_up.html", "Chapter 8 Mersey II - Set-up 8.1 Practical overview 8.2 Install programs 8.3 Download data 8.4 Data description 8.5 Open RStudio 8.6 Projects and Scripts 8.7 Loading packages", " Chapter 8 Mersey II - Set-up 8.1 Practical overview This practical is comprised of six primary tasks, with three weeks of class time available (Weeks 10 - 12). Each of the steps is described in more detail in the remainder of this document. An outline of the key tasks is as follows: Mersey III: To complete in class in Week 10, and finish before the class in Week 11: Task 1: Flow routing Task 2: Seed points Task 3: Watershed delineation Mersey IV: To complete in class in Week 11: Task 4: Extracting catchment characteristics Mersey V: To begin in class in Week 11, and complete in Week 12 and via independent study: Task 5: Model building Task 6: Model evaluation It is anticipated that students should have made good progress through the above tasks in Weeks 10 and 11, giving time in Week 12 to complete any outstanding analysis, develop further skills and understanding (e.g., more ambitious model evaluation, with staff guidance), commence the practical report, and ask any questions you may have before submission. The above structure was developed in direct response to student feedback from previous years, with requests for additional class time for statistical analysis and for assessment guidance. 8.2 Install programs You should have already installed R and RStudio. If not, please refer to the instructions here before continuing. 8.3 Download data The data for this practical have already been downloaded here and can be found in data/practical_2. The directory structure is outlined in Chapter 2. 8.4 Data description In Practical 1, we used a single raster file (dem_10m.tif) and visualised and assessed the outputs of a number of WBT functions (e.g. fill, breach, slope, pointer, flow accumulation). In Practical 2, we are going to use a much wider range of input data, all of which use the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Alongside a few supplementary files (which you may wish to use for plotting), the key data are described here: Raster data (.tif): mersey_dem_fill a filled digital elevation model of the Mersey Basin; mersey_rainfall a raster of precipitation values; mersey_bedrock a categorical raster of bedrock geology types; mersey_HOST a categorical raster of soil types (Hydrology of Soil Types); mersey_LC a categorical raster of land cover classes, based on LCM2000 data; Vector data (.shp): mersey_EA_sites a point vector representing the locations of water quality monitoring stations. The attribute table contains a unique Environment Agency ID for each (EA_ID): Tables (.csv): mersey_EA_chemisty a comma-delimited table containing measurements for the following water quality indicators, as well as corresponding Environment Agency IDs: pH: acidity/alkalinity; SSC: suspended solids concentration (mg l−1); Ca: calcium (mg l−1); Mg: magnesium (mg l−1); NH4: ammonium (mg-N l−1); NO3: nitrate (mg-N l−1); NO2: nitrite (mg-N l−1); TON: total oxidised nitrogen (mg-N l−1); PO4: phosphate (mg-P l−1); Zn: zinc (μg l−1). mersey_EA_characteristics a comma-delimited table containing some catchment characteristics, as well as corresponding Environment Agency IDs: Percentage of the each of the five land cover classes present (derived from mersey_LC); Percentage of the each of the four soil types present (derived from mersey_HOST); Percentage of the each of the three bedrock geology types present (derived from mersey_bedrock). Read through the above descriptions carefully, making sure you understand the data we are using before moving on. We’ll be combining the Environment Agency measurements of water quality (e.g. pH, SSC, …) with spatial data representing the catchment (e.g. topography, rainfall, land cover, …) to investigate the controls on water quality across the Mersey Basin. 8.5 Open RStudio To begin, open RStudio. 8.6 Projects and Scripts 8.6.1 Using an existing R project In Practical 2 (Eskdale), we utilised an R Project called GEOG70581. Your GEOG70581 directory should resemble the following: Here we have sub-directories for the original data (data) and any spatial data files or images we might create (output and images). We can also see the GEOG70581 project file and the Practical-1-Eskdale R script. The former is used to improve file access and to ensure our code is reproducible, self-contained and portable (see here for a refresher). The latter contains all our code and comments relating to Practical 1. In this practical, we don’t need to create a new R project. We will simply use the existing R project as follows: Go to File, Open Project in New Session, and select the GEOG70581 project file. If this has been successful, your console should have been updated to include the path to your project working directory as follows: 8.6.2 Creating an R script As shown in the file directory image above, we already have an R script for the Eskdale practical (Practical-1-Eskdale). As we are now working on a separate practical, with different input data and analytical techniques, it makes sense to create a new script to store the code and comments. To create a new script for Practical 2: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-2-Mersey) This should now resemble the following: 8.7 Loading packages As we’re working in the same R project from Practical 1, we don’t need to re-install already utilised packages (e.g. whitebox). However, we will need to install some new packages and ensure that all packages are loaded into the R environment. Copy and paste the check.packages function into your new script, either from below or from Practical-1-Eskdale.R: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } In Practical 1, we used the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); In Practical 2, we are going to use a number of additional packages: dplyr for easy manipulation of data frames; units for calculation of measurement units; glmnet for regression analysis (LASSO) To load new packages, you can either use the install.packages() and library() functions or more simply, add package names to the packages vector, as shown here: # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;ggspatial&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;sf&quot;, &quot;whitebox&quot;, # Practical-1 &quot;dplyr&quot;, &quot;units&quot;, &quot;glmnet&quot;) # Practical-2 check.packages(packages) Use the above code to install and load the required packages. "],["mersey_three.html", "Chapter 9 Mersey III - Hydrology 9.1 Task 1: Flow routing 9.2 Task 2: Seed points 9.3 Task 3: Watershed creation", " Chapter 9 Mersey III - Hydrology 9.1 Task 1: Flow routing It should be evident from the lecture and the previous practical that DEMs have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions: We can trace the path that water will take from any point in an elevation grid to a basin outlet; The discharge of water and sediment from a particular location is directly related to its catchment area. However, the first assumption is not met when the flow direction is undefined. This occurs when a single grid cell (pit), or group of grid cells (depression), is lower than all neighbouring cells. When these DEM sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. While these sinks can reflect true features of the landscape, they are often DEM artefacts and can be removed using depression filling or breaching. For more information on DEM sinks and the methods used to remove them, see the previous practical. For this practical, we’ll be working with a DEM where sinks have already been removed, as indicated by the file name (mersey_DEM_fill). While it is not always practical to use file names which include all analytical steps, it is good practice to include the key analytical steps (e.g. _fill, _slope, _hillshade). 9.1.1 Creating a D8 pointer One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. One common approach is the D8 algorithm, which sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell (see Figure 1). Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). We also know from the previous practical that Whitebox stores D8 pointers as binary numbers in base-2 (see Figure 2): Figure 2: Pointer data format in Whitebox Tools. Flow directions (NE, E, SE, S, SW, W, NW, N) are stored as binary numbers in base-2 (\\(2^0, 2^1, 2^2, 2^3, 2^4, 2^5, 2^6, 2^7\\)) Initially, this can seem unnecessarily complex, particularly when compared to a simple sequential storage method (e.g. \\(N = 0, NE = 1, E = 2, ...\\)). However, the advantage of this approach is only apparent when multi-directional flow algorithms are considered such as FD8 (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)) and D-infinity (Tarboton (1997)). While the D8 algorithm assigns all water flow to a single neighbouring cell, multi-directional algorithms split water flow into two or more neighbouring cells. For these methods, a binary base-2 approach enables this information to be stored efficiently as a single number (see Figure 3). With eight neighbouring cells, there are a total of 255 possible combinations2. Figure 3: Comparison of Whitebox Tools and SAGA GIS flow direction formats. In WBT, all combinations of multi-directional water flow can be expressed as a unique number using base-2. In this example, the value of the centre cell (dark blue) is equal to 44, which is the sum of flow to its W (32), S (8) and SE neighbours (4). In SAGA GIS, the same flow distribution is equal to 13, which is the sum of W (6), S (4) and SE flow (3). However, this same value (13) can also be obtained with flow to the NE, NW and SW (among others!). For this practical, we’ll be calculating the pointer file using the simplest D8 algorithm, as shown in Practical 1. Using your experience from Practical 1, run the wbt_d8_pointer function, using mersey_dem_fill.tif as the input file and saving with an appropriate output name (mersey_dem_D8_pointer.tif) and to the correct directory (output/practical_2). Remember: use the here package for accessing file locations. Load the raster into R using the raster package and plot using ggplot2 and ggspatial. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. Note that the map below is using a continuous scale - a categorical scale would be more appropriate for pointer data, see last week! Save your script before continuing. 9.1.2 Catchment area Using this pointer information, we can calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. For now, we’ll be calculating the Specific Contributing Area (SCA), which utilises the Upslope Contributing Area (UCA) and Number of Upslope Elements (NUE) metrics: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment and is unaffected by the grid resolution of the DEM. In Practical 1, we used the filled DEM to calculate the SCA for each raster cell using the wbt_d8_flow_accumulation function, but we can also use the D8 pointer raster we just created: Run the wbt_d8_flow_accumulation function, using the D8 pointer raster as the input file (mersey_dem_D8_pointer.tif). Important: because we’re working with a pointer raster, rather than a DEM, we need to specify this parameter (pntr = TRUE). If we don’t set this parameter, WBT will assume the DEM values are actually elevations (ranging from 1 - 128 m) and we will obtain spurious results! Make sure to set the output type (out_type) as ‘specific contributing area’ (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. mersey_dem_flow_accumulation.tif) and to the correct sub-directory (output/practical_2). Load the raster to R and plot, and your output should resemble the following: 9.2 Task 2: Seed points The UK Environment Agency (EA) collects river water quality data at a series of fixed locations across the Mersey Basin. The location of each of these sites is recorded using a GPS and locations have a unique identification number. Seed points are sometimes referred to as “pour points”, and are routinely used to delineate watersheds. The locations of the EA monitoring stations are shown below: 9.2.1 Seed repositioning However, before you can delineate the watershed for each of these outlet points, the seed point location must be checked, repositioned if necessary and given its own unique identifier. This is a very important part of any watershed delineation process. Below is a figure showing the flow accumulation raster (mersey_dem_flow_accumulation.tif) and the EA monitoring stations (mersey_EA_sites.shp). Here, we’re using the raster package to load the raster data, and the st_read() function from the sf package (Simple Features) to load the vector data. To add spatial data to our ggplot map, we are using the annotation_spatial() function which is designed for spatial geometries. Helpfully, this works across the main vector types; points, lines, and polygons can all be added using geom_sf(). For design reasons, we’ve modified the shape type as well as the fill, colour and size of the point data. # Loads flow accumulation raster using the raster and here packages mersey_accumulation &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_flow_accumulation.tif&quot;)) # Loads EA monitoring sites using the st_read function seed_points &lt;- st_read(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites.shp&quot;)) ## Reading layer `mersey_EA_sites&#39; from data source ## `C:\\Users\\44797\\Desktop\\Repo\\GEOG-70581\\data\\practical_2\\mersey_EA_sites.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 70 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 345751 ymin: 368724 xmax: 404810 ymax: 427178 ## Projected CRS: OSGB36 / British National Grid # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_accumulation, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = seed_points, shape = 21, fill = &quot;#FFFFFF&quot;, colour = &quot;black&quot;, size = 3) + # Adds vector layer theme_classic() + labs(fill = &quot;Log-transformed flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p Inspect the above code and implement it in your R script. At this map scale, it is quite difficult to assess the correspondence between the accumulation raster and the monitoring stations. To address this, add the following code before your ggplot2 call and update the layer_spatial() argument to include this new raster. Here, we are cropping our flow accumulation raster using indexing, extracting rows 632 - 645 and columns 540 - 578. There are, however, more sophisticated approaches to cropping (e.g. using the extent of other spatial layers) but this is reasonably efficient for now. # Crops the accumulation raster based on a defined extent (row and col indexes) mersey_accumulation_crop &lt;- crop(mersey_accumulation, extent(mersey_accumulation, 632, 645, 540, 578)) As the plot extent is determined by the extent of the feature/layer specified in the layer_spatial() argument, your map should now resemble the following: Inspect the map. You should notice that while some vector points sit directly on raster cells with high accumulation values, others are located just off, often by only one or two grid cells. These errors could be due to imprecise GPS coordinates, the resolution of the DEM, or limitations of the analytical approach (i.e. flow routing), particularly in areas of low lying topography. To ensure accurate watershed creation, the sites that lie off the stream network need to be correctly repositioned. While it is possible to rectify these errors manually i.e. by checking each point in turn and assigning new coordinates if necessary, this approach would be extremely time consuming with large datasets and could introduce unnecessary bias or human error. One common approach to reposition individual points is known as pour point snapping, which relocates points to the nearest stream or area of high accumulation, within a specified threshold distance. See Lindsay et al. (2008) for an overview of snapping approaches. Whitebox Tools provides two primary methods for snapping: Snap Pour Points, using the wbt_snap_pour_points function, which moves each point to the location coincident with the highest flow accumulation value within a specified distance; Jenson Snap Pour Points, using the wbt_jenson_snap_pour_points function, which moves each point the nearest stream grid cell within a specified distance following Jenson (1991). Of these, the Jenson Snap Pour Points approach (Lindsay et al. (2008)) is more sophisticated and has one clear advantage: when an initial outlet point is located on a stream cell, it will not be moved. For our EA dataset, which contains points of variable accuracy, this is beneficial because correctly placed points will retain their original positions, and only those incorrectly placed will be modified. In contrast, a standard snap pour points approach will often move all the points, even those which were located correctly! However, the Jenson approach requires additional inputs and utilises a streams raster file, rather than an SCA accumulation file. To calculate this, the overall process is as follows: Calculate the Number of Upslope Elements (NUE) using the wbt_d8_flow_accumulation function. Define the stream network using the wbt_extract_streams function. Reposition EA seed points using the wbt_jenson_snap_pour_points function. Save your script before continuing. 9.2.1.1 Number of Upslope Elements Run the wbt_d8_flow_accumulation tool, using the original filled DEM as the input file (pntr = FALSE) and saving with an appropriate name mersey_dem_D8_flow_accumulation_NUE.tif and in the correct location. Set the output type as cells (i.e. the number of upslope cells) and don’t log-transform values. 9.2.1.2 Stream network Next, we are going to extract the stream network based on the NUE raster created in Step 1. To extract streams, we first need to set the channelization threshold. This threshold represents the minimum area required to initiate and maintain a channel. Small threshold values will result in more extensive stream networks, while larger threshold values will extract only the largest streams. Unfortunately, there is no widely accepted rule for determining the channelization threshold, which will be unique for each landscape and dataset, and may vary due to raster resolution. As a result, experimentation is required! For our dataset, we want to ensure that each EA measurement site is located on or near a digitised stream, so setting a large channelization threshold and focusing on the largest streams might not be appropriate, as some measurement sites are located on minor streams in small catchments. As such, we need to set the channelization threshold sufficiently low to mitigate against this. Feel free to experiment with this threshold and evaluate its effects on the delineated stream network. For our analysis, a channelization threshold of 200 upslope grid cells would be appropriate. Using the output of Step 1, we are going to extract the stream network using wbt_extract_streams with a channelization threshold of 200 upslope grid cells (threshold = 200). Set the background value to 0 (zero_background = TRUE). As usual, save with an appropriate name (e.g. mersey_dem_streams_act200.tif) and in the correct location. Your output should resemble the following, whether raster cells corresponding to streams are coded as “1” and non-stream regions are coded as “0”: 9.2.1.3 Snapping Finally, we are going to reposition the EA seed points to match the stream network. Remember, seed points already located on the steam network will not be moved. Before we execute the tool, we need to decide on the Snap Distance, which is measured in the units of the input data and is the maximum distance that points can be moved to match the stream network. Our raster is measured in metres (m) with a grid resolution of 50 m2. Most points are incorrectly positioned by one to three grid cells (50 - 150 m). However, to ensure we include all points, we’ll set the snap distance to 500 m. Using the output of Step 2 (mersey_dem_streams_act200.tif) and the EA monitoring stations shapefile mersey_EA_sites.shp, run the wbt_jenson_snap_pour_points function, using a snap distance of 500 m (snap_dist = 500). Save with an appropriate name mersey_EA_sites_snapped.shp. As we’re using two inputs to a Whitebox function (rather than just one), it can be useful to specify inputs directly in R e.g. streams = mersey_dem_streams_act200.tif, pour_pts = mersey_EA_sites.shp, output = .... You can check the names of the inputs by reading the documentation, by hovering the mouse over the name of the function, or pressing the tab button. Run the following code to inspect the outputs of Jenson Snap Pour Points. The stream network is a Boolean (logical) raster, where stream cells = 1 (yellow) and non-stream cells = 0 (blue). The original pour points are shown in white, with the snapped points in red. # Loads streams raster using the raster and here packages mersey_streams &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_streams_act200.tif&quot;)) # Crops the streams raster based on a defined extent (row and col indexes) mersey_streams_crop &lt;- crop(mersey_streams, extent(mersey_streams, 632, 645, 540, 578)) # Loads original and snapped EA monitoring sites using the st_read function seed_points &lt;- st_read(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites.shp&quot;)) snapped_seed_points &lt;- st_read(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites_snapped.shp&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_streams_crop, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = seed_points, shape = 21, fill = &quot;#FFFFFF&quot;, colour = &quot;black&quot;, size = 3) + # Adds seeds layer annotation_spatial(data = snapped_seed_points, shape = 21, fill = &quot;#FB5858&quot;, colour = &quot;black&quot;, size = 3) + # Adds snapped seeds layer theme_classic() + labs(fill = &quot;Stream network ID&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p 9.3 Task 3: Watershed creation To finish the Week 10 class (Chapter 9), we are going to combine the previous outputs to create unique (non-overlapping) watersheds for each EA seed point. Use the wbt_watershed function with the D8 pointer raster (mersey_dem_D8_pointer.tif) and the snapped pour points vector (mersey_EA_sites_snapped.shp) as inputs. Save in the output/practical_2 directory as mersey_watersheds.tif. Your output should resemble the following: # Loads streams raster using the raster and here packages mersey_watersheds &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.tif&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_watersheds, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = snapped_seed_points, shape = 21, fill = &quot;#FB5858&quot;, colour = &quot;black&quot;, size = 3) + # Adds snapped seeds layer theme_classic() + labs(fill = &quot;Stream network&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p The watersheds are currently stored as rasters, but it is often easier to work with vectors, especially when dealing with attributes and spatial relationships (topology). Use the following code to convert your watersheds raster (.tif) to vector format (.shp) # Converts watershed to vector format (polygon) wbt_raster_to_vector_polygons(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.tif&quot;), here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.shp&quot;)) These delineated watersheds can now be used for further analysis. However, it is important that we carefully assess them for any errors. This typically requires in-depth knowledge to assess whether the correct watershed(s) has been extracted. In turn, the position of seed points is critical, as placing seeds points above/below stream confluences can have a major impact on the extracted watershed(s), as shown in Figure 4: Figure 4: The impact of seed point location on extracted watersheds. (A) In this example, rivers A and B are sourced from distinct watersheds but meet at a confluence (white circle). (B) When seeds points are placed above this confluence, each river has its own individual watershed. (C) When a seed point is placed at or below the confluence, these watersheds are combined as all water falling within both will ultimately drain to the confluence. Prior knowledge of the catchment is required to know if the correct watershed has been extracted. To finish, save your script and compare your outputs to those shown below. If there are any clear differences in your version, repeat the previous tasks and play close attention to the input settings. References "],["mersey_bonus.html", "Chapter 10 Mersey IV - Catchments 10.1 Task 4: Extracting catchment characteristics", " Chapter 10 Mersey IV - Catchments 10.1 Task 4: Extracting catchment characteristics We know from the lecture that river hydrochemistry can be influenced by a range of catchment characteristics, including land cover, soil type, bedrock geology, topography, and rainfall. Before we undertake statistical analysis, our next step is to extract these characteristics for each of the 70 catchments in the mersey_watersheds file (.shp), so we can relate these to the Environment Agency water quality data collected at each of the 70 monitoring sites. Information on these characteristics for the Mersey region can be obtained from the relevant files in the data pack, including: mersey_LC [categorical], based on LCM2000 data; mersey_HOST [categorical] i.e., Hydrology of Soil Types; mersey_bedrock [categorical], sourced from the British Geological Survey; mersey_dem_fill [continuous], a digital elevation model (50 m spatial resolution); mersey_rainfall [continuous], annual average rainfall (mm a-1). To simplify our analysis, I am going to walk you through the approach to calculate some of these variables, including the average of the continuous datasets (elevation, slope, aspect, rainfall). For the categorical datasets (e.g., percentage of the each of the land cover / soil / bedrock classes present), I have provided you with the summary values, stored in data/practical_2/mersey_EA_characteristics.csv. While extracting these values is an important part of the data cleaning-preparation process, it is time-consuming, requiring reclassification of each categorical raster (e.g., simplifying the number of classes and removing classes that are not present in the study area) and normalisation of the values based on catchment area. Given the focus of EMMC, this part has been removed to give you more time to focus on the statistical analysis and interpretation (as requested by previous student feedback), and to relate your results to key environmental concepts. If you are interested in the reclassification approach, I have provided some guidance in the final chapter here. 10.1.1 Calculating surface derivatives Before we can extract characteristics for each watershed, we need to produce some additional layers. Here, we are interested in the effects of elevation (the data from mersey_DEM_fill) and rainfall (mersey_rainfall), as well as topographic slope and aspect, both of which likely influence the path of water through the landscape. These two attributes (slope, aspect) are known as surface derivatives as they are calculated (derived) from the DEM. To produce slope and aspect rasters: use the wbt_slope and wbt_aspect functions, using the original filled DEM as the input data (mersey_dem_fill.tif) and using appropriate output names (e.g. mersey_dem_slope and mersey_dem_aspect). Your outputs should resemble the following: 10.1.2 Extracting continuous characteristics To begin: Load the mersey_watersheds.shp file into R (produced in Task 3) using the st_read function, storing in a variable called watersheds. Next, print out attribute names for the shapefile as follows: colnames(watersheds) ## [1] &quot;FID&quot; &quot;VALUE&quot; &quot;geometry&quot; For our analysis, the attribute of interest is VALUE, which contains the unique Environment Agency ID for each watershed. Importantly, this is also found in mersey_EA_chemisty.csv file. This will enable us to join the two datasets, populating the attribute table of the watersheds variable with the water quality measurements stored in the csv. To simplify this, use the following code to replace the column name VALUE with a new name Seed_Point_ID. The latter is used in the mersey_EA_chemisty.csv. # Replaces column name &#39;VALUE&#39; with &#39;SEED_Point_ID&#39; names(watersheds)[names(watersheds) == &#39;VALUE&#39;] &lt;- &#39;Seed_Point_ID&#39; You can re-use the colnames function to check it worked correctly: With this updated: We can now load the Environment Agency data using read.csv(), as shown in Chapter 3: # Loads csv using read.csv ea_data &lt;- read.csv(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_chemistry.csv&quot;)) and merge using the merge function: # Merge based upon matching Seed_Point_IDs watersheds_ea &lt;- merge(watersheds, ea_data, by = &quot;Seed_Point_ID&quot;) Use the head() function to inspect the first few rows of our new data frame. ## Simple feature collection with 6 features and 14 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 343660.8 ymin: 382963.7 xmax: 353552.6 ymax: 394555.6 ## Projected CRS: OSGB36 / British National Grid ## Seed_Point_ID FID EA_ID Group Ph SSC Ca Mg NH4 NO3 NO2 TON ## 1 1 54 1940214 Training 7.79 21.37 60.82 11.12 0.24 2.64 0.08 2.73 ## 2 2 42 1941025 Testing 7.79 33.52 75.45 17.52 4.46 3.24 0.11 3.35 ## 3 3 40 1941017 Training 8.55 11.69 58.54 20.50 0.25 0.83 0.02 0.84 ## 4 4 47 1941007 Training 7.71 34.06 96.83 46.33 0.24 3.65 0.07 3.73 ## 5 5 45 1941002 Training 8.08 70.81 141.98 85.85 0.40 4.47 0.05 4.52 ## 6 6 44 1941003 Training 8.12 34.00 174.49 86.27 0.21 2.69 0.06 2.75 ## PO4 Zn geometry ## 1 0.34 50.00 MULTIPOLYGON (((345159.5 38... ## 2 0.99 20.51 MULTIPOLYGON (((348107.1 39... ## 3 0.07 35.23 MULTIPOLYGON (((346108.7 39... ## 4 0.21 74.26 MULTIPOLYGON (((350505.1 39... ## 5 0.14 20.16 MULTIPOLYGON (((351854 3906... ## 6 0.15 18.27 MULTIPOLYGON (((352903.1 39... Save your script before continuing. Before we move on to extract our continuous derivatives (average elevation, rainfall, slope and aspect), it is worth noting that R variables can be removed from the environment as follows: # Removes object(s) from memory rm(mersey_dem) This can be useful if R is running slowly. To extract continuous derivatives, we are going to use the extract function from the raster package, which is described here. We’ll be using this function for each continuous dataset. It is also used for extracting categorical derivatives, as demonstrated here, so it’s important that you understand what it’s doing. Broadly, the function extracts values from a raster object at the locations of spatial vector data, where the value of interest is user-defined. For example, this could be the mean (e.g. the average elevation of a DEM within a vector polygon), the count (e.g. the number of cells within a vector polygon), or a minimum or maximum (e.g. the maximum elevation within a vector polygon), as shown in the figure below: Figure 5: Visual example of the extract function. In this example, our raster layer is the slope of the terrain in the Eskdale catchment, studied in Week 9, with values ranging from 0° (flat) to 76° (very steep). Our vector layer is a series of three \\(Polygons\\), covering three upland locations (other valid vector types include \\(Points\\) and \\(Lines\\)). Here, the extract function would return the average and minimum-maximum slope for \\(Polygons\\) \\(A\\) and \\(B\\) respectively, and the count (number of raster cells) for \\(Polygon\\) \\(C\\). To use the extract function, the raster dataset needs to be loaded into R. Load the digital elevation model into R and store with a sensible variable name e.g. mersey_dem: Now we can use the extract function as follows. This takes in both raster (mersey_dem) and vector input data (watersheds_ea), where the value of interest is determined by the fun parameter (i.e. a function). Here we are using an existing base R function (e.g. mean) but we can use user-defined functions as well. For example, fun=function(x, ...) length(x) would count the length (or number) of raster cells for each watershed, and min or max would return the minimum and maximum elevations, respectively. In our case, we just want to calculate the mean (average) elevation, so we can use the approach below: # Extracts raster values for each watershed, calculates mean (fun=mean), stores in attribute table ($average_elevation), with NA values removed (na.rm=TRUE) watersheds_ea$average_elevation &lt;- extract(mersey_dem, watersheds_ea, fun=mean, na.rm=TRUE) When you’re happy you understand the process, load the other continuous rasters into R with sensible variable names (e.g. mersey_rainfall, mersey_slope, mersey_aspect) and use the extract function to calculate the relevant attributes e.g. $average_rainfall, $average_slope, $average_aspect. Use the head() function to inspect the output. 10.1.3 Extracting categorical characteristics As noted above, I have produced some catchments characteristics for you, derived from mersey_LC, mersey_HOST and mersey_bedrock. If you want to learn about the approach, it is detailed here. This information is stored in mersey_EA_characteristics.csv and includes: Percentage of the each of the five land cover classes present: Arable % Heath % Grassland % Urban % Wetland % Percentage of the each of the four soil types present: Permeable % Impermeable % Gleyed % Peats % Percentage of the each of the three bedrock geology types present: Sands_and_Muds % Limestone % Coal % Load the above dataset using read.csv() and then merge this dataset with watersheds_ea (containing the continuous characteristics calculated above) using the merge function and Seed_Point_ID as the merge value. Again, use the head() function to check this worked as expected. To finish Task 4: Run the following code, which removes the geometry stored in the data frame (not required for subsequent analysis) and saves our dataframe as a comma-separated file. # Drops geometry attribute from watersheds_ea watersheds_ea &lt;- st_drop_geometry(watersheds_ea) # Writes data frame to comma-separated file write.csv(x = watersheds_ea, here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds_ea.csv&quot;), row.names=FALSE) To check your calculations: Run head(watersheds_ea), which should resemble the following: ## Seed_Point_ID FID EA_ID Group Ph SSC Ca Mg NH4 NO3 NO2 TON ## 1 1 54 1940214 Training 7.79 21.37 60.82 11.12 0.24 2.64 0.08 2.73 ## 2 2 42 1941025 Testing 7.79 33.52 75.45 17.52 4.46 3.24 0.11 3.35 ## 3 3 40 1941017 Training 8.55 11.69 58.54 20.50 0.25 0.83 0.02 0.84 ## 4 4 47 1941007 Training 7.71 34.06 96.83 46.33 0.24 3.65 0.07 3.73 ## 5 5 45 1941002 Training 8.08 70.81 141.98 85.85 0.40 4.47 0.05 4.52 ## 6 6 44 1941003 Training 8.12 34.00 174.49 86.27 0.21 2.69 0.06 2.75 ## PO4 Zn average_elevation average_rainfall average_slope average_aspect ## 1 0.34 50.00 21.51344 539.8778 0.4947966 94.18135 ## 2 0.99 20.51 39.95542 581.1832 1.2605843 205.45723 ## 3 0.07 35.23 66.75006 591.3545 1.9887265 195.48206 ## 4 0.21 74.26 44.82042 549.0123 0.5363760 140.63359 ## 5 0.14 20.16 43.72755 557.1751 1.6656744 120.61153 ## 6 0.15 18.27 49.89805 562.9039 1.1076461 118.44821 ## Arable_percent Heath_percent Grassland_percent Urban_percent Wetland_percent ## 1 23.881537 0 21.42407 49.46440 0 ## 2 44.563367 0 23.93578 20.94381 0 ## 3 2.862986 0 33.46967 35.71915 0 ## 4 36.419753 0 10.08230 51.23457 0 ## 5 62.289562 0 18.51852 19.19192 0 ## 6 27.402135 0 10.85409 43.77224 0 ## Permeable_percent Impermeable_percent Gleyed_percent Peats_percent ## 1 0.00000 0 64.90233 35.097669 ## 2 6.59207 0 84.23741 9.170518 ## 3 19.90457 0 80.09543 0.000000 ## 4 0.00000 0 100.00000 0.000000 ## 5 0.00000 0 100.00000 0.000000 ## 6 0.00000 0 100.00000 0.000000 ## Sands_and_Muds_percent Limestone_percent Coal_percent ## 1 89.09893 0 0 ## 2 100.00000 0 0 ## 3 100.00000 0 0 ## 4 100.00000 0 0 ## 5 100.00000 0 0 ## 6 100.00000 0 0 If you wish, it might be good idea to clean / simplify our R environment before commencing the statistical analysis. To do so, best practice is to restart R (Ctrl + Shift + F10 on Windows) or you can use the following code rm(list = ls()) to remove all objects from the workspace. While the latter isn’t a perfect solution (see here), it can be of value if used prudently. # Delete user-created objects from the global workspace rm(list = ls()) "],["mersey_four.html", "Chapter 11 Mersey IV - Statistical analysis 11.1 Task 5: Model building 11.2 Task 6: Model evaluation", " Chapter 11 Mersey IV - Statistical analysis 11.1 Task 5: Model building In this next task, we will compare the information about catchment characteristics with the water quality data collected at each of the 70 monitoring stations. To begin: Load the csv file created at the end of Task 4 (mersey_watersheds_ea.csv), saving to a new variable called watersheds_df: # Reads completed file from csv watersheds_df &lt;- read.csv(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds_ea.csv&quot;)) If you have any other variables in your R environment, these can be removed using rm(). This data frame should contain the following 10 water quality indicators for each watershed: pH: acidity/alkalinity; SSC: suspended solids concentration (mg l−1); Ca: calcium (mg l−1); Mg: magnesium (mg l−1); NH4: ammonium (mg-N l−1); NO3: nitrate (mg-N l−1); NO2: nitrite (mg-N l−1); TON: total oxidised nitrogen (mg-N l−1); PO4: phosphate (mg-P l−1); Zn: zinc (μg l−1). It should also contain the continuous derivatives (e.g. average elevation) and categorical derivatives (e.g. land cover percentage) for each watershed. Note: some of the calculated percentages may not add up to 100%. During the reclassification of the categorical datasets (land_cover, soils, bedrock), we focused on the macro-classes which make up the majority of the catchment and are known to have the greatest impact of river hydrochemistry (e.g. urban areas, farmland). While other land cover categories are found within each watershed (hence \\(total\\) \\(percent &lt; 100\\)), these typically account for only a small percentage of the total area and have a limited effect on the river environment. These categories have been excluded to simplify the analysis. 11.1.1 An introduction to linear models in R It is now time to examine the relationships between river water quality and catchment metrics. The key model outputs that are ultimately required for the assessment are: Regression equations relating each water quality variable (dependent variable; n = 10) to the important catchment characteristics (independent variables; n = 16). An assessment of model quality. The exact metrics / plots / tests are up to you! Remember, you don’t have to run every code block shown below, but you can do so if it would help your understanding. In the following instructions, I will guide you through some of the keys steps for one water quality indicator (Mg). You can replicate this approach and add further analysis (e.g., model evaluation) for the other nine indicators. The simplest way to run a linear regression in R is to use the lm() function (i.e., a linear model), an example of which is shown below, storing in an output called model: # Fit a linear model (ordinary least squares regression) model &lt;- lm(formula = Mg ~ average_elevation, data = watersheds_df) We have defined the data frame being used (data = watersheds_df) and the input variables from that data frame. This is achieved by including their column names, shown here: colnames(watersheds_df) ## [1] &quot;Seed_Point_ID&quot; &quot;FID&quot; &quot;EA_ID&quot; ## [4] &quot;Group&quot; &quot;Ph&quot; &quot;SSC&quot; ## [7] &quot;Ca&quot; &quot;Mg&quot; &quot;NH4&quot; ## [10] &quot;NO3&quot; &quot;NO2&quot; &quot;TON&quot; ## [13] &quot;PO4&quot; &quot;Zn&quot; &quot;average_elevation&quot; ## [16] &quot;average_rainfall&quot; &quot;average_slope&quot; &quot;average_aspect&quot; ## [19] &quot;Arable_percent&quot; &quot;Heath_percent&quot; &quot;Grassland_percent&quot; ## [22] &quot;Urban_percent&quot; &quot;Wetland_percent&quot; &quot;Permeable_percent&quot; ## [25] &quot;Impermeable_percent&quot; &quot;Gleyed_percent&quot; &quot;Peats_percent&quot; ## [28] &quot;Sands_and_Muds_percent&quot; &quot;Limestone_percent&quot; &quot;Coal_percent&quot; Input variables in the formula are separated by ~, where the variable to the left is the dependent variable (Mg) and the variable to the right is an independent variable (average_elevation). We can, however, include multiple independent variables to perform multiple linear regression. This is achieved as follows, where additional independent variables are separated by +: # Fits a linear model model &lt;- lm(formula = Mg ~ average_elevation + Grassland_percent, data = watersheds_df) We can then assess the model output using the summary function: summary(model) ## ## Call: ## lm(formula = Mg ~ average_elevation + Grassland_percent, data = watersheds_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.055 -5.932 -2.206 1.366 59.981 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.69098 4.02921 7.617 1.18e-10 *** ## average_elevation -0.05654 0.01389 -4.070 0.000126 *** ## Grassland_percent -0.12687 0.08066 -1.573 0.120473 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.97 on 67 degrees of freedom ## Multiple R-squared: 0.2964, Adjusted R-squared: 0.2754 ## F-statistic: 14.11 on 2 and 67 DF, p-value: 7.672e-06 For this set of independent variables, we have an R2 of 0.30 (Multiple R-squared: 0.2964) and a model p value of &lt; 0.01 (p-value: 7.672e-06). What is your opinion of the model performance? The model coefficients for the independent variables are described above, where * denotes p values &lt; 0.05 (95% probability) and ** denotes p values &lt; 0.01 (99% probability). As the coefficients are very small, they are presented in scientific notation. These can be converted to numeric (non-scientific) format using the following code: format(-2.096e-04, scientific = FALSE) ## [1] &quot;-0.0002096&quot; We can supply multiple values to the format function by creating a vector: format(c(-2.096e-04, -8.358e-06, ...) , scientific = FALSE) When you’re happy you understanding the formatting of the lm function, move on to the next section. 11.1.2 Training vs. Testing One limitation of the above approach is that our dataframe (watersheds_df) contains observations from all 70 EA monitoring stations. When performing statistical analysis, it is common practice to split any dataset into: a training subset, which is used to create the model(s). a testing subset, which is used to evaluate the model(s). Subsetting our data in this way allows models to be evaluated more rigorously. Many models perform well “in-sample” but poorly “out-of-sample” when evaluated against independent data (i.e. the testing subset). This is commonly referred to as over-fitting. Training and testing subsets are usually defined randomly, with an approximate ratio of 70:30 (although this varies). However, and to ensure reproducibility, this step has been completed for you: the watersheds_df dataframe contains a group variable denoting which monitoring sites belong to the training and testing subsets. Run the code above to create training and testing dataframes: # Extracts training dataset, comprising 50 observations (~70%) training &lt;- subset(watersheds_df, Group == &quot;Training&quot;) # Extracts training dataset, comprising 20 observations (~30%) testing &lt;- subset(watersheds_df, Group == &quot;Testing&quot;) Before you move on to the next section, can you think of any limitations of this approach? Hints: How important is the training-testing ratio? How are training-testing subsets created? 11.1.3 The full model An additional limitation of the above approach is that we have manually defined the independent variables of interest (average_elevation + Grassland_percent). For exploratory analysis, however, we may not know which are the most important variables. Perhaps there is a combination of independent variables which produces a better model fit (e.g. R2 &gt; 0.30)? As a first step, we could create a full model including all the relevant independent variables. This would include the normalised variables (e.g. Arable_percent, Heath_percent, …), but we’d want to exclude the categorical counts (e.g. Arable, Heath, … influenced by catchment size), as well as any IDs or geometry variables. In general, we are only interested in testing the continuous derivatives (column names starting with average_) and the normalised categorical derivatives (column names ending in _percent). Rather than typing out the columns of interest manually, we are going to use the select function from the dplyr package to achieve this: # Creates a vector of column names, including only those which contain &quot;average&quot; or &quot;percent&quot; factors &lt;- colnames(watersheds_df %&gt;% dplyr::select(contains(c(&quot;average&quot;, &quot;percent&quot;)))) # Prints to console factors ## [1] &quot;average_elevation&quot; &quot;average_rainfall&quot; &quot;average_slope&quot; ## [4] &quot;average_aspect&quot; &quot;Arable_percent&quot; &quot;Heath_percent&quot; ## [7] &quot;Grassland_percent&quot; &quot;Urban_percent&quot; &quot;Wetland_percent&quot; ## [10] &quot;Permeable_percent&quot; &quot;Impermeable_percent&quot; &quot;Gleyed_percent&quot; ## [13] &quot;Peats_percent&quot; &quot;Sands_and_Muds_percent&quot; &quot;Limestone_percent&quot; ## [16] &quot;Coal_percent&quot; Run the above code. Note, the formatting of dplyr::select may be slightly confusing but it is necessary because there is also a select function in the MASS package. Here, we are telling R to use select from dplyr. Using this vector of column names, we are going to create new data frames (called training_df and testing_df) containing only the independent variables of interest for the training dataset: training_df &lt;- training[factors] testing_df &lt;- testing[factors] Run the above code and use head() to inspect the results. To run a linear regression using these data, we need to ensure our dependent variable of interest is also present in the dataframe, which can be achieved using cbind. Note that we have specified variable as the name of the column containing the Mg values, but this can be changed if you desire: # Column bind (cbind) the dependent (n=1) and independent variables (n=16) model_df &lt;- cbind(variable = training$Mg, training_df) Run the above code which creates a new dataframe (model_df) containing the dependent variable we want to model (Mg) and the full set of independent variables for our training set. Next, run a new lm() model using all the independent variables, which can be acheived using the equation variable ~ ., and inspect the output. ## ## Call: ## lm(formula = variable ~ ., data = model_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.906 -4.577 -0.762 2.907 45.248 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85.884874 612.423593 0.140 0.8893 ## average_elevation 0.073763 0.082405 0.895 0.3772 ## average_rainfall -0.032572 0.032796 -0.993 0.3278 ## average_slope 0.959319 2.989139 0.321 0.7503 ## average_aspect -0.056961 0.071344 -0.798 0.4303 ## Arable_percent 0.745315 0.345833 2.155 0.0385 * ## Heath_percent -0.008754 0.459029 -0.019 0.9849 ## Grassland_percent -0.111924 0.347421 -0.322 0.7494 ## Urban_percent 0.060574 0.366645 0.165 0.8698 ## Wetland_percent -0.093537 0.405536 -0.231 0.8190 ## Permeable_percent -2.510005 5.944348 -0.422 0.6756 ## Impermeable_percent -2.682134 5.919185 -0.453 0.6534 ## Gleyed_percent -2.306212 5.941057 -0.388 0.7004 ## Peats_percent -2.609482 5.974192 -0.437 0.6651 ## Sands_and_Muds_percent 1.896667 1.506477 1.259 0.2169 ## Limestone_percent 1.849674 1.584516 1.167 0.2514 ## Coal_percent 1.958293 1.508695 1.298 0.2033 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.95 on 33 degrees of freedom ## Multiple R-squared: 0.6147, Adjusted R-squared: 0.4279 ## F-statistic: 3.291 on 16 and 33 DF, p-value: 0.001857 Question: How has model performance changed? Do you think adding all our independent variables to the model is a good approach? 11.1.3.1 Problems with the full model Including all the independent variables has improved our model performance on the training set. However, there are some important issues we need to consider: While adding all the independent variables has increased the overall model fit (R2), we don’t know which variables are actually important. Perhaps only one or two of these have a demonstrable and/or high magnitude effect on the dependent variable? In the example above, only one coefficient is marked as significant at \\(p &lt; 0.05\\) (Arable_percent). In general, we prefer models with the minimum number of parameters (independent variables). These models require fewer assumptions, less intensive data collection, can be applied more confidently to new data sets/locations, and are often easier to interpret. This principle of model parsimony is based upon Occam’s Razor: “other things being equal, simpler explanations are generally better than more complex ones”. Another, and perhaps more important issue, is that we not yet assessed the performance of the model on the testing data set. Good performance on the training set is a promising sign, but we also need to evaluate how the model performs on unseen and independent data. To achieve this, we can use the predict() function which predicts new values for Mg based on our model and using catchment characteristics from the testing set (newdata = testing): # Predict Mg values based upon linear model, saving to testing dataframe testing$predicted_Mg &lt;- predict(linear_model, newdata = testing) Run the above code, which will create a new column in the testing dataframe called predicted_Mg, using our model as input (linear_model). To evaluate the model performance, create a scatter plot of measured Mg vs. predicted Mg values, using the skills developed in the Introduction to R. I would recommend adding a 1:1 line to the plot, as follows: geom_abline(intercept = 0, slope = 1, lty = \"dashed\"). You could also add a linear regression using the measured and modelled values, although see below for important disucssion. This can be acheived using the following: geom_smooth(method = \"lm\", se = FALSE, colour=\"#FF953C\"). Use this plot to assess the model performance on the testing set. Can you see evidence of systematic and/or random error? While this plot allows for visual evaluation of the model, it is also useful to produce metrics for a quantitative evaluation, such as root-mean-square error (RMSE), normalised root-mean-square-error (nRMSE), or R2, which you’ve covered earlier in the unit. It is also tempting at this stage to produce a regression between the measured and modelled values, such as the following: # Regression (measured vs. modelled) prediction_model &lt;- lm(formula = Mg ~ predicted_Mg, data = testing) summary(prediction_model) ## ## Call: ## lm(formula = Mg ~ predicted_Mg, data = testing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.965 -4.264 -3.068 2.177 24.748 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4309 2.1783 3.870 0.00112 ** ## predicted_Mg 0.2560 0.0906 2.826 0.01119 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.145 on 18 degrees of freedom ## Multiple R-squared: 0.3073, Adjusted R-squared: 0.2688 ## F-statistic: 7.986 on 1 and 18 DF, p-value: 0.01119 While this looks like a sensible approach, this is actually incorrect because this is evaluating the deviations from the modelled line-of-best-fit (shown in \\({\\color{#FF953C}{orange}}\\) above), and not the 1:1 line. An easy mistake! To illustrate, below is a plot of some data from Alexander et al. (2015), a highly recommended paper on \\(R^2\\). As you can see from the plot, there is a clear correspondence between the observed and predicted values. If we run a linear regression on these data (lm(formula = observed ~ predicted...), then \\(R^2=1\\) i.e., a straight line can be drawn through the data points. This is misleading, however, because the data are far from the 1:1 line. The correct \\(R^2\\) is actually \\(-1.26\\), which is very different! ## ## Call: ## lm(formula = observed ~ predicted, data = df) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.001274 -0.002371 0.001560 -0.002084 0.006710 -0.009373 0.004283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.022030 0.018586 -1.185 0.289 ## predicted 0.852532 0.002171 392.738 2.03e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.005749 on 5 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 1.542e+05 on 1 and 5 DF, p-value: 2.031e-12 To produce the correct \\(R^2\\), we need to do so manually using the following equation: \\[R^2=1-\\frac{RSS}{TSS}\\] Here \\(RSS\\) is the residual sum of squares i.e., deviations of predicted values (e.g., predicted_Mg) from actual empirical values (e.g., Mg), and can be calculated in R as follows: # Residual sum of squares i.e., the size of the residuals from the model rss &lt;- sum((testing$Mg - testing$predicted_Mg)^2) \\(TSS\\) refers to the total sum of squares i.e., the dispersion of the data points from the average value, or as Alexander et al. (2015) summarise: “the size of the residuals for a null model where all predictions are the same, i.e. the mean value.” \\(TSS\\) involves: calculating the difference between each measured value and the mean of the measured values, for example using the mean() function, squaring the differences, calculating the total sum of the squared values. For example, if these were the measured values and the mean: \\[[5.3, 4.6,6.5,8.5,10.1]\\] \\[mean=7.0\\] the differences would be as follows: \\[[-1.7,-2.4,-0.5,1.5,3.1]\\] the squared differences would be: \\[[2.89, 5.76, 0.25, 2.25, 9.61]\\] resulting in: \\[TSS = 20.76\\] Write code to calculate \\(TSS\\) using the guidance above, and use the \\(RSS\\) example for assistance. If you get stuck, ask for help! The ratio of \\(RSS\\) to \\(TSS\\) accounts for the unexplained variation in the dependent variable that is not accounted for by the model. In turn, using \\(1-\\frac{RSS}{TSS}\\) returns our explained variation (\\(R^2\\)), which for the measured-modelled Mg data, produces: ## [1] -2.309345 This has produced an unexpected negative value, similar to the example data from Alexander et al. (2015) above. How can we explain this? Numerically, this has occurred because \\(RSS &gt; TSS\\) i.e., the error (residuals) in the model (\\(RSS\\)) are greater than the variation of the observations around their mean (\\(TSS\\)). But what does that actually mean? When \\(R^2&lt;0\\), this means that the model predictions are worse than simply using the mean of the dependent variable (Mg) as a predictor i.e., a horizontal line at the Mg mean would have lower residuals than our predictions. Not good! If testing R2 is lower than training R2, we would describe our models as overfit and/or poorly generalised i.e., the model performs well on the training set, but poorly on the testing set. In most cases, there may be some overfitting, but not extreme (e.g., training \\(R^2=0.7\\), testing \\(R^2=0.4\\)). In our example above, we have extreme overfitting (\\(R^2&lt;0\\)) One solution to prevent overfitting is to reduce model complexity i.e., the number of independent variables. While this will generally reduce training performance, this can lead to improved performance on independent data (testing). Moreover, by reducing the number of independent variables, we can focus on the important processes (model parsimony), can make fewer assumptions, and can more easily apply our model to new situations. How can we acheive this? 11.1.4 Variable selection strategies Determining which variables to include/exclude from a model is a very difficult problem, which has resulted in many different variable selection strategies. Common approaches include expert opinion and/or theory, stepwise regression, implemented in MASS, partial least squares (PLS) regression, implemented in PLS, as well as elastic net methods and ridge regression, both implemented in glmnet. You may want to explore some of these approaches if you use regression analysis for your dissertation. We are going to use a very popular and well-respected approach, known as Least Absolute Shrinkage and Selection Operator (LASSO), implemented in glmnet. Published by Robert Tibshirani in 1996, the original work has &gt;65,000 citations! There are lots of online resources available to learn more about LASSO, including the original paper. 11.1.5 LASSO in theory As explored in the lecture, LASSO is slightly different to ordinary least squares (OLS) regression, which produces a model which minimises the sum of the squared residuals, as shown below. In ordinary least square regression, the line of best fit minimizes the sum of the squared (vertical) residuals between the data points and the line itself While this approach produces the best fit to the training data, this can often result in overfitting, and poor performance on testing data, as demonstrated with our analysis of Mg. Instead, LASSO minimises the sum of the squared residuals, but also adds an additional penalty term known as lambda (λ), which is applied to the slope coefficient of the model (\\(λ×slope\\)). While OLS minimises the sum of the squared residuals, LASSO returns the model with the minimal overall error (sum of the squared residuals + \\(λ×slope\\)). The aim of this approach is to add a small amount of bias to our model. While this generally results in a worse fit for the training data, this often leads to improved performance on the testing data. In LASSO, an additional penalty term (λ) is used to reduce overfitting This approach is known as regularisation, which in effect is reducing the sensitivity of our model to the training data. A model that is highly tuned to the training data often won’t perform well in other situations. In contrast, models containing fewer variables and/or additional bias (\\(λ×slope\\)) are often more generalisable. One key aspect we haven’t discussed in depth is lambda (λ). This refers to the size of the penalty we apply to the slope coefficient i.e., how much should we penalise steeper slope coefficients in our model? Note that the shallower the slope of the model, the less sensitive it is to the training data. If λ = 0, then no penalty would be applied, and the results of LASSO would be identical to OLS. As we increase λ, the slope of the model will get closer and closer to 0 (i.e., the model is very insensitive to the training data) How then should we choose a suitable lambda for λ? 11.1.6 Cross-validation for λ A common approach to choose λ (and other model parameters) is known as cross-validation. There are many different methods that can be used here, such as k-fold cross-validation or leave-one-out cross-validation (LOOCV), but the underlying approach is similar and is summarised in the figure below. In k-fold cross-validation, the training set is split into k-folds, with one fold left out for each model run. The model performance on this fold is used to evaluate performance, as with the testing set. We’ve already split our data into training-testing subsets. Cross-validation repeats the process, splitting the training data into different groups (or sets or folds). For example, 10-fold cross-validation would split the training data into ten folds. The model would then be trained using a dataset comprising nine of the ten folds, with the final fold left out and used for model evaluation. This process would then be repeated, leaving a different fold out for each model run. When all combinations have been tested, the final model might be the average of the k model runs or the best performing one. One disadvantage of cross-validation is that the training data will be split randomly into different folds. If you run your analysis again, the data within each fold will be different and your model output might also differ! One way to account for this is to repeat cross-validation many times (e.g., \\(×100\\) or \\(×1000\\)), which generally produces more stable coefficients. This is the approach we’ll use in the following sections. Before we move on the use LASSO in R, make sure you understand the approaches outlined above. If you can answer the following, move on! How does LASSO differ from OLS? What is λ? How does cross-validation work? 11.1.7 LASSO in practice In R, we can run a LASSO regression via glmnet, which is described in full here. The main function, which confusingly is also named glmnet(), takes in dependent and independent variables in a similar way to lm(), but as vectors or matrixes, rather than data frames, as shown below. One key input of glmnet() is the alpha parameter. For our analysis, we are going to keep \\(alpha = 1\\), which denotes that we are performing LASSO regression. The package glmnet() is designed to enable a more complex approach known as elastic net regression, published by Zou and Hastie (2005). This method incorporates both LASSO and a similar technique known as ridge regression. While elastic net is an excellent approach, combining the strengths of both LASSO and ridge, it is much more complex. # Independent variables (x) x &lt;- as.matrix(training_df) # Dependent variable (y) y &lt;- training$Mg # A vanilla Lasso regression (alpha = 1) lasso &lt;- glmnet( x = x, y = y, alpha = 1 ) Run the above the code and print the regression results using print(lasso). This output shows a range of potential models, which vary in terms of the number of non-zero coefficients (Df) (i.e., the number of independent variables in the model), the percentage of deviance explained (%dev), and the value of λ i.e., the parameter that controls the strength of the penalty. ## ## Call: glmnet(x = x, y = y, alpha = 1) ## ## Df %Dev Lambda ## 1 0 0.00 11.5300 ## 2 1 6.77 10.5100 ## 3 2 12.61 9.5720 ## 4 2 19.30 8.7220 ## 5 2 24.86 7.9470 ## 6 2 29.47 7.2410 ## 7 2 33.30 6.5980 ## 8 2 36.47 6.0120 ## 9 2 39.11 5.4780 ## 10 2 41.30 4.9910 ## 11 2 43.12 4.5480 ## 12 2 44.63 4.1440 ## 13 2 45.88 3.7750 ## 14 2 46.92 3.4400 ## 15 2 47.79 3.1340 ## 16 2 48.51 2.8560 ## 17 2 49.10 2.6020 ## 18 2 49.60 2.3710 ## 19 3 50.05 2.1600 ## 20 4 50.49 1.9690 ## 21 5 50.99 1.7940 ## 22 5 51.43 1.6340 ## 23 6 51.85 1.4890 ## 24 6 52.30 1.3570 ## 25 6 52.67 1.2360 ## 26 6 52.97 1.1260 ## 27 6 53.23 1.0260 ## 28 7 53.61 0.9352 ## 29 7 54.32 0.8521 ## 30 7 54.90 0.7764 ## 31 7 55.38 0.7075 ## 32 7 55.78 0.6446 ## 33 7 56.11 0.5873 ## 34 8 56.41 0.5352 ## 35 8 56.66 0.4876 ## 36 8 56.87 0.4443 ## 37 8 57.05 0.4048 ## 38 9 57.27 0.3689 ## 39 9 57.47 0.3361 ## 40 9 57.64 0.3062 ## 41 9 57.78 0.2790 ## 42 10 57.94 0.2542 ## 43 10 58.16 0.2317 ## 44 10 58.33 0.2111 ## 45 10 58.48 0.1923 ## 46 10 58.60 0.1752 ## 47 10 58.70 0.1597 ## 48 12 58.79 0.1455 ## 49 12 58.87 0.1326 ## 50 12 58.93 0.1208 ## 51 12 58.99 0.1101 ## 52 12 59.03 0.1003 ## 53 12 59.07 0.0914 ## 54 12 59.10 0.0833 ## 55 13 59.14 0.0759 ## 56 13 59.16 0.0691 ## 57 13 59.19 0.0630 ## 58 14 59.21 0.0574 ## 59 15 59.24 0.0523 ## 60 14 59.37 0.0476 ## 61 14 59.42 0.0434 ## 62 15 59.60 0.0396 ## 63 15 59.86 0.0360 ## 64 15 60.08 0.0328 ## 65 15 60.27 0.0299 ## 66 15 60.43 0.0273 ## 67 15 60.56 0.0248 ## 68 15 60.66 0.0226 ## 69 15 60.76 0.0206 ## 70 15 60.83 0.0188 ## 71 14 60.90 0.0171 ## 72 14 60.95 0.0156 ## 73 14 61.00 0.0142 ## 74 14 61.03 0.0130 ## 75 14 61.07 0.0118 ## 76 14 61.09 0.0107 ## 77 15 61.11 0.0098 ## 78 15 61.14 0.0089 ## 79 16 61.15 0.0081 ## 80 16 61.17 0.0074 ## 81 16 61.18 0.0068 ## 82 16 61.20 0.0062 ## 83 16 61.21 0.0056 ## 84 16 61.22 0.0051 ## 85 16 61.23 0.0047 ## 86 16 61.23 0.0042 ## 87 16 61.24 0.0039 ## 88 16 61.25 0.0035 ## 89 16 61.25 0.0032 ## 90 16 61.26 0.0029 ## 91 16 61.26 0.0027 ## 92 16 61.26 0.0024 ## 93 16 61.27 0.0022 ## 94 16 61.27 0.0020 ## 95 16 61.27 0.0018 ## 96 16 61.27 0.0017 ## 97 16 61.28 0.0015 ## 98 16 61.28 0.0014 ## 99 16 61.28 0.0013 ## 100 16 61.28 0.0012 There are lots of models here, with differing values for λ, so how do we pick one? This is where cross-validation comes in, splitting the data into k-folds, and repeating the process until we find the best value for λ. Run the code below to run LASSO with cross-validation, using 10-folds. # Lasso regression (alpha = 1), with 10-fold cross-validation lasso_cv &lt;- cv.glmnet( x = x, y = y, alpha = 1, n_folds = 10 ) # Plot results plot(lasso_cv) You can use plot() to visualise the results of cross-validation, which shows how the model performance varies with the number of non-zero coefficients and the value for λ. Note, your plot might look different to the one above, but don’t panic… Marked with dashed lines are two candidate models that the user might want to select as the final model: \\(λ_{min}\\) (lambda.min) i.e., the model with the minimum mean cross-validated error (y-axis) \\(λ_{1se}\\) (lambda.1se) i.e., the model with the largest λ within 1 standard error of \\(λ_{min}\\). \\(λ_{min}\\) is therefore the “best” model, based upon the mean cross-validated error. However, a user might prefer to select \\(λ_{1se}\\). As this has a larger penalty value (λ), this will guard better against overfitting, while typically resulting in fewer independent variables. This is an improvement, because we now have just two candidate models to select from, either \\(λ_{min}\\) (minimum error, but potential for overfitting), or \\(λ_{1se}\\) (larger error, but more parsimonious). However, there is still an issue to be solved. Run cv.glmnet() again with the same settings and keep plotting the results. What do you notice? If this has worked as expected, your plots and final models should have changed each time. Sometimes this difference might be quite subtle, but occasionally you will obtain a very different result. As discussed above, this is due to the random splitting of the data during cross-validation. Our relatively small dataset is quite sensitive to this split, whereas we might expect to see less sensitive behaviour for datasets consisting of hundreds or thousands of measurements, or where there are high magnitude or unambigious links between the dependent and independent variables. To stabilise our results, we will repeat cross-validation 100 times, as shown below. This uses a for loop to iterate the chosen number of times, perform cross-validation, and then store the \\(λ_{1se}\\) value for each iteration. I am using \\(λ_{1se}\\) for model parsimony here, but you can explore the effects of using \\(λ_{min}\\) for the assessment. For example, if using \\(λ_{1se}\\) returns no important predictors (except for the intercept), then the model is underfit, and \\(λ_{min}\\) might be preferable. # Number of iterations n_repeats &lt;- 100 # Empty numeric `variable` to store lambda values lambda_values &lt;- numeric(n_repeats) # Iterate n_repeat times for (i in 1:n_repeats) { # Perform LASSO regression (alpha = 1), with 10-fold cross-validation lasso_cv &lt;- cv.glmnet( x = x, y = y, alpha = 1, n_folds = 10 ) # Store lambda.1se lambda_values[i] &lt;- lasso_cv$lambda.1se } Run the code above and inspect the results. For example, you could plot of a histogram of the λ values using hist() and print out the median() λ. Thinking back to earlier in the course, why would using the median λ be a better choice than mean? The median λ based on my repeated (n=100) 10-fold cross validation is as follows: ## [1] 6.011583 Your λ value should be identical or very similar. If so, it can be used to produce a final model for this dependent variable: # Final LASSO model, without cross-validation final_model &lt;- glmnet(x, y, alpha = 1) # Print model coefficients, using our selected value for λ coef(final_model, s = median(lambda_values)) Run the above code to run the LASSO regression and print out the model coefficients. ## 17 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s=6.011583 ## (Intercept) 8.78973962 ## average_elevation . ## average_rainfall . ## average_slope . ## average_aspect . ## Arable_percent 0.32765141 ## Heath_percent . ## Grassland_percent . ## Urban_percent . ## Wetland_percent . ## Permeable_percent . ## Impermeable_percent . ## Gleyed_percent 0.06928212 ## Peats_percent . ## Sands_and_Muds_percent . ## Limestone_percent . ## Coal_percent . Our model coefficients are as follows: intercept = 9.62080435 Arable_percent = 0.29694731 Gleyed_percent = 0.05816263 Coefficients are important because they are used in regression equations, which can then be used for prediction. The general format for a regression equation is as follows: \\[ y = a + (b_1 \\cdot x_1) + (b_2 \\cdot x_2) + (b_n \\cdot x_n) \\] where a is the constant (intercept) value, and b is the coefficient of x. For our Mg model above, we can define our regression equation (presented using sensible data precision) as: \\[ Mg = 9.62 + 0.30 \\cdot Arable \\: percent + 0.06 \\cdot Gleyed \\: percent \\] Well done! You have now calculated a regression which links the dependent variable (Mg) to the important independent variables, in this case the percentage of arable land cover and gleyed soils. For your assessment, we would like you to explain the regression results, linking to hydrological processes and literature. For example, why are \\(Arable \\: percent\\) and \\(Gleyed \\: percent\\) present in the model? Think about specific sources of pollution, transport pathways, types of flow… 11.2 Task 6: Model evaluation Having created a statistical model, it is necessary to evaluate its performance. Comparison plots of measured vs. modelled values are one common way to assess model quality, alongside other metrics such as root-mean-square error (RMSE), normalised root-mean-square-error (nRMSE), Q-Q plots, or histograms of model residuals. You may want to explore some of these for the assessment. To predict values based on our LASSO model for the testing set, we can use the predict() function, taking the model variable (final_model) as the input, and using our selected λ value: # Predict Mg values based upon LASSO model and our selected λ, using &#39;testing&#39; as input testing$predicted_Mg &lt;- predict(final_model, newx = as.matrix(testing_df), s = median(lambda_values)) Run the above code block to predict Mg concentrations in the testing dataset, using the LASSO model produced from the training dataset. Note that you can also run this process for the training set as well e.g., predict(final_model, newx = x, s = median(lambda_values)). This allows you to calculate metrics (e.g., R2) for both the training and testing set. Other metrics include RMSE or nRMSE (plus others introduced earlier in the course) using your own code or additional packages (e.g. Metrics); \\[ RMSE = \\sqrt{mean(measured\\:values - modelled\\:values)^2} \\] Plots of measured vs. modelled values (as well as Q-Q plots and histograms) can be created in ggplot2. Here is an example: Does the regression line match the 1:1 line? Is there any evidence of under- or over-prediction? Are there any outliers? What types of errors can you identify? You could also assess this relationship statistically, but be careful to calculate \\(R^2\\) correctly (\\(1-\\frac{RSS}{TSS}\\)), rather than lm(formula = Mg ~ predicted_Mg.... How well does our Mg model perform on the testing dataset, based on the above graphs/statistics? Is out-of-sample performance comparable to in-sample performance? To finish the practical and to prepare for the assessment: Replicating the above approaches, calculate regression equations using LASSO for all 10 water quality indicators (NO2, pH, SSC, Ca, Mg, NH4, NO3, TON, PO4, Zn). Evaluate the performance of the models. The exact approach is up to you, but I would encourage you to be ambitious. Some common approaches includes \\(R^2\\) for the training and the testing sets, Q-Q plots, histograms of residuals, error metrics (e.g., \\(RMSE\\), \\(NRMSE\\)). Advice: when constructing your models, think carefully about both over- and under-fitting. For example, in the Mg example above, I used \\(λ_{1se}\\) to favour model parsimony. However, models can be underfit (e.g., no important predictors, except for the intercept), so in some cases \\(λ_{min}\\) might be preferable. when evaluating model performance, don’t worry if performance is not “good”, however you define this. Predicting water quality is challenging and we don’t always fully understand the links between catchment characteristics and river hydrochemistry. Moreover, if the models were all “good” (e.g., high \\(R^2\\), low \\(RMSE\\)), you would have very little to discuss in your report! "],["Hints.html", "Handy Hints", " Handy Hints This chapter contains a few handy hints which may be useful if you’re having any issues loading R packages or running Whitebox tools. Dealing with errors Errors are an inevitable aspect of coding, irrespective of how experienced you are! As we gain programming experience, these errors increase in difficulty from initially simple problems, such as incorrect file paths or errors in code syntax, to the more complex. Being able to deal with errors is therefore an essential programming skill. My top tips for fixing errors are: Read the error message (red text in the R console). If this appears, it usually means that something has gone wrong! Error messages can vary in quality, but at the very least they should tell you: where the error was encountered i.e. which line of code failed. some indication as to why the error occurred e.g. No such file or directory Use help(). If you’re unsure of how a particular function works, then you can use the in-built function help(), with the name the of the function as the input, e.g., # Call help() for the &#39;st_read&#39; function from sf package help(&quot;st_read&quot;) This will give you valuable information on the function, including a description, inputs, arguments, and examples. If the error message doesn’t provide a simple solution to the problem, and help() hasn’t helped, then the next step should be to work through your code step-by-step, testing each line or code block to ensure it is working correctly (usually using print() statements). To illustrate this, here is some code which has failed: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;) check.packages(packages) # Fill depressions in a DEM wbt_fill_depressions(dem = here(&quot;data&quot;, &quot;practical_2&quot;, &quot;dem_10m.tif&quot;), output = here(&quot;output&quot;, &quot;practical_2&quot;, &quot;dem_10m_fill.tif&quot;)) # Loads raster filled_dem &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;dem_10m_fill.tif&quot;)) # Plots using ggplot g &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(filed_dem, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Elevation (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot; + theme(legend.position = &quot;top&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot; na.value = NA) # Shows plot g I would first inspect the error message (No such file or directory) but if I didn’t understand it, I would then troubleshoot by: First running the check.packages(packages) function. Were all the packages installed correctly i.e. TRUE? Then checking the wbt_fill_depressions() function. Does this produce an error? Does it correctly produce an output raster in the intended location? Does the output look sensible? Plotting the raster might be useful here to check. Then running the raster() function to load the raster into R. Here the code would fail and I could then start to investigate further: Are there any syntax errors? e.g. use of commas, correct number of brackets, spelling errors? Are the inputs defined correctly? In the above example, the code fails because the raster package is trying to load dem_10m_fill.tif from data/practical_1 when it was actually saved in output/practical_1. Running the code again might produce another error, in which case I would repeat the above process, checking the code step-by-step until the error can be located and fixed. Can you spot any errors in the code below? There are three to find… # Plots using ggplot g &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(filed_dem, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Elevation (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot; + theme(legend.position = &quot;top&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot; na.value = NA) TLDR: the best way to fix errors is to be systematic in your approach. Work your way through the code step-by-step, checking it is producing the intended outputs at each stage - just because the code doesn’t produce an error, it doesn’t mean it is producing the output that you want! Whitebox functions vs. the raster package So far, there has been some confusion regarding the role of whitebox functions, the raster package and ggplot2. Each of these has a unique role, as visualised below, while analysis needs to be conducted in the correct order to avoid errors. Analysis structure. [1] Create output files using whitebox [2] Load outputs into R using raster [3] Plot them using ggplot To illustrate this, below we have an example of the wbt_d8_pointer function, which creates a D8 pointer file based on an input digital elevation model (dem): # Creates a D8 pointer file (.tif) at the output location, based on the input dem wbt_d8_pointer(dem = here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_dem_fill.tif&quot;), output = here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_D8_pointer.tif&quot;)) This uses the here package for locating the input file (mersey_dem_fill.tif), which is stored in data/practical_2, and for saving the output, which is stored as mersey_dem_D8_pointer.tif in output/practical_2. If this runs successfully (make sure to carefully check file paths, file names, use of commas), then an output raster will be created at the specified location. Crucially, this does not load the raster into the R environment, so any attempt to plot the raster using ggplot2 would fail. To achieve this, a raster must be first loaded into the R environment as follows: # Loads D8 pointer raster using the raster and here packages mersey_pointer &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_D8_pointer.tif&quot;)) Again, we have determined the input file location using here package (output/practical_2/mersey_dem_D8_pointer.tif) and stored the raster as an object in the R environment called mersey_pointer. With this stored, it can then be plotted using ggplot, setting layer_spatial to the mersey_pointer object name: # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_pointer, aes(fill = stat(band1))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) p TLDR: for subsequent analysis, make sure to adhere to the following structure: Create output files using whitebox functions. Load those output files into R using the raster package (stored as objects) Plot the R objects using ggplot2 Any deviation from this order (e.g. plotting without loading using raster, loading files that have not yet been created using whitebox) will cause errors! R Projects Make sure to initialise an R project for your work (Section 5.4) and ensure this is created for your GEOG70581 directory. This should resemble the following, with the R project file and a separate R script for the Eskdale and Mersey Basin practicals: File paths Remove spaces in directories or file paths. As a rule, avoid using spaces when naming files or folders as this can cause issues, particularly when coding: “P:/Environmental Monitoring Concepts/GEOG70581/…” Bad “P:/Environmental_Monitoring_Concepts/GEOG70581/…” Good Output files Make sure to inspect output files. When you run a command (e.g. a Whitebox function), check that it produces the intended output (e.g. a raster file (.tif) in the output directory). If there is an error or the output hasn’t been created, subsequent code will fail. Packages If you’re having difficulty loading the raster or sf packages on a University managed computer, this is due to multiple package installs. Go to the packages window, identify the older version of each package, and tick the check box to load it into your R library. Code structure While you will not be assessed on the quality of your code (you do not have to submit your code for the assessment), remember that well-formatted code (with comments and good structure) is easier to read and understand and will be less prone to error. Inspect the code below: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) This code includes comments for each main code block, line spaces to distinguish different parts of the code, and is written in a logical order (e.g. first loading packages, then loading/selecting data, running tools). Any non-essential code has been removed e.g. additional calls to install.packages() or library(). Errors with R projects and the here package If you’re working on a university-managed computer, you may have encountered errors while trying to use whitebox functions, such as wbt_fill_depressions(), or accessing files using relative file paths via the here package. This problem actually originates with our use of R projects (described here). R projects are designed to make accessing files simple and to ensure our analyses are self-contained and portable. They do this by automatically setting the working directory in RStudio. You can test this by using the getwd() function, which should print your working directory to the console. If the R project is working, this should set your working directory to the directory where your R project is saved e.g., an R project file stored in C:/Users/Bob/Documents/GEOG70581 should have an identical working directory. Unfortunately, R projects were not working as intended on university-managed computers in 2023/24: the working directory was not being automatically updated. In most cases, the working directory was stuck at P:/, even if the R project is saved elsewhere! This was a completely new problem for 2023/24, but it is possibly due to changes in the waycomputers are managed after the recent cyber incident. This can lead to difficulties accessing files using here, but also utilising whitebox functions, even when inelegant absolute file paths are used. In both cases, R is looking in the wrong place, either for the data files, or the WBT executable. This issue should be solved for 2024/25, but if it arises again, there are two options to fix this: The simplest approach is to start from scratch, downloading the course data, and creating a new R project in the same directory. If successful, it should resemble the following: The next step is to set the working directory manually using the setwd() function (see below for an example). This should update the working directory to the correct location, such as C:/Users/Bob/Documents/GEOG70581 or P:/GEOG70581 depending on where you data are saved, and should be done before installing packages. The practical steps can then be following as normal and the whitebox functions should work as intended. Try running the code below, making sure to update the path to the correct working directory. # FIRST, set your working directory manually setwd(---YOUR PATH GOES HERE---) # e.g., &quot;C:/Users/Bob/Documents/GEOG70581&quot; # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;sf&quot;, &quot;ggspatial&quot;) check.packages(packages) # Initialise whitebox tools executable whitebox::install_whitebox() # Check whitebox is in the library and the version library(whitebox) wbt_version() # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g If you want to continue with an existing directory/R Project, then you will need to set the working directory manually (setwd()) and set the origin for the here package, as shown below: # Set the working directory manually setwd(&quot;C:/Users/Bob/Documents/GEOG70581&quot;) # UPDATE TO YOUR DIRECTORY # Manually set the origin for the &#39;here&#39; package here::i_am(&quot;practical-1-eskdale.R&quot;) # UPDATE IF A DIFFERENT FILE NAME USED The above solutions have been tested on a range of university-managed computers, working on both the P: and C: drive. For other installation issues, please contact me on my @manchester email. For other R issues, please post on the forum. Catchment characteristics from categorical data In Task 4, we calculated some important catchment characteristics from the continuous datasets (e.g., average_elevation, average_rainfall, average_slope, average_aspect). To simplify the analysis, and to give you more time to focus on statistical analysis and interpretation, I dealt with the categorical datasets (land cover, soil type, bedrock type) and provided the summary values in mersey_EA_characteristics.csv. The following instructions outline how this was completed. While you do not need to run this code as part of the practical, it is useful for you to understand the process, especially if you wanted to use this or similar techniques in future e.g., for your dissertations. Reclassification The categorical files mersey_LC (based on LCM2000 data), mersey_HOST (Hydrology of Soil Types) and mersey_bedrock contain many different detailed classes, some of which are not applicable to the study region. Therefore, the datasets need to be simplified by aggregating some classes and omitting unnecessary classes. For example, there are 26 classes in LCM2000 data, each with a unique numeric identifier. These were simplified into the following five macro-classes, alongside their numeric identifiers: Arable = 41, 42, 43; Heath = 91, 101, 102; Grassland = 51, 52, 61, 71, 81; Urban = 171, 172; Wetland = 111, 121. This was achieved using the fct_collapse function from the forcats package, using the following approach, which can you replicate: First, load the land cover raster into R as normal: # Loads land cover raster land_cover &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_LC.tif&quot;)) Because our raster is categorical (rather than continuous), it makes sense to convert the data format to a factor. In R, these are used to represent categorical variables. Next, convert the land cover raster to a factor: # Converts the land cover raster to a factor, overwriting the original variable land_cover &lt;- as.factor(land_cover) To assess the values stored in the land_cover raster, use the unique function, which should produce the following: ## [1] -9999 0 11 21 41 42 43 51 52 61 71 81 91 ## [14] 101 102 111 121 131 161 171 172 191 211 212 221 As not all categories are applicable to our study area, we are next going to create a data frame of the land cover categories of interest. Inspect the code below. Many of the code elements should be familiar to you. We are creating a vector called categories using the c() function, which contains all the classes of interest c(41, 42, 43, ...)). Next, we have converted that to a data frame using as.data.frame() and assigned a column name (ID). When you understand what is happening, add to your script and run. # Categories of interest categories &lt;- as.data.frame(c(41, 42, 43, 91, 101, 102, 51, 52, 61, 71, 81, 171, 172, 111, 121)) colnames(categories) &lt;- &quot;ID&quot; If you want to inspect the output, you can use head(categories) to print out the first 6 rows: ## ID ## 1 41 ## 2 42 ## 3 43 ## 4 91 ## 5 101 ## 6 102 Using this new data frame, we are going to create a new column called name, which corresponds to the name of the land cover class (e.g. \\(Arable = 41\\)) # Collapse categories into groups based on ID categories$name &lt;- fct_collapse(as.factor(categories$ID), &quot;Arable&quot; = c(&quot;41&quot;, &quot;42&quot;, &quot;43&quot;), &quot;Heath&quot; = c(&quot;91&quot;, &quot;101&quot;, &quot;102&quot;), &quot;Grassland&quot; = c(&quot;51&quot;, &quot;52&quot;, &quot;61&quot;, &quot;71&quot;, &quot;81&quot;), &quot;Urban&quot; = c(&quot;171&quot;, &quot;172&quot;), &quot;Wetland&quot; = c(&quot;111&quot;, &quot;121&quot;)) Inspect the above code. The syntax is reasonably complex, but you should understand what is happening if you inspect the output: # Prints categories data frame categories ## ID name ## 1 41 Arable ## 2 42 Arable ## 3 43 Arable ## 4 91 Heath ## 5 101 Heath ## 6 102 Heath ## 7 51 Grassland ## 8 52 Grassland ## 9 61 Grassland ## 10 71 Grassland ## 11 81 Grassland ## 12 171 Urban ## 13 172 Urban ## 14 111 Wetland ## 15 121 Wetland Finally, we can use this updated data frame to replace (or substitute) values in the land cover raster (i.e. \\(41, 91, ...\\)) with the land cover class it represents. In this case, values are stored numerically (i.e. \\(Arable =1,Heath =2, ...\\)). One way to achieve this is using the subs function from the raster package. Inspect the following code, which substitutes (reclassifies) the raster layer and saves to a new raster (.tif) using the writeRaster function. # Substitutes raster values with new categories land_cover_classified &lt;- subs(land_cover, categories) # Write to new raster writeRaster(land_cover_classified, here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;), overwrite = TRUE) When you understand it, run the code, load the new raster into R and plot. Use the code below to visualise the reclassified raster, taking note of the manual fill aesthetic (scale_fill_manual): # Loads land cover raster using the raster and here packages mersey_land_cover &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;)) # Manually defined colours for land cover categories # (1=Arable, 2=Heath, 3=Grassland, 4=Urban, 5=Wetland) colours &lt;- c(&quot;1&quot; = &quot;#D1F5D8&quot;, &quot;2&quot; = &quot;#87C292&quot;, &quot;3&quot; = &quot;#498955&quot;, &quot;4&quot; = &quot;#FFA888&quot;, &quot;5&quot; = &quot;#4575B4&quot;) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_land_cover, aes(fill = factor(after_stat(band1)))) + # Adds raster layer theme_classic() + labs(fill = &quot;Land cover class&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_manual(values = colours, na.value = NA, # Updated fill aesthetic labels=c(&quot;Arable&quot;, &quot;Heath&quot;, &quot;Grassland&quot;, &quot;Urban&quot;, &quot;Wetland&quot;)) + # Manual legend labels theme(legend.position = &quot;top&quot;) p Using the methodology outlined above, this process can be repeated for the soil type (mersey_HOST) and bedrock geology rasters (mersey_bedrock). Make sure to use a consistent approach to file naming e.g. mersey_HOST_reclass.tif and mersey_bedrock_reclass.tif. There are 29 classes in HOST, each with a unique numeric identifier. Reclassify these into the following four new classes: Permeable = 1, 3, 4, 5, 6, 7, 15, 16; Impermeable = 12, 17, 18, 19, 21, 22; Gleyed = 8, 9, 13, 24; Peats = 10, 11, 14, 25, 26, 27, 29. Utilising the order above, the values of the output raster should be as follows: Permeable = 1 Impermeable = 2 Gleyed = 3 Peats = 4 There are 34 bedrock geology classes in Mersey Basin region, each with a unique numeric identifier. Reclassify into the following three new classes listed below: Sands_and_Muds (sands and muds) = 5, 16, 18, 24, 28, 34; Limestone = 10, 11, 17, 19; Coal = 9, 15, 22. Utilising the order above, the values of the output raster should be as follows: Sands_and_Muds = 1 Limestone = 2 Coal = 3 When complete, save your script before continuing. You should have three reclassified rasters in your output directory i.e., mersey_LC_reclass.tif,mersey_HOST_reclass.tif,mersey_bedrock_reclass.tif`. Extracting-normalising surface derivatives In the next phase, we want to count the number of cells for each class per watershed i.e., number of arable cells, number of grassland cells, number of limestone cells. If we know the total number of cells per watershed, we can calculate the proportion i.e., arable %, grassland %, limestone %. First, ensure the reclassified land cover, soil type and bedrock rasters are loaded into R and stored with sensible variable names (e.g. land_cover, soils, bedrock). # Load raster data land_cover &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;)) soils &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_HOST_reclass.tif&quot;)) bedrock &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_bedrock_reclass.tif&quot;)) Next, we are going to use the extract function again, but this time returning the count of each category (e.g. Arable, Heath, Grassland, Urban, Wetland) for each watershed area, as shown here: # Load watersheds (vector, sf, VALUE column is important) and renames watersheds_ea &lt;- st_read(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.shp&quot;)) # Extract land cover counts (5 classes so levels = 1:5) land_cover_classes &lt;- extract(land_cover, watersheds_ea, fun=function(i,...) table(factor(i, levels = 1:5))) Rather than trying to decipher the code straight away, copy to your script, run and use head() to inspect the output, which should be as follows. This may take a little while (~30 seconds on a i7 computer with 16 Gb of RAM): head(land_cover_classes) ## 1 2 3 4 5 ## [1,] 18 41 2779 88 0 ## [2,] 28 73 4727 223 21 ## [3,] 16 108 1333 0 221 ## [4,] 90 61 1544 669 0 ## [5,] 0 90 1008 13 221 ## [6,] 16 49 1230 6 18 We have produced a data frame with 5 columns (representing the 5 land cover classes) and 70 rows (representing the 70 watersheds), where the row-column values represent the number of raster cells corresponding to each land cover class. This is based upon a user-defined function, incorporating the table function from the data.table package. Important: as the land cover dataset contains 5 classes, the function splits the underlying data into five groups using levels = 1:5. This needs to be updated when applying to the soils and bedrock datasets. While this information is useful, it complicates our statistical analysis and our comparison between watersheds, as there are a different number of cells per watershed, reflecting their varying sizes. To convert these to percentages, there are a couple of possible approaches. The first is to use the rowSums function, which is available in base R and (as the name suggests) calculates the sum of each row within a matrix or dataframe. Note that we use the form base:: to specify we are using the base package, as rowSums is also the name of a function in other packages, such as raster, see here. You can test the code below, which calculates the land cover percentage (proportion * 100) # Land cover proportion as a function of the total number of land cover cells (rowSums), converted to a percentage (* 100) land_cover_classes &lt;- land_cover_classes / base::rowSums(land_cover_classes) * 100 The above approach calculates the percentage per class (arable, heath, grassland, urban, wetland) as a function of the total number of cells in those five classes. In our reclassification we focused on the macro-classes which are known to have the greatest impact of river hydrochemistry (e.g. urban areas, farmland). However, other land cover categories are found within each watershed, but these typically account for only a small percentage of the total area and are assumed to have a limited effect on the river environment. While these categories have been excluded to simplify the analysis, this does mean that the total number of cells in the watershed will be greater than the number of cells in the five macro classes, which will influence our percentage calculation. To reflect this, we might want to calculate the class percentage as a function of the total watershed area. This can be completed using the extract function, this time using a custom function (fun=function(x, ...) length(x)) to count the number of cells within each watershed: # Total number of cells within each watershed land_cover_count = extract(land_cover, watersheds_ea, fun=function(x, ...) length(x)) # Calculate proportion using sweep, convert to percentage land_cover_classes &lt;- sweep(land_cover_classes, 1, land_cover_count, FUN = &quot;/&quot;) * 100 To improve the readability of the data frame, update its column names as follows: colnames(land_cover_classes) &lt;- c(&quot;Arable_percent&quot;, &quot;Heath_percent&quot;, &quot;Grassland_percent&quot;, &quot;Urban_percent&quot;, &quot;Wetland_percent&quot;) Use head() to inspect the output: head(land_cover_classes) ## Arable_percent Heath_percent Grassland_percent Urban_percent Wetland_percent ## [1,] 0.5584859 1.272107 86.22401 2.7303754 0.000000 ## [2,] 0.5217067 1.360164 88.07527 4.1550214 0.391280 ## [3,] 0.8311688 5.610390 69.24675 0.0000000 11.480519 ## [4,] 3.5074045 2.377241 60.17147 26.0717069 0.000000 ## [5,] 0.0000000 5.579665 62.49225 0.8059516 13.701178 ## [6,] 1.1644833 3.566230 89.51965 0.4366812 1.310044 Replicate this approach for the soil type and bedrock datasets, using sensible variable names (e.g. soils_classes and bedrock_classes), remembering to update the extract function used (i.e. levels = 1:4 for the soil data and levels = 1:3 for the bedrock data). When complete, you should have the following data frames in your R environment: watersheds_ea: containing the water quality measurements, the spatial areas and the continuous derivatives; land_cover_classes: containing the proportion for each land cover class; soils_classes: containing the proportion for each soil type; bedrock_classes: containing the proportion for each bedrock type; To simplify your R environment, remove all other variables using the rm() function. To merge the remaining files, we can use cbind(), which binds data frames together based on their columns: # Combines watersheds data frame with proportions watersheds_ea &lt;- cbind(watersheds_ea, land_cover_classes, soils_classes, bedrock_classes) "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
