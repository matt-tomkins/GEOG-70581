[["index.html", "Chapter 1 Introduction 1.1 Practical outline 1.2 Objectives 1.3 Schedule", " Hydrological analysis in R Chapter 1 Introduction 1.1 Practical outline In this set of practicals, well be using R, RStudio and WhiteboxTools to perform hydrological analysis. In Chapters 2 and 3, well install and get to grips with the required programs. In Practical 1 (Chapters 4 - 6), which uses data for the Eskdale watershed, well introduce you to some of the key approaches, including DEM pre-processing, calculating flow parameters and comparing flow algorithms. In Practical 2 (Chapters 7 - 11), well utilise these approaches to investigate the water quality of the Mersey Basin. This forms the basis for your assessment. Note: the relevant chapters will be made available as we progress through the semester, hence why Practicals 2 and the assessment information are not currently visible. Practical 2 will be released prior to the Week 10 lecture. 1.2 Objectives After completing these practicals, you will: have gained experience with new tools (e.g. R, RStudio, WhiteboxTools); become familiar with hydrological approaches used to simulate overland flow from digital elevation models (DEMs); utilised statistical analysis to investigate the relationships between water quality indicators and catchment characteristics; improved your understanding of the hydrological processes influencing water quality. 1.3 Schedule These practicals will be completed across Weeks 8 to 12 as follows: Week 8: Introductory lecture (~1 hr) Complete Chapters 1 - 3: An introduction to R (~1 hr) Weeks 9: Lecture on Hydrological and Geomorphological models (~1 hr) Complete Chapters 4 - 6: Eskdale practical (~2 hr) Weeks 10 - 12: Lecture on Water Quality Modelling (~1 hr) Complete Chapters 7 - 11: Mersey practical (~8 hr) The assessment, based upon Practical 2 (Mersey), is due 05/01/2023 (14:00). "],["Installation.html", "Chapter 2 Installation 2.1 R 2.2 R Studio 2.3 Course materials", " Chapter 2 Installation During this practical you will be using WhiteboxTools, an open-source GIS software package for hydro-geomorphic applications. While it contains much of the functionality found in a standard GIS software package, it also includes novel functionality specific to hydrological analysis. The tools are also computationally efficient (i.e. fast!), taking advantage of parallel processing and other advancements in computing power. Finally, the tools are developed for cross-platform use, so can be accessed through code (e.g. R, Python) or other GUIs (e.g. QGIS, ArcGIS) if required. To run WhiteboxTools functions (WBT), well be using R as a front-end interface. This will allow us to access and run all of the relevant hydrological functions from WBT, while utilising the statistical and visualisation capabilities of R. 2.1 R If youre working on a university-managed computer (in HBS 2.2, HBS 2.88 or Mansfield Cooper 2.01) or accessing the course via Citrix, both R and RStudio (described below) should already be installed or can be accessed through the Software Centre. To Install R on a personal computer: Open an internet browser and go to https://www.r-project.org/. Click the download R link in the middle of the page under Getting Started. Select a CRAN location (a mirror site) and click the corresponding link e.g. the site provided by Imperial College London. Click on the Download R for Windows link, or the relevant link for your operating system (macOS, Linux). Click on base and then Download R [Version Number] for Windows. The current version is 4.2.2. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.2 R Studio While R is now installed, it is good practice to use an integrated development environment (IDE) to access and run R functions. The most well known IDE for R is RStudio, which includes a console, higher quality graphics, as well as tools for plotting, history, debugging and workspace management. Note: R may be used without RStudio, but RStudio cannot be used without R. To Install RStudio on a personal computer: Go to https://www.rstudio.com/products/rstudio/download/ and click on Download RStudio for Windows. Installers for macOS and other operating systems are available below. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.3 Course materials All the data required for this sets of practicals are available on Blackboard under Course Content  Week 8 to 12. Download the folder, save it to an appropriate location and extract the contents (unzip). The directory structure is as follows: GEOG70581 Data Practical_1 Practical_2 Images Practical_1 Practical_2 Output Practical_1 Practical_2 All the required data files for Practicals 1 and 2 are stored in the associated sub-directory within Data. As we move through the practicals, well be creating a number of additional GIS files and we encourage you to save these within the corresponding sub-directory within Output. Finally, maps and figures will be required for the assessment. These can be exported directly from R and stored in Images. "],["Intro_to_R.html", "Chapter 3 A (brief) introduction to R 3.1 Overview 3.2 Loading R 3.3 Objects 3.4 Data structures 3.5 Scripts 3.6 Loading packages 3.7 Loading data 3.8 Plotting 3.9 Formative task", " Chapter 3 A (brief) introduction to R 3.1 Overview The following chapter provides a basic introduction to coding in R. If you already have experience with coding, perhaps from Spatial Ecology or Understanding GIS, then the following content should be familiar to you. For those of you with little or no experience with code, the content builds from the simple (using R as a calculator) to the complex (loading packages and data, plotting) and will equip you with the skills needed to complete Practicals 1 (Eskdale) and 2 (Mersey) You should aim to work through this Chapter by the end of Week 8, making sure you understand the concepts and code before moving on to the next sub-section. To test your understanding, there is a short formative task (non-assessed) at the end of this chapter. This covers many of the techniques covered here, as well as unfamiliar ones, so you may have to do some independent research to complete it. Ill post the solution on the Monday after the class (22/11/2021) so you can check your working. 3.1.1 What is R? R was originally developed in the 1990s for statistical data analysis. While it is still optimized for statistical analysis and data visualization, it has been utilised for a much wider range of applications. As a high-level programming language, it is less complicated and more user friendly than lower-level languages (e.g. C, C++, Java, Rust), but this comes at the expense of being slightly slower to execute. Importantly, R is free and open-source and its core functionality can be extended using packages. These are available through the Comprehensive R Archive Network (CRAN) and allow the user to perform a range of tasks (e.g. reading and writing files, rendering images), without the requirement to delve into the underlying source code. While there are a myriad of free online resources available to help you learn R, including R for Data Science and YaRrr! The Pirates Guide to R, as well as paid options from Data Camp, this chapter and the subsequent practicals will equip you with all the skills required to complete the course. 3.1.2 Why code? For those of you familiar with desktop GIS (e.g. ArcGIS, QGIS) or other statistical programs (e.g. Excel, SPSS), the transition to using code can be a frightening experience! However, there are a several reasons why it is worth persevering with. First, and with career progression in mind, the ability to code is becoming increasingly important, particularly for those of you who want to progress down GIS or environmental career paths. Getting to grips with R and understanding the fundamentals of coding will equip you to learn new skills and new coding languages which may be required by a future employer (e.g. Python, MATLAB, Julia, Ruby, ). Second, performing your analysis using code ensures that your outputs are reproducible. If you provide someone else with the original data files and the corresponding code i.e. a list of operations carried out by the computer (e.g. read data, modify, plot, perform statistical test, save), they should be able to reproduce your outputs exactly. Third, code is repeatable. In standard GIS software (for example), most functions allow you to perform a single task on a single data file e.g. clipping a digital elevation model to the geometry of a shapefile. By comparison, if that same functionality can be implemented in code, it can be applied as easily to 10 data files, or 1000, or 10,000. Finally, code is easily modifiable. Code can be tweaked, adapted, or completely rewritten to produce the desired outputs. In R, a wide range of functions are already available in packages, but you can write your own functions or combine existing functions to suit your analysis. This functionality is often not the case in other software, which are generally more complicated to modify and typically provide Black Box solutions, where we can inspect the inputs and outputs, but typically have limited understanding of the steps in between. Black Box programming 3.1.3 A quick note on the practicals As we progress through this course, make sure to read the content carefully. However, you do not have to run every bit of code in this document, although have a go where you feel it would help your understanding. If I explicitly want you to do something, I will write an instruction that looks like this: This is an instruction that tells you something to either think about, or do. 3.1.4 Dealing with errors Before we load R and take our first steps towards coding excellence, it is worth noting that errors are an inevitable aspect of coding. Irrespective of how good you are, you will spend more time fixing your code (i.e. debugging) than writing it! Luckily, when R encounters an error, it will try and help you out. For example, shown below is some simple R code. Here, we are tying to create a new object called a, which is the product (+) of object b and the numeric value 1: a &lt;- b + 1 ## Error in eval(expr, envir, enclos): object &#39;b&#39; not found In this example, the code fails and prints the error message object 'b' not found because the object b does not exist! We could solve this by defining b beforehand as follows: b &lt;- 3 a &lt;- b + 1 a ## [1] 4 These error messages are important and contain helpful information - do not ignore them! If youve struggled to resolve the problem, you should then Google it (normally by just typing R followed by the error message into Google), e.g. R Error in eval(expr, envir, enclos): object 'b' not found. Another useful source of information is StackOverflow which is a public collection of coding questions and answers. If youve come across a challenging error, theres a high probability that someone else has also encountered the same error, and there may be a solution already available. Understanding, interpreting and fixing error messages is a key programming skill, so read the error messages carefully and use the above resources (Google, StackOverflow). If you need any additional help, staff and TAs will be happy to help during the practicals. 3.2 Loading R Now that we understand what R is, why its useful and what to do when something goes wrong, lets begin. Open RStudio. The RStudio user-interface should resemble the following, although the exact arrangement of the windows may differ from that below: This contains the following primary windows, which are described below: In the console window, commands can be typed and results returned. For example, typing the following command into the console: 1 + 2 produces: ## [1] 3 Copy and paste the above code into the console and press Enter to run. Does it produce the correct result? In the Workspace window, you can see all the objects that you have created in the current R session (Environment tab; currently empty) as well as a record of the commands youve used (History tab; 1 + 2). At its most basic, R can operate as a powerful calculator. We can add +, subtract -, multiply * and divide /, take the exponent ^, calculate the square root sqrt() or the logarithm of a number log10(), or melt our computers by using the factorial() function. Operators can also be combined to produce more complicated mathematical functions. 3.3 Objects One of the most important things to know about in R is the variable or object. In this set of practicals, we use the term object, although variable is also used commonly across different programming languages. Objects are containers that you can store values in, and then refer to again later. R creates objects for you automatically, so all that is required is to name it and then use the assignment operator &lt;- to assign a value to it. As a general rule (and for historical and other complicated reasons), use &lt;- for assignment, as shown below, and dont use =: b &lt;- 3 Run the above code. This assigns the value of 3 to an object named b: You should now see that under the Workspace window, the object b has been added to the Environment tab: The object b can now be called again by the user. Run the following in the console. Does it work as expected? b + 5 There are number of benefits to using objects. For example, take the following: a &lt;- 5 b &lt;- 10 result &lt;- (sqrt(a) + log(b))^a + 2*b result # Typing the name of an object (&#39;result&#39;) prints it in the Console Window ## [1] 1945.905 This is a relatively complicated mathematical formula! It uses a square root sqrt() and logarithm log() function, as well as multiplication * and powers ^. We could replicate this result by substituting each value of a for 5 and each value of b for 10, but this would be time consuming and prone to error, and would be very frustrating if we wanted to re-calculate the result for a = 6 and b = 11! Using objects allows us to use and modify values multiple times within the code. Modify the code above to test for a = 6 and b = 11. Compare your answer with a colleague in class or on Zoom. Did you get the same result? Using objects is also important for repeatability. Lets say we want to calculate the sum of the squares from 1 to 5 i.e. \\(1^2 + 2^2 + 3^2 + 4^2 +5^2\\). We could write this numerically in R as follows: 1^2 + 2^2 + 3^2 + 4^2 + 5^2 This produces the correct result: ## [1] 55 However, what if wanted to perform this calculation for all the numbers from 1 to 50 i.e. \\(1^2 + 2^2 + 3^2 + ... + 50^2\\)? What if we wanted to cube each value (i.e. \\(n^3\\)), rather than squaring? Either change would require a great deal of manual editing, with lots of potential for error. By comparison, using objects allows us to loop (or iterate) through a series of calculations. The code below creates a object called result and loops through a sequence of numbers using seq() to perform the above calculation, where the object i is updated each iteration. # Initialises an object called result, with a value of 0 result &lt;- 0 # Loops through a sequence from 1 to 5 in increments of 1 (i.e. 1, 2, 3, 4, 5) for(i in seq(from = 1, to = 5, by = 1)){ # Squares each number (i) and adds to result result &lt;- result + i^2 } # Prints object &#39;result&#39; to the Console Window result ## [1] 55 Run the above code in the console. Try to modify the code to perform the calculation for 1 - 50; this should equal \\(42925\\) At this stage, dont worry about understanding the syntax of the code above. The important thing to remember is that using objects allows to us to perform more complex and repeatable analyses. 3.3.1 Object names When defining a object name, you cant use spaces or characters that are not a-z, A-Z or 0-9. As a general rule, objects in R should be written in snake case (as opposed to upper case or lower case) where all words are in lower case and are separated by underscores (_). All of these are valid object names in snake case: snake_case a_really_long_snake_case_name snake s Other popular naming conventions include camelCase, UPPER_CASE or hyphen-case (among many others). Pick one and use it consistently. 3.3.2 Object types Each object will have a particular type, which specifies what kind of a value it can hold. Whilst this is handled automatically for you by R, it is important that you understand what the different types are. Here are some common examples: # Character i.e. a string of letters, numbers and/or characters a &lt;- &quot;Bill&quot; # Numeric i.e. a number with decimals b &lt;- 3.567 # Integer c &lt;- 3 # Logical i.e. TRUE or FALSE d &lt;- TRUE Run the above code. If successful, your workspace should now include the objects a, b, c, d. Some data types are compatible e.g. # Numeric + integer b + c ## [1] 6.567 But others are not! # Numeric + character b + a ## Error in b + a: non-numeric argument to binary operator Can you understand the error message above? Why does b + a produces an error? 3.4 Data structures In the previous examples, each object has consisted of a single element. This could be a name (bill), a decimal number (3.567), an integer (3), and so on. However, R can store data in a range of different structures, some of which you will encounter as we progress through the course. 3.4.1 Vectors One of the simplest structures is the vector, which contains multiple elements of the same type. These are typically created using the command c(): # Creating a vector (v) of the numbers 1 - 5 v &lt;- c(1, 2, 3, 4, 5) v ## [1] 1 2 3 4 5 As a vector must have elements of the same type, R will try and coerce elements to the same type: # Creating a mixed vector (m) containing numeric (1,2), character (&quot;Bill&quot;, &quot;Ben&quot;) and logical elements (TRUE) m &lt;- c(1, 2, &quot;Bill&quot;, &quot;Ben&quot;, TRUE) m ## [1] &quot;1&quot; &quot;2&quot; &quot;Bill&quot; &quot;Ben&quot; &quot;TRUE&quot; In the above example, the numeric and logical elements have been coerced to characters, as signified by these elements being enclosed by quotation marks \" \". We can also create vectors programmatically, rather than manually defining each element. For example, we can generate sequences using seq() or repetitive sequences using rep(): # Creating a sequence from 1 to 5, in increments of 1 s &lt;- seq(from = 1, to = 5, by = 1) s ## [1] 1 2 3 4 5 # Creating a vector of length 5, consisting of the value 10 r &lt;- rep(10, times = 5) r ## [1] 10 10 10 10 10 3.4.2 Data frames Other common R data structures include matrices, lists and data frames. The data frame is the most commonly used structure for tabular data and will be familiar to those of you with experience in Microsoft Excel. Here is an example data frame, which is created by combining four existing vectors of the same length (name, height, nationality, bereft_of_life): # Create numeric, character and logical vectors name &lt;- c(&quot;John&quot;, &quot;Eric&quot;, &quot;Michael&quot;, &quot;Graham&quot;, &quot;Terry&quot;, &quot;Terry&quot;) height &lt;- c(1.96, 1.85, 1.78, 1.88, 1.75, 1.73) nationality &lt;- c(&quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;American&quot;, &quot;British&quot;) bereft_of_life &lt;- c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE) # Combines into a data frame df &lt;- data.frame(name, height, nationality, bereft_of_life) df ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 5 Terry 1.75 American FALSE ## 6 Terry 1.73 British TRUE Run the above code in your R console Well be working with data frames throughout the practicals, so it will be useful to know that we can access data frame elements in a variety of different ways. We can access individual columns of a data frame using the $ operator, for example: # Extract the &quot;name&quot; column df$name ## [1] &quot;John&quot; &quot;Eric&quot; &quot;Michael&quot; &quot;Graham&quot; &quot;Terry&quot; &quot;Terry&quot; We can also extract using indexing, which requires us to provide the row and columns indexes in the following format: df[row.index, column.index] # Extract the element at row 3, column 4 df[3,4] ## [1] FALSE # Extract rows 1 to 4, all columns df[1:4,] ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE # Extracts all rows, the second and third columns df[,2:3] ## height nationality ## 1 1.96 British ## 2 1.85 British ## 3 1.78 British ## 4 1.88 British ## 5 1.75 American ## 6 1.73 British We can also extract rows and columns based on the values within the data frame, for example using the subset() function. This can be used to select and exclude observations as follows: # Extract data frame rows where height is more than or equal to (&gt;=) 1.8 m. subset(df, height &gt;= 1.8) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 4 Graham 1.88 British TRUE # Extract data frames rows where nationality is not equal to (!=) American subset(df, nationality != &quot;American&quot;) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 6 Terry 1.73 British TRUE # Extract data frames rows where bereft_of_life is equal to (==) FALSE subset(df, bereft_of_life == FALSE) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 5 Terry 1.75 American FALSE Using the above code as an example, extract all the rows where name is equal to Terry. Assign the result to a new object. You can calculate the number of rows using the function nrow(). Did you get the following answer? ## [1] 2 The sections above have given you a crash course in some of the fundamentals of R. Learning to code (or learning a new code syntax for those of your familiar with Python, MATLAB, Java, ) is similar to learning a new language. Dont worry if it seems complicated! You can always refer back to these instructions if you get stuck, or ask for guidance. To finish this Chapter, well provide a brief overview of some other key skills, including writing R scripts, loading data and packages, and plotting. This should prepare you for the formative task. This is not an exhaustive list of R functions, and well cover other functionality in Practicals 1 and 2 (e.g. loading GIS raster and vector data, creating maps and figures, performing statistical tests). However, the R functionality outlined here will cover all the steps required for the assessment and should equip you with the core skills required to progress further with R in the future e.g. as the basis for your dissertation. 3.5 Scripts So far weve been typing commands into the console. This works as expected; we can print results, store objects in the workspace and create plots in the plots window. However, it is often more convenient to store our code in a script, which is simply a file containing a set of commands and comments. For example, a single R script could contain commands to load data, perform statistical analysis, and output the results to a new file. This can be run in a single keystroke, which is much simpler than typing each command in one after another. To create a new R script: Navigate to File, New File and R Script. This should open a new window, which is an untitled R script: To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. learning_R) One of the easiest ways to run an entire script is to use Ctrl + Shift + Enter. However, you can also run aspects of a script by clicking on the individual line you want to run and using Ctrl + Enter. The same command applies if you highlight (select) multiple lines of code. 3.5.1 Comments When writing code, it is good practice to include comments which describe what the code does. As your code becomes more complex and as time passes, it is very easy to forget what your code does! Commenting your code is especially important if your code is being used by other people or even by yourself weeks/months/years in the future; save yourself time and effort by describing your code fully as you write it. Comments can be written in R using the # symbol. Any line of code which begins with # is ignored by R when the script is run and are used solely to improve the readability of the code. To add a comment, insert the # symbol and write some text. Below is some uncommented code. Copy and paste into your script and add some comments to describe what is taking place. a &lt;- seq(10, 100, 10) b &lt;- a/10 c &lt;- b*a plot(c, a) The above code is our first encounter with plot() which (as you may have guessed), is used to plot objects in base R.1 The new plot youve created should now appear in the plots window and should resemble the following figure. There are lots of more exciting ways to plot data in R, some of which are described below. As a general rule, well-presented code would typically comprise: 1/3 code; 1/3 empty space; 1/3 comments. This is known as the Rule of Thirds. 3.6 Loading packages Now that we have our script set up, we need to understand is how to install and load packages in the R environment. To begin, remove all the previous code from your script learning_R.R and save. For the remainder of this chapter, we are going to use just one package as an example, but more will be required for Practicals 1 and 2, as detailed here. The package we are going to use is ggplot2, which is a widely used visualization package for R. If theres a figure, map or graphic you want to make, its likely that ggplot2 can do it for you. Its now even used by the BBC! BBC graphics created using ggplot2. See here for details. We can install the package using the helpfully named function install.packages() as follows: # Function to install packages. Name of package enclosed by quotation marks. install.packages(&quot;ggplot2&quot;) This function installs the package to your computer, but before we can use it, it needs to be activated within your current R session. This can be achieved by adding it to the library() as follows: # Function to add package to the library (no quotation marks) library(ggplot2) Copy and paste the two code snippets above into your script and run to install and load ggplot2. This is the standard approach to installing and loading packages in R. However, one of the weaknesses of this approach is that R will try and install and load each package every time the script is run. This is unnecessary; the package is already loaded! Try to re-install ggplot2 using the code above. The following pop-up should appear: Press No. The following message will appear in the console: Warning in install.packages : package ggplot2 is in use and will not be installed We can address this issue by using a user-defined function (called check.packages) to check whether a package is already installed. If it is installed, R will skip the installation. If it is not installed, R will install as normal. This function is similar to the functions weve been using before, such as sqrt() or seq(), which are available by default in base R; the difference is its been written from scratch! # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) Copy and paste the above code at the start of your R run script, removing the previous install.packages() and library() commands. Re-run the code. In future, you can add new packages by simply adding the package name to the packages vector. This works in exactly the same way as described previously, with the vector created using the c() command. For example, we could install packages as follows: packages &lt;- c(\"ggplot2\", \"dyplr\", \"raster\", \"sf\"). A full list of the available packages is here (&gt;18,000 and counting). Finally, packages should typically be installed and loaded at the start of any R script. If your code requires a package to run and it has not yet been installed, the code will fail. Installing and loading all the required packages at the start of the script avoids this problem. 3.7 Loading data In the final two sections of this chapter, we will load some data from a external file and plot using ggplot2. In the previous examples, weve been creating objects, vectors and data frames from scratch, but most data you will work with will be saved in another file format. This could be a comma-separated file (.csv), tab-delimited (.txt) or in an Excel format (.xls), or could be a spatial data type, such as a raster (.tif, .asc, .img) or vector (.shp). Being able to load and manipulate different data types is a key programming skill. Well use one of the most well-used formats as an example; the comma-separated file (.csv). This consists of rows and columns of data, where columns are delineated by commas. For example, here is an example of some comma-separated data: name,age,height,year_of_birth,instrument stewart,70,1.87,1952,drums andy,79,1.70,1942,guitar gordon,70,1.81,1951,vocals When read by R, it is interpreted as follows, splitting columns based on the occurrence of commas, and using the first row as column names: name age height year_of_birth instrument stewart 70 1.87 1952 drums andy 79 1.70 1942 guitar gordon 70 1.81 1951 vocals To load a .csv file into R, we can use the command read.csv(). The most basic way to load a file is to write the absolute file path. This is the location on your computer where the file is saved. For example, a file saved on your C drive, within your Documents folder, and within a folder called Manchester would be formatted as: C:\\Documents\\Manchester To determine the absolute file path, navigate in your file explorer (e.g. Windows Explorer, finder on macOS) to the data sub-directory within GEOG70581: For the final two sections of this Chapter (and the formative task), well using a data file called flow_data.csv which contains the average daily river flow (m3 per second) from the National River Flow Archive and the UK Centre for Ecology &amp; Hydrology. The data are from monitoring station 90003 on the River Nevis, which drains Ben Nevis, the highest mountain in the UK, and cover the period from the 1st October 1982 to the 30th September 2019. You can explore the Glen Nevis catchment below: The file path to your file might read as follows (if your name was Bill): C:\\Users\\Bill\\GEOG-70581\\data\\flow_data.csv To load this file into R: Copy and paste this file path into your script, enclosing the file path within quotation marks. Importantly, R uses forward slashes within file paths /, whereas the file path in your file explorer probably uses backslashes \\, so these will need to be swapped. # Loading data (ensure file path is updated) df &lt;- read.csv(&quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot;) If this has worked successfully, a new object df will have been loaded into the workspace: As you may have noticed, this is not the most elegant way to load data, as it requires us to copy, paste and edit the full file path each time. This would quickly become tiring if we wanted to load multiple data sets. In Practical 1, well guide you through a solution using R projects and the here package but as ever, it is important to understand the basics before moving on to more sophisticated solutions. We can use a variety of commands to inspect this new data set, including: head(df) colnames(df) str(df) Run the above code. What do each of the commands do? head(), colnames(), str() What data types do we have in each column? See here for a reminder. What data structure are we working with? As outlined previously, we can access individual columns of a data frame using the $ operator and could start to do some simple analysis using base R functions, such as: # What is the average daily flow? mean(df$flow) # What is the maximum daily flow? max(df$flow) # What is the minimum daily flow? min(df$flow) Question: What is the range of daily flow measurements? As outlined previously, we can also subset our data based on the values within the data frame, based on columns names ($) or using indexing. To simplify our subsequent analysis, were just going to work with data collected in 2019, although well ask you to extract a different year for the formative task. The data frame rows which correspond to 2019 are from the 1st January [13242] to the 30th September [13514]. Extract these rows and all columns using indexing; see here for a reminder if you get stuck. You may also have noticed from the str() command that our column of dates (day/month/year) were stored as characters e.g. $ date: chr \"01/10/1982\". Working with dates and times is a complicated task (see this Tom Scott video for a light introduction to the problem), so to simplify plotting, well convert these characters e.g. \"01/10/1982\" into a date format. This can be achieved using the following code, which overwrites the date column in the data frame: df$date &lt;- and uses the function as.Date() to define a date format. In this case, our numbers are listed in day, month and then year, which is formatted in R as \"%d/%m/%Y\": # Converts dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) Once youve extracted the flow data corresponding to 2019, run the above code. 3.8 Plotting In the final task, were going to plot these data using ggplot2. One of the challenges of ggplot2 is its steep learning curve, but well walk through this example to show how we can progress from the raw data to high quality figures. These approaches will be useful for your assessment. To finish this chapter, copy and paste the following code elements into your R script in turn and run them. Make sure to read and understand what each step is doing. To begin, were going to create an empty ggplot canvas i.e. the blank canvas upon which we can add ggplot elements, such as axes, legends, data, scale bars etc. # Initialising an empty canvas ggplot() Next, we specify which data set we are going to plot. Note: ggplot2 works almost exclusively with data frames: # Which data frame do we want to use? ggplot(data = df) With the data frame defined, we now need to specify which columns of date we wish to plot. The code below uses the function mapping=aes()to do so, where the date variable (within the df object) is assigned to the x-axis x=date and the flow rate variable is assigned to the y-axis y=flow using their column names: # Which data frame columns do we want to plot? ggplot(data = df, mapping=aes(x=date, y=flow)) However, our data are still not visible because we need to select a geom type, which is a complicated way of saying the type of geometric object we want to plot. We could plot our data as points, lines or polygons; we could choose a histogram, bar plot, or box and whiskers plot; there are many to choose from! In this example, we are going to plot our data as points, which is done using the geom_point() command. As you can see from the code below, the structure differs slightly from normal code, in that ggplot elements are separated by the + operator. This allows the user to add each ggplot element on a separate line of code, thereby improving readability. # What type of geom do we want? ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() In the same way, we could also plot the same data as a line, using geom_line(). In this example, both geoms are using the same data frame as inputs x=date, y=flow. This is because the data frame and columns have been specified in the initial call to ggplot here: data = df, mapping=aes(x=date, y=flow). All subsequent geom objects (e.g. points, lines, bar plots) will these inputs, unless an alternative is specified. # Adding a line geom ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() + geom_line() The above code covers the basics of ggplot, but we can start to improve the design of our plots. For example, we can choose one of the available themes. We could re-order our geoms to show the data points above the line, while adding some colour using HEX codes. We could change the aspect ratio to produce a square plot and maybe add some axis labels: # Specifying data frame and columns ggplot(data = df, mapping=aes(x=date, y=flow)) + # Specifying colours geom_line(colour = &quot;#56ACFF&quot;) + geom_point(colour = &quot;#767676&quot;) + # Adding a ggplot theme theme_classic() + # Setting the aspect ratio theme(aspect.ratio = 1) + # Updating axis labels (including superscript elements) labs(x = &quot;Date (January - October 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) Using the above plot, we could start to identify periods of above or below average flow. Adding a trend line could also be useful to identify patterns in the data. Add a trendline to your plot using the geom_smooth() argument. You can experiment with different smoothing methods e.g. method = lm, glm, gam or loess. As a final step, we will probably want to export our figure so that it can be used elsewhere e.g. in the assessment. To do so, we simply assign our ggplot object to an object using the assignment operator: &lt;- as normal: # Assign our ggplot object to the object &quot;g&quot; g &lt;- ggplot(data = df, mapping=aes(x=date, y=flow)) + ... Finally, we can save our ggplot object using the ggsave() function, as shown below. This takes in the name of the plot (g), as well as the intended name, extension and file path e.g. \"C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png\". In this example, we are saving our output within the images sub-directory of GEOG-70581. We have also defined the name of the file flow_data_2019 and specified its file type: .png (portable network graphics file). We can also modify other settings, such as the dpi which determines the plot resolution. A list of ggsave options can be found here. # Saves to a png ggsave(plot = g, &quot;C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png&quot;, dpi = 150) ## Saving 7 x 5 in image Run the above code to assign your plot to the object g and export it to your images folder, making sure to update the file path! We have now exported our plot and should be able to see it in the images sub-directory of GEOG-70581. Importantly, and for future plots, you dont need to run the commands in the sequence shown above e.g. first creating a canvas ggplot(), then defining a data frame ggplot(data = df), and so on. This sequence was used to show you how we can add elements to the ggplot object, building from the blank canvas to exporting the final figure. 3.9 Formative task Congratulations! You have now completed the guided portion of Chapter 3: the (not-so) brief introduction to R. While this may have been challenging, we have covered most of the core skills which you will need. You should now have an appreciation of the different data types and structures that are available in R, experience running code in the console and within scripts, and the ability to install packages, load data and create plots. Well use many of these techniques in Practicals 1 and 2. To finish the practical, please complete the formative task outlined below. Create an R Script which does the following: Installs the required packages. Loads the flow data for Glen Nevis. Extracts all rows corresponding to the years 2000 to 2019, making sure to convert to date format. Finds the average daily river flow (m3 per second) for each month and plots the result. Questions: on average, which month had the highest average daily flow and which had the lowest? Hints and Tips: For general guidance on using ggplot2, see here. To extract the month from each record, you may want to use the months() function from base R, while aggregate() (described here) can be used to compute summary statistics (i.e. the mean) based on groups. 3.9.1 Formative solution A code solution to the formative task will be uploaded here after the class. Note If youre working on a personal computer with Chinese language settings, you may need to run Sys.setlocale(\"LC_TIME\", \"English\") at the start of your code to fix language-related errors the in the as.Date() function. # Solution to the Chapter 3 formative task # Author: matt-tomkins # Date of upload: 17-11-2023 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) ## ggplot2 ## TRUE # Load data from csv (personal file path - update to your own) df &lt;- read.csv(&quot;C:/Users/44797/Desktop/Repo/GEOG-70581/data/flow_data.csv&quot;) # Convert dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) # Extracts post-2000 river flow df_post2000 &lt;- subset(df, date &gt;= as.Date(&quot;2000-01-01&quot;)) # Create a new column, corresponding to the month of measurement, using the months() function df_post2000$month &lt;- months(df_post2000$date) # Summarise by month, using the aggregate() function and &#39;mean&#39; summary &lt;- aggregate(flow ~ month, df_post2000, mean) # Determines plotting order by converting the month column to a &#39;factor&#39; # The factor levels are determined by the base R month.name function summary$month &lt;- factor(summary$month, levels = month.name) # Plotting using ggplot2 g &lt;- ggplot(data = summary, mapping=aes(x=month, y=flow)) + # Stat = &quot;identity&quot; is used when the values of x and y are known geom_bar(fill = &quot;#3EBBFB&quot;, stat=&quot;identity&quot;) + # Sets the theme theme_classic() + # Add x and y-axis labels labs(x = &quot;Month (2000 - 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) + # Tight fitting y-axis scale_y_continuous(expand = c(0,0)) + # Adjusts angle of x-axis labels theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # Displays figure g # Prints highest and lowest average daily flows print(paste0(&quot;The month with the highest average daily flow is &quot;, summary$month[summary$flow == max(summary$flow)])) ## [1] &quot;The month with the highest average daily flow is January&quot; print(paste0(&quot;The month with the lowest average daily flow is &quot;, summary$month[summary$flow == min(summary$flow)])) ## [1] &quot;The month with the lowest average daily flow is July&quot; Base R is used to refer to the functions that are part of the core packages deployed with R, such as plot(), sqrt() or seq(). As we install and utilise user-created packages, we are moving away from base R. "],["FirstPractical.html", "Chapter 4 Eskdale I - Introduction 4.1 Overview", " Chapter 4 Eskdale I - Introduction It should be evident from the lectures that digital elevation models (DEMs) have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions that (1) we can trace the path that water will take from any point in an elevation grid to a basin outlet, and (2) the discharge of water from a particular location is directly related to its catchment area. This is a fairly reasonable assumption in most drainage basins. 4.1 Overview In this lab exercise, we will use a DEM of the Upper Eskdale catchment to model flow-related phenomena, and will have gained experience in: DEM pre-processing; Calculating flow parameters (e.g. pointers and contributing areas); Comparing flow algorithms. Make sure to save your R script regularly! Upper Eskdale catchment 4.1.1 Intended learning outcomes In Chapter 5, well download the data and install the required programs and packages, and will combine these in Chapter 6 to: gain experience with WhiteboxTools, a specialist GIS specifically designed for digital terrain analysis (DTA); become familiar with some of the most commonly used flow algorithms used to simulate overland flow from digital elevation models (DEMs); consider the utility of DTA for routing applications; 4.1.2 Assessment This practical is formative and as such will not count towards your final mark for GEOG 70581, but you are encouraged to complete the practical to the best of your ability as the skills developed here will be used throughout the assessed Practical 2. "],["Eskdale_set_up.html", "Chapter 5 Eskdale II - Set-up 5.1 Install programs 5.2 Download data 5.3 Open RStudio 5.4 Initialise an R project 5.5 Creating an R script 5.6 Install WhiteboxTools 5.7 Loading packages", " Chapter 5 Eskdale II - Set-up 5.1 Install programs If you havent installed R and RStudio, please refer to the instructions here before continuing. 5.2 Download data If youve not already done so, the data for this practical can be downloaded here. The directory structure is outlined in Chapter 2. 5.3 Open RStudio To begin, open RStudio. 5.4 Initialise an R project In the previous chapter, we typed commands into the console and used R scripts to store commands and comments. For both Practicals 1 (Eskdale) and 2 (Mersey), we are going to expand on these by also utilising R projects. Projects make life simpler by allowing us to access file locations (e.g. for loading and writing data) using relative file paths, rather than using the clunky and prone-to-breaking absolute file paths. Relative file paths work by setting the working directory, which is simply the the default location where R will look for files you want to load and where it will put any files you save. We can do this manually by using the setwd() function, but a simpler approach is to use an R project, which sets the working directory for your automatically. Rather than the complicated absolute file paths: &quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot; which indicates that the file is located in data, which is within GEOG-70581, which is within Documents, which is within Bill, which is within Users, which is on the C drive, we can set the working directory to GEOG-70581 (for example) and then access the file using the here package: here(&quot;data&quot;, &quot;flow_data.csv&quot;) This approach not only makes it easier to access files, but also improves reproducibility. Using projects ensures that our work is self-contained (the project directory contains all the data, scripts and outputs) and portable. Transferring the project directory to another user, to a different sub-folder on your computer, or a different drive, would not cause any code to fail. By comparison, scripts featuring absolute file paths or a manually defined working directory would likely fail in any of the above scenarios. To initialise a new R project: Go to File, New Project, and select Existing Directory. Using the Browse icon, set the project working directory to the folder GEOG70581, which you downloaded and unzipped here. Select Create new project If this has been successful, your console should have been updated to include the path to your project working directory as follows: 5.5 Creating an R script Now that we have created our project for GEOG70581, we are going to use scripts to store the code for each practical. Well create a script for Practical 1 and a separate script for Practical 2. Important: a script can be understood as a set of instructions. As a result, the order of the code is critical. A useful metaphor may be to think of a script as a recipe. For example, a simple cake recipe might require butter, sugar, eggs, flour, milk, vanilla extract and lemon. These could be combined as follows: Pre-heat oven. Whisk butter and sugar. Crack the eggs in one at a time and whisk. Add the lemon zest, vanilla, flour and milk and whisk. Bake in the oven for 30 mins. In code form, it might resemble the following, with a vector including the ingredients and different functions used to combine the ingredients (whisk), turn on the oven (pre_heat) and bake (bake_mixture): # Vector of ingredients ingredients &lt;- c(butter, sugar, eggs, flour, milk, vanilla, lemon) # Pre-heats oven for 20 minutes pre_heat(20) # Create a mixture of butter and sugar using the whisk function, mixture &lt;- whisk(ingredients$butter, ingredients$sugar) # Iterates (loops) over eggs variable for (i in length(ingredients$eggs)){ # Extracts each egg, one at a time current_egg &lt;- ingredients$eggs[i] # Adds current egg to mixture mixture &lt;- whisk(mixture, current_egg) } # Add remaining ingredients mixture &lt;- whisk(mixture, ingredients$flour, ingredients$milk, ingredients$vanilla, ingredients$lemon) # If oven temperature is 180 degrees of more if(oven_temperature &gt;= 180){ # Bake for 30 minutes cake &lt;- bake(mixture, time = 30) } From the text and code example above, it should be clear that our recipe (and code) would not work correctly if we tried to bake in the oven before pre-heating it, or added the ingredients in the wrong order. In exactly the same way, it is important that your code is formatted in a logical order (e.g. Function 1, Function 2, Function 3, ). Ensure that your future code reflects the recipe above, where individual functions utilise variables created previously, and dont rely on variables created further down in the code. To create an R script for Practical 1: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-1-Eskdale) This should now resemble the following: 5.6 Install WhiteboxTools For Practical 1, we are going to use the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); These packages can be installed as normal using the install.packages() and library() functions. In previous versions of this course, whitebox was not available (because of complicated reasons) on the Comprehensive R Archive Network (CRAN). Instead, it had to be installed using R-Forge. While CRAN is the official package repository for R and generally features release versions of packages, R-Forge provides candidate, beta or work-in-progress packages or packages which dont meet the criteria for inclusion on CRAN. However, as of the 27th October, whitebox is now back on CRAN so we can install as normal using the install.packages() function. It is worth noting, however, that you can install packages from R-Forge by simply specifying the repository (web location) where the package is stored, in this case: http://R-Forge.R-project.org. # Installing a package via R-Forge install.packages(&quot;insert package name here&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) Note You dont need to run the above code - this is just an example of how to install via an alternative repository. One additional step, which is different from normal R package installation, is to use the function whitebox::install_whitebox(), which initialises WhiteboxTools: # Initialise WBT whitebox::install_whitebox() This should print the following to your console: Performing one-time download of WhiteboxTools binary from https://jblindsay.github.io/ghrg/WhiteboxTools/WhiteboxTools_win_amd64.zip (This could take a few minutes, please be patient...) WhiteboxTools binary is located at: C:/Users/44797/OneDrive/Documents/R/win-library/4.0/whitebox/WBT/whitebox_tools.exe You can now start using whitebox &gt; library(whitebox) &gt; wbt_version() This code checks if a suitable WhiteboxTools executable is present (.exe) and installs if missing. The WhiteboxTools executable contains all the code used for geospatial analysis, which we then access using R via the whitebox R package. This is a slightly different configuration from normal R packages but enables WhiteboxTools to be used cross-platform i.e. WhiteboxTools functionality stored in the executable can be assessed through R, Python, ArcGIS or QGIS etc, depending on the user needs. Run the below code to install the whitebox package, initialise WhiteboxTools (which should perform a one-time download of the WhiteboxTools executable) and check it is installed correctly. # Install the whitebox R package install.packages(&quot;whitebox&quot;) # Initialise WhiteboxTools whitebox::install_whitebox() # Add the whitebox R package to library library(whitebox) # Check the version is correct wbt_version() When complete, WhiteboxTools has now been successfully installed and initialised and is almost ready to be used. However, to avoid re-installing and re-initialisng WBT every time we run our script, we can either delete the code or comment-out # each line. Remember that R ignores any line of code that begins with #: # Code to install WhiteboxTools # whitebox::install_whitebox() Either delete or comment-out the previous command (install_whitebox()). Ctrl + Shift + C is a useful shortcut for adding or removing comments from multiple lines of highlighted code. 5.7 Loading packages To finish the set-up for Practical 1, we are going to install the remaining required R packages (described above) and load them into the R library. Weve already practiced installing and loading the ggplot2 package in Chapter 2 so you are ready for the following: Referring back to the instructions here, install and load ggplot2, here, raster, sf, ggspatial and whitebox. Hint 1 - weve already installed whitebox, so this can be excluded if youre using install.packages() and the library() commands. Hint 2 - the user-defined function check.packages might be useful here. If the package has been installed and loaded correctly, the console should print the following: ggplot2 here raster sf whitebox ggspatial TRUE TRUE TRUE TRUE TRUE TRUE where the logical value TRUE equates to a successful installation (and vice versa for FALSE). "],["Eskdale_flow_algorithms.html", "Chapter 6 Eskdale III - Flow routing 6.1 DEM pre-processing: flow enforcement 6.2 Flow Parameters: Pointers 6.3 Flow Algorithms 6.4 Practical solution", " Chapter 6 Eskdale III - Flow routing Overland and near-surface water flow can be modelled using DEMs if we assume that surface topography is the sole factor which influences the distribution of water. One very simple model routes all water from a particular grid cell in a DEM to a single neighbouring cell (i.e. water is not partitioned between multiple neighbours). This D8 (8 direction) method sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell. Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). Using this method, water flow is allowed in one of eight possible directions (       ), assuming that water will travel along the steepest downslope path. In turn, the method is sometimes referred to as the steepest descent method. Based on the \\(3 * 3\\) cell neighbourhood shown in Figure 1, flow would be directed from the centre cell (8 m elevation) to the southwest cell (4 m elevation). Reminder: you dont have to run every bit of code in this document. Indented text (such as this) is an instruction that tells you something to either think about, or do. 6.1 DEM pre-processing: flow enforcement One common issue encountered when performing hydrological analyses is the presence of sinks, which interrupt the drainage network. When sinks are encountered, flow direction is undefined when a grid cell, or group of grid cells, is lower than all neighbouring cells (see Figure 2). When sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. These features are referred to as pits if they are a single cell in size, and depressions if they consist of groups of cells. Figure 2: Schematic of a sink in a DEM. In this \\(5 * 5\\) matrix, water is routed into the top right cell (9 m) and then is routed to the lowest elevation cell at each step (9 m  8 m  7 m  6 m  2 m) using the D8 method (blue cells). However, the drainage network is interrupted by a sink at the 2 m cell (orange), as all neighbouring cells are of higher elevation. Sinks can often be artefacts of the data and should be removed during DEM pre-processing. Pre-processing involves altering the elevations of the DEM in a way that enforces continuous flow-paths. However, it is important to realise that sometimes these digital depressions reflect actual features in the landscape, and should be preserved during flow modelling. This is a particular issue for hydrological analysis of karst environments, where water can be routed into dolines and fractures (see Figure 3). However, for our work, we will assume that all depressions in DEMs are artefacts and are justified in being removed. Figure 3: An example of a true DEM sink. A sinkhole extending into the limestone karst of Twisleton Scar, Yorkshire Dales, with Ingleborough behind (723 m). How should we model water flow in environments with sinkholes, fractures, dolines and extensive sub-surface water flow? Several methods have been developed for removing depressions from DEMs. These methods vary greatly in terms of their sophistication and impact on the DEM. The two most common depression removal methods (Figure 4) are: depression filling, which raises cells within a depression to the elevation of the outlet cell; depression breaching, which digs a trench from a depressions bottom to some point downslope. Figure 4: Schematic of depression filling and breaching in a DEM. Using the same values from Figure 2, original values are modified to allow water to escape the sink (orange). Depression filling has raised the value of the sink (2 m  4 m), while depression breaching has lowered the value of a neighbouring cell (3 m  1 m). In this simplified example, the outputs of these two distinct approaches are identical but care should be taken when working with real world data as they will often produce different results. Not all interruptions to flow routing are caused by depression cells. Often, DEMs contain extensive flat regions (areas of equal elevation). Flat areas interrupt flow routing in the same way as depressions. Cells within a flat region do not have downslope neighbours, and therefore, flow routing is impossible on flat sites without pre-processing. Correction of flow direction on flat sites typically involves finding an outlet cell, forcing flow from cells adjacent to the outlet to the outlet, and continuing backwards in an iterative manner (e.g., Jenson and Domingue (1988)). 6.1.1 Data description The DEM we are working with is centred on the Upper Eskdale catchment; an upland valley which drains the highest mountain in England (Scafell Pike; 978 m), as shown below: Figure 5: Upper Eskdale panorama, viewed from Harter Fell [Location: 54.386907, -3.205004, Elevation: 649 m]. The catchment ranges in elevation from 978 m (Scafell Pike) to ~160 m at the catchment outlet (white circle), and is ringed by numerous summits with elevations in excess of 800 m (white triangles). The catchment has an area of ~15.7 km2 and all water which falls with the catchment ultimately drains to the Irish Sea via the River Esk. You can explore the catchment below: The DEM we are working with was downloaded from EDINA Digimap, has a cell size of 10 m and uses the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Unfortunately, we dont have time in this course to delve into the exciting world of map projections, although these are covered excellently by Dr. Jonny Huck in Understanding GIS. However, it is important to know that different map projections have different uses and work more/less effectively in different regions. As we are working within the UK, it makes sense to use a map projection which is tailored to the UK (e.g. BNG) as this minimises different types of map distortion (length, shape, area). When loaded into R using the raster package and plotted using ggplot2 and ggspatial, our DEM can be visualised as follows, where the outlet point (white circle) and summits (black triangles) match those shown in Figure 5. To accentuate areas of relief, the DEM has been combined with a semi-transparent hillshade layer, which is shown below: 6.1.2 Breaching and filling To evaluate the effects of depression breaching and filling, were going to use the whitebox package and the wbt_fill_depressions and wbt_breach_depressions functions. One of the advantages of using whitebox is that functions can be applied to spatial data without having to load files into the R environment (and assign them to objects), which can be an issue when working with large files (e.g. high spatial resolution rasters) or when working on computers with limited available memory. We can call whitebox functions by simply naming the function, specifying the names of the input and output layers, and deciding on parameters (if required). For example, the code to fill depressions, might resemble the following: # Fill depressions wbt_fill_depressions(&quot;name of input file goes here&quot;, &quot;name of output file goes here&quot;) Given the structure of our file directory, this could be written as: # Fill depressions wbt_fill_depressions(&quot;C:/Users/Bill/GEOG-70581/data/practical_1/dem_10m.tif&quot;, &quot;C:/Users/Bill/GEOG-70581/output/practical_1/dem_10m_fill.tif&quot;) Here, weve used absolute file paths to set the input DEM as dem_10m.tif, stored in data/practical_1. The function wbt_fill_depressions will fill depressions in the input DEM and save the new DEM as dem_10m_fill.tif, stored in output/practical_1. However, because were using an R project we can simplify this further. As well be using the DEM (dem_10m.tif) on multiple occasions, we can store its file path using the here package in the variable dem as follows: # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem Copy and paste the above code into your Practical-1-Eskdale script and run. This should print the absolute file path to dem_10m.tif. With this file path stored, we can run the fill depressions function as follows, using the here package to specify the name and intended location of the output file: # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) Run the above code, which should produce a new raster file in the output/practical_1 folder. Repeat this process to also breach depressions, making sure to update the output file name. The difference between the original DEM and the processed DEM(s) may be very subtle and you probably wont be able to observe the differences directly. As such, it may be useful to identify the specific areas of the raster which have been modified. This can be achieved by using the subtract function to work out the difference between the original DEM and the processed DEM, as follows: # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file Run the above code. To visualise this new raster file, were going to load it into the R environment using the raster and here packages and plot with ggplot2 and ggspatial. The raster can be loaded as follows: # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) It can be plotted as follows: # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g Most of the above should be familiar to you, with a few exceptions: Under scale_fill_continuous(), weve set limits for the fill symbology. This now ranges between the maximum value, calculated using max(values(dem)), and the minimum value that is greater than 0, calculated using minimum_value &lt;- min(values(dem)[which(values(dem) &gt; 0)]). The latter is useful because most of the DEM was unmodified by wbt_fill_depressions so most raster cells contain a value of 0. The layer spatial() is a non-standard geom type and is available through the ggspatial package. This simplifies the process of plotting raster data in ggplot2, and this approach should be replicated for future plotting. Run the above code elements to reproduce the plot above. You can remove limits from the scale_fill_continuous() argument to plot the actual distribution of DEM values. Question: to what degree has the original DEM been modified by the wbt_fill_depressions function? Consider the spatial extent and magnitude of the observed differences. Note: For this catchment, the difference between DEM breaching and filling is relatively minor. Well use the latter (dem_10m_fill.tif) for all subsequent calculations. As outlined above, the areas which are most affected by the depression filling algorithm are partly determined by local relief i.e. relatives differences in elevation. To help visualise this, lets create and plot a slope image for the area, where the values in the raster cells represent the gradient of the slope, rather than their raw elevation. This can be achieved using the wbt_slope function. Using the examples above, run wbt_slope, using the original DEM as the input, saving the output file with a logical name (e.g. dem_10m_slope.tif) and to the correct sub-directory (output/practical_1). Load the data into R and plot using ggplot2. Your figure should resemble the following: Compare the slope and difference images. Hint: you can toggle between plots using the left and right arrows in the plot window. Based on your visual inspection the images, which parts of the landscape appear to have been affected by the depression filling algorithm? Why do you think that this is the case? Before we move on to further analysis, it is worth thinking carefully about the symbology we are using for our raster layers. This is important, because not all colour ramps are created equal! Colour ramps differ in terms of their perceptual uniformity and their accessibility for colour-blind individuals. Save your script before continuing. 6.1.3 Choosing a colour ramp Perceptual uniformity refers to how well a colour ramp represents the underlying data. Some (e.g. rainbow; Hawkins (2015)) can introduce artificial boundaries (Alcalde et al. (2017)) which influence how we assess and then use that data (see here). In the schematic below, there are numerous artificial boundaries in the rainbow colour ramp (e.g. dark blue  light blue, light blue  green, green  yellow, yellow  orange), whereas the monochrome colour ramp smoothly transitions from black  white. The viridis colour ramp we have been using so far is perceptually uniform, but others are not. Inspect Figure 6 below. These show the same slope raster, but one is visualised using rainbow, while the other uses a monochrome (perceptually-uniform) colour ramp. Does this influence how you interpret the data? Figure 6: Perceptual uniformity with rainbow and monochrome colour ramps. In a similar fashion, not all colour ramps are suitable for individuals with various forms of colour-blindness (see Levine (2009) and Geissbuehler and Lasser (2013)), who may have difficulty distinguishing reds from greens, or yellows from blues. Luckily, there are now a range of perceptually uniform and colour-blind friendly colour ramps (e.g. Crameri (2018)), including Scientific Colour Maps 6.0. These can be downloaded and used across a range of GIS programs to suit a range of data types. For now, utilise the colour ramps available in R (i.e. viridis) but bear these issues in mind for future map creation. 6.2 Flow Parameters: Pointers Now that we have a DEM that has had all interruptions to flow removed, we can continue to calculate the data needed to define the drainage network i.e. the topological network describing how each cell in a DEM is connected to all others. One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. Whitebox Tools stores D8 pointers as binary numbers in base-2, as follows: Well explore the reason for this in Practical 2. In our earlier example of the D8 flow algorithm (see Figure 1), the flow direction was to the SW. This has a binary value of \\(2^4 = 16\\). Therefore, in the pointer file, the highlighted dark blue cell would be assigned a value of 16. Several sub-programs in Whitebox Tools use pointer files to calculate other terrain data, including the algorithms used to calculate catchment area (see below). However, sub-programs rarely require a pointer file as an input, as WBT will automatically generate a pointer layer from the input DEM when required, before discarding the file afterwards. While this processing step is not readily apparent to the user, it is important to realise that it occurs, and it is often good practice to generate a pointer file manually. This can be achieved using the wbt_d8_pointer function. Using the understanding gained above, run the wbt_d8_pointer function, using the correct input files and output file name and directory. Load the raster into R and plot. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. While this is an acceptable plot, it is actually slightly misleading, because the scale bar is continuous, despite the fact that our underlying data are discrete i.e. there are only eight possible values, which are listed below: \\(2^0 = 1\\) \\(2^1 = 2\\) \\(2^2 = 4\\) \\(2^3 = 8\\) \\(2^4 = 16\\) \\(2^5 = 32\\) \\(2^6 = 64\\) \\(2^7 = 128\\) Here is the same data but using a discrete scale: # Manually defined colours based on &quot;RdYlBu&quot; colour palette colours &lt;- c(&quot;1&quot; = &quot;#D73027&quot;, &quot;2&quot; = &quot;#F46D43&quot;, &quot;4&quot; = &quot;#FDAE61&quot;, &quot;8&quot; = &quot;#FEE090&quot;, &quot;16&quot; = &quot;#E0F3F8&quot;, &quot;32&quot; = &quot;#ABD9E9&quot;, &quot;64&quot; = &quot;#74ADD1&quot;, &quot;128&quot; = &quot;#4575B4&quot;) # Plotting p &lt;- ggplot() + layer_spatial(eskdale_pointer, aes(fill = factor(stat(band1)))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_manual(values = colours, na.value = NA) p which is described here: Question: Can you think of any issues with symbolising pointer data? Save your script before continuing. 6.3 Flow Algorithms Once you have determined how each grid cell in a DEM is connected to its neighbours, it is possible to use this pointer information to calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. Measures of catchment area are often used for stream network extraction and other terrain indices. There are three primary measures of catchment area: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment. Unlike the NUE and UCA, SCA is not affected by the grid resolution of the DEM. To calculate the upslope catchment area for each raster cell, we can use the wbt_d8_flow_accumulation function. Run the above function, using the filled DEM as the input file and the output type (out_type) as specific contributing area (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. dem_10m_flow_accumulation.tif) and to the correct sub-directory (output/practical_1). Load the raster to R and plot, and your output should resemble the following: Inspect the map output. Question: Which parts of the landscape have low/high values of SCA, respectively? 6.3.1 Comparing flow algorithms So far we have used the most simplistic flow algorithm (D8) to model specific catchment area (SCA). However, there are many flow algorithms available (e.g. Fairfield and Leymarie (1991), Seibert and McGlynn (2007), Grabs et al. (2010)) and each algorithm results in slightly different spatial patterns of water flow. As flow routing is fundamental to the modelling of surface water hydrology, it is important to familiarise yourself with several commonly used methods, although it can be difficult to decide when or where it is appropriate to use a particular routing algorithm. In this section of the practical, we will compare the output of three flow routing algorithms including: D8: see OCallaghan and Mark (1984) FD8: see Freeman (1991), Quinn et al. (1991) and its modification by Quinn et al. (1995) D-infinity: see Tarboton (1997); Flow algorithms differ in the way in which they: calculate flow direction; model flow divergence, i.e., partition flow between neighbouring downslope cells; handle streams. The D8 algorithm is incapable of modelling flow divergence (or dispersion) because the flow from a particular grid cell is directed to a single downslope neighbour. FD8 (i.e. fractional D8; Freeman (1991), Quinn et al. (1991)) divides flow to each downslope neighbour using a partitioning scheme that is based on the slope angle to neighbouring cells, while a later modification by Quinn et al. (1995) increases the degree of flow convergence from the catchment divide to the channel head. Finally, D-infinity divide flows between a maximum of two downslope neighbours based on the direction of maximum downward slope. Of these, FD8-Quinn (Quinn et al. (1995)) is the only algorithm of the three that explicitly recognises that flow on hillslopes should be divergent while flow along valley bottoms should be convergent. In the final part of the practical, we will assess the specific catchment areas (SCA) derived from each of the flow algorithms discussed above. 6.3.2 FD8 catchment area To calculate the catchment area using the FD8 algorithm (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)), use the wbt_fd8_flow_accumulation function. Use the wbt_fd8_flow_accumulation function, making sure to select specific contributing area and remembering to log-transform the values. The exponent parameter should be set to 1.1 (exponent = 1.1), as suggested by Freeman (1991). Save with an appropriate name (e.g. dem_10m_flow_accumulation_fd8.tif). Load the raster in R and plot. 6.3.3 D-infinity catchment area To calculate the catchment area using the D-infinity algorithm of Tarboton (1997): Use wbt_d_inf_flow_accumulation. Use the same input parameters as above (i.e. SCA, log-transformed values). Save with an appropriate name (e.g. dem_10m_flow_accumulation_d_inf.tif). Load the raster in R and plot. 6.3.4 Comparing the outputs The output of the algorithms should resemble the following: Figure 7: Flow algorithm comparison. Results from the D8, FD8 and D-Infinity algorithms. Plot created using ggplot2 and the patchwork package (useful for combining plots). When water flows over a hillslope, you would expect there to be some degree of divergence depending on the topography (i.e. some hillslopes are convergent and some are divergent). However, once water becomes channelised, you would no longer expect flow to be divergent. Once water is in a channel it stays there until it reaches a lake or ocean. Branching in channels is rather rare. So lets assume that the pattern of SCA should display some degree of dispersion on hillslopes and no dispersion at all along valley bottoms (i.e. in streams). Flow divergence or dispersion is apparent in SCA images by a fuzzy quality, almost like the SCA has been smudged. You can zoom into individual plots using the Zoom button in the Plot Window or edit the x- and y-axis limits of your plots by adding the following to your ggplot2 call: # Sets x- and y-axis limits using coord_sf() + coord_sf(ylim=c(504063,506727), xlim = c(320705, 322255)) The above code uses British National Grid coordinates (m), rather than decimal degrees. To finish Practical 1, save your script and complete the following: Compare the D8, FD8 and D-Inf outputs. Either cycle between the plots using the plot window or combine them using the patchwork package. This would need to be installed and loaded to the library. Which algorithms do a good job modelling the pattern of contributing area on hillslopes? Can you see signs of flow divergence along streams in any of the SCA images? Which algorithms perform best for modelling the pattern of SCA in fluvial (stream) environments? Do any of the algorithms do a good job for both hillslopes and streams? Congratulations! You have now completed the Practical 1 (Eskdale; Chapters 4 - 6). You now have considerable experience running whitebox functions as well as visualising spatial data using ggplot2. 6.4 Practical solution As with the introduction to R (Chapter 2 - 3), I will provide a code solution to the Eskdale practical (Chapters 4 - 6), outlining the major analytical steps. This will be released prior to the Week 10 class. # Solution to Practical 1 (Eskdale) # Author: matt-tomkins # Date of upload: 25/11/2022 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) ## ggplot2 sf here raster whitebox ggspatial patchwork ## TRUE TRUE TRUE TRUE TRUE TRUE TRUE # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) # Calculates FD8 accumulation file (SCA), with log-transformed values and an exponent of 1.1 wbt_fd8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_fd8.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;, exponent = 1.1) # Calculates D-Infinity accumulation file (SCA), with log-transformed values wbt_d_inf_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_d_inf.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) # Loads accumulation files using raster and here packages eskdale_d8 &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;)) eskdale_fd8 &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_fd8.tif&quot;)) eskdale_d_inf &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_d_inf.tif&quot;)) # Plots D8 using ggplot2 p_d8 &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_d8, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;D8&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Removes legend, sets title size theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) # Plots FD8 using ggplot2 p_fd8 &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_fd8, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;FD8&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Sets legend location and axis labeling theme(legend.position = &quot;bottom&quot;) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.title = element_text(hjust = 0.5)) # Plots D-Infinity using ggplot2 p_dinf &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_d_inf, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;D-Infinity&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Sets fill symbology theme(legend.position = &quot;top&quot;) + # Removes legend and sets axis labeling theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) # Combines using the patchwork library combined_plot &lt;- p_d8 + p_fd8 + p_dinf # Displays figure combined_plot References "],["SecondPractical.html", "Chapter 7 Mersey I - Introduction 7.1 Overview", " Chapter 7 Mersey I - Introduction Within a Geographical Information System (GIS) framework, readily available spatial datasets, such as land use and geology, have been used to explore the controls on river water quality. Understanding the relationships between catchment characteristics (or metrics) and river water quality provides a base for determining how future changes in both land use and climate will impact river water quality. Therefore, it is important to determine the processes that regulate river water quality in landscapes under increasing pressure from human population, whether from urbanisation or more intensive food production. The Mersey Basin is one of the UKs most environmentally varied regions, with rich rural landscapes and urban-industrial centres. Industrial prosperity in the region over the last few hundred years resulted in severe pollution of the many rivers in the basin. Although there has been significant improvements in water quality in recent years (see here), the rivers in the Mersey Basin are still subjected to a number of sewage and industrial inputs, and agricultural runoff. You can explore the basin below: 7.1 Overview In this practical you will explore the controls on river water quality in the Mersey Basin and develop empirical models of river water quality across the region. 7.1.1 Topics covered in this practical Terrain analysis, including catchment delineation and derivation of catchment characteristics; Writing and execution of scripts; Multiple linear regression to create models of catchment hydrochemistry; Methods for model evaluation. 7.1.2 Intended Learning Outcomes Gain practical experience of modelling environmental phenomena using a Geographical Information Systems (GIS) framework; Gain practical experience of hydrological modelling using GIS; Gain practical experience of manipulating and analysing raster datasets; Gain practical experience of using Map Algebra functions and writing scripts; Gain practical experience of using multivariate analysis; Understand some of the key controls on river water quality; Be able to critically evaluate model outputs. 7.1.3 Assessment This practical is assessed by completion of the tasks outlined in this document, full details of which are provided here. You will need to write up the practical in the format of a report. This practical is worth 50% of your overall mark for GEOG70581. As well as the standard SEED PGT criteria, you will also be assessed on your ability to: Effectively follow instructions to complete the practical tasks (technical prowess), i.e. producing the regression equations and output images correctly; Analyse and interpret the outputs in the context of your knowledge of river hydrochemistry (and with reference to appropriate literature); Relate the monitoring and modelling approaches here to theory covered earlier in the course (and with reference to appropriate literature); Explain ideas clearly and concisely; Present figures and data clearly and effectively; Be original and insightful in terms of independent research and additional analysis of the data where appropriate. Use references to relevant academic literature to support your arguments; The deadline for this practical is 14:00 on Thursday 5th January 2023. The word count for this practical is 1500. "],["Mersy_set_up.html", "Chapter 8 Mersey II - Set-up 8.1 Practical overview 8.2 Install programs 8.3 Download data 8.4 Data description 8.5 Open RStudio 8.6 Projects and Scripts 8.7 Loading packages", " Chapter 8 Mersey II - Set-up 8.1 Practical overview This practical is comprised of eight primary tasks, with three weeks of class time available (Weeks 10 - 12). Each of the steps is described in more detail in the remainder of this document. An outline of the key tasks is as follows: Mersey III (Chapter 9): To complete in class in Week 10, and finish before the class in Week 11: Task 1: Flow routing Task 2: Seed points Task 3: Watershed delineation Mersey IV (Chapter 10): To complete in class in Week 11, and finish before the class in Week 12: Task 4: Reclassification of categorical datasets Task 5: Calculating surface derivatives Task 6: Extracting surface derivatives Mersey V (Chapter 11): To complete in class in Week 12: Task 7: Model building i.e. relating river water quality to catchment metrics Task 8: Model evaluation Any class time remaining can be used to commence your practical report and ask any questions you may have before submission. 8.2 Install programs You should have already installed R and RStudio. If not, please refer to the instructions here before continuing. 8.3 Download data The data for this practical have already been downloaded here and can be found in data/practical_2. The directory structure is outlined in Chapter 2. 8.4 Data description In Practical 1, we used a single raster file (dem_10m.tif) and visualised and assessed the outputs of a number of WBT functions (e.g. fill, breach, slope, pointer, flow accumulation). In Practical 2, we are going to use a much wider range of input data, all of which use the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. The data are described here: Raster data (.tif): mersey_dem_fill a filled digital elevation model of the Mersey Basin; mersey_basin a Boolean or logical raster (0 or 1) representing the extent of the Mersey Basin; mersey_rainfall a raster of precipitation values; mersey_bedrock a categorical raster of bedrock geology types; mersey_HOST a categorical raster of soil types (Hydrology of Soil Types); mersey_LC a categorical raster of land cover classes, based on LCM2000 data; Vector data (.shp): mersey_watershed_basin a polygon vector representing the extent of the Mersey Basin; mersey_roads a line vector representing A roads and motorways; mersey_EA_sites a point vector representing the locations of water quality monitoring stations. The attribute table contains a unique Environment Agency ID for each (EA_ID): Tables (.csv): mersey_EA_chemisty a comma-delimited table containing measurements for the following water quality indicators, as well as corresponding Environment Agency IDs: pH: acidity/alkalinity; SSC: suspended solids concentration; Ca: calcium; Mg: magnesium; NH4: ammonium; NO3: nitrate; NO2: nitrite; TON: total oxidised nitrogen; PO4: phosphate; Zn: zinc. Read through the above descriptions carefully, making sure you understand the data we are using before moving on. Well be combining the Environment Agency measurements of water quality (e.g. pH, SSC, ) with spatial data representing the catchment (e.g. topography, rainfall, land cover, ) to investigate the controls on water quality across the Mersey Basin. 8.5 Open RStudio To begin, open RStudio. 8.6 Projects and Scripts 8.6.1 Using an existing R project In Practical 2 (Eskdale), we utilised an R Project called GEOG70581. Your GEOG70581 directory should resemble the following: Here we have sub-directories for the original data (data) and any spatial data files or images we might create (output and images). We can also see the GEOG70581 project file and the Practical-1-Eskdale R script. The former is used to improve file access and to ensure our code is reproducible, self-contained and portable (see here for a refresher). The latter contains all our code and comments relating to Practical 1. In this practical, we dont need to create a new R project. We will simply use the existing R project as follows: Go to File, Open Project in New Session, and select the GEOG70581 project file. If this has been successful, your console should have been updated to include the path to your project working directory as follows: 8.6.2 Creating an R script As shown in the file directory image above, we already have an R script for the Eskdale practical (Practical-1-Eskdale). As we are now working on a separate practical, with different input data and analytical techniques, it makes sense to create a new script to store the code and comments. To create a new script for Practical 2: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-2-Mersey) This should now resemble the following: 8.7 Loading packages As were working in the same R project from Practical 1, we dont need to re-install already utilised packages (e.g. whitebox). However, we will need to install some new packages and ensure that all packages are loaded into the R environment. Copy and paste the check.packages function into your new script, either from below or from Practical-1-Eskdale.R: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } In Practical 1, we used the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); In Practical 2, we are going to use a number of additional packages: data.table for easy manipulation of tables; dplyr for easy manipulation of data frames; forcats for working with categorical variables (factors); MASS for statistical analysis, based upon Venables and Ripley (2002) Modern Applied Statistics with S; units for calculation of measurement units. To load new packages, you can either use the install.packages() and library() functions or more simply, add package names to the packages vector, as shown here: # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;ggspatial&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;sf&quot;, &quot;whitebox&quot;, # Practical-1 &quot;data.table&quot;, &quot;dplyr&quot;, &quot;forcats&quot;, &quot;MASS&quot;, &quot;units&quot;) # Practical-2 check.packages(packages) Use the above code to install and load the required packages. "],["mersey_three.html", "Chapter 9 Mersey III - Hydrology 9.1 Task 1: Flow routing 9.2 Task 2: Seed points 9.3 Task 3: Watershed creation", " Chapter 9 Mersey III - Hydrology 9.1 Task 1: Flow routing It should be evident from the lecture and the previous practical that DEMs have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions: We can trace the path that water will take from any point in an elevation grid to a basin outlet; The discharge of water and sediment from a particular location is directly related to its catchment area. However, the first assumption is not met when the flow direction is undefined. This occurs when a single grid cell (pit), or group of grid cells (depression), is lower than all neighbouring cells. When these DEM sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. While these sinks can reflect true features of the landscape, they are often DEM artefacts and can be removed using depression filling or breaching. For more information on DEM sinks and the methods used to remove them, see the previous practical. For this practical, well be working with a DEM where sinks have already been removed, as indicated by the file name (mersey_DEM_fill). While it is not always practical to use file names which include all analytical steps, it is good practice to include the key analytical steps (e.g. _fill, _slope, _hillshade). 9.1.1 Creating a D8 pointer One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. One common approach is the D8 algorithm, which sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell (see Figure 1). Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). We also know from the previous practical that Whitebox stores D8 pointers as binary numbers in base-2 (see Figure 2): Figure 2: Pointer data format in Whitebox Tools. Flow directions (NE, E, SE, S, SW, W, NW, N) are stored as binary numbers in base-2 (\\(2^0, 2^1, 2^2, 2^3, 2^4, 2^5, 2^6, 2^7\\)) Initially, this can seem unnecessarily complex, particularly when compared to a simple sequential storage method (e.g. \\(N = 0, NE = 1, E = 2, ...\\)). However, the advantage of this approach is only apparent when multi-directional flow algorithms are considered such as FD8 (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)) and D-infinity (Tarboton (1997)). While the D8 algorithm assigns all water flow to a single neighbouring cell, multi-directional algorithms split water flow into two or more neighbouring cells. For these methods, a binary base-2 approach enables this information to be stored efficiently as a single number (see Figure 3). With eight neighbouring cells, there are a total of 255 possible combinations2. Figure 3: Comparison of Whitebox Tools and SAGA GIS flow direction formats. In WBT, all combinations of multi-directional water flow can be expressed as a unique number using base-2. In this example, the value of the centre cell (dark blue) is equal to 44, which is the sum of flow to its W (32), S (8) and SE neighbours (4). In SAGA GIS, the same flow distribution is equal to 13, which is the sum of W (6), S (4) and SE flow (3). However, this same value (13) can also be obtained with flow to the NE, NW and SW (among others!). For this practical, well be calculating the pointer file using the simplest D8 algorithm, as shown in Practical 1. Using your experience from Practical 1, run the wbt_d8_pointer function, using mersey_dem_fill.tif as the input file and saving with an appropriate output name (mersey_dem_D8_pointer.tif) and to the correct directory (output/practical_2). Remember: use the here package for accessing file locations. Load the raster into R using the raster package and plot using ggplot2 and ggspatial. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. Save your script before continuing. 9.1.2 Catchment area Using this pointer information, we can calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. For now, well be calculating the Specific Contributing Area (SCA), which utilises the Upslope Contributing Area (UCA) and Number of Upslope Elements (NUE) metrics: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment and is unaffected by the grid resolution of the DEM. In Practical 1, we used the filled DEM to calculate the SCA for each raster cell using the wbt_d8_flow_accumulation function, but we can also use the D8 pointer raster we just created: Run the wbt_d8_flow_accumulation function, using the D8 pointer raster as the input file (mersey_dem_D8_pointer.tif). Important: because were working with a pointer raster, rather than a DEM, we need to specify this parameter (pntr = TRUE). If we dont set this parameter, WBT will assume the DEM values are actually elevations (ranging from 1 - 128 m) and we will obtain spurious results! Make sure to set the output type (out_type) as specific contributing area (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. mersey_dem_flow_accumulation.tif) and to the correct sub-directory (output/practical_2). Load the raster to R and plot, and your output should resemble the following: 9.2 Task 2: Seed points The UK Environment Agency (EA) collects river water quality data at a series of fixed locations across the Mersey Basin. The location of each of these sites is recorded using a GPS and locations have a unique identification number. Seed points are sometimes referred to as pour points, and are routinely used to delineate watersheds. The locations of the EA monitoring stations are shown below: 9.2.1 Seed repositioning However, before you can delineate the watershed for each of these outlet points, the seed point location must be checked, repositioned if necessary and given its own unique identifier. This is a very important part of any watershed delineation process. Below is a figure showing the flow accumulation raster (mersey_dem_flow_accumulation.tif) and the EA monitoring stations (mersey_EA_sites.shp). Here, were using the raster package to load the raster data, and the st_read() function from the sf package (Simple Features) to load the vector data. To add spatial data to our ggplot map, we are using the annotation_spatial() function which is designed for spatial geometries. Helpfully, this works across the main vector types; points, lines, and polygons can all be added using geom_sf(). For design reasons, weve modified the shape type as well as the fill, colour and size of the point data. # Loads flow accumulation raster using the raster and here packages mersey_accumulation &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_flow_accumulation.tif&quot;)) # Loads EA monitoring sites using the st_read function seed_points &lt;- st_read(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites.shp&quot;)) ## Reading layer `mersey_EA_sites&#39; from data source ## `C:\\Users\\44797\\Desktop\\Repo\\GEOG-70581\\data\\practical_2\\mersey_EA_sites.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 70 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 345751 ymin: 368724 xmax: 404810 ymax: 427178 ## Projected CRS: OSGB 1936 / British National Grid # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_accumulation, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = seed_points, shape = 21, fill = &quot;#FFFFFF&quot;, colour = &quot;black&quot;, size = 3) + # Adds vector layer theme_classic() + labs(fill = &quot;Log-transformed flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p Inspect the above code and implement it in your R script. At this map scale, it is quite difficult to assess the correspondence between the accumulation raster and the monitoring stations. To address this, add the following code before your ggplot2 call and update the layer_spatial() argument to include this new raster. Here, we are cropping our flow accumulation raster using indexing, extracting rows 632 - 645 and columns 540 - 578. There are, however, more sophisticated approaches to cropping (e.g. using the extent of other spatial layers) but this is reasonably efficient for now. # Crops the accumulation raster based on a defined extent (row and col indexes) mersey_accumulation_crop &lt;- crop(mersey_accumulation, extent(mersey_accumulation, 632, 645, 540, 578)) As the plot extent is determined by the extent of the feature/layer specified in the layer_spatial() argument, your map should now resemble the following: Inspect the map. You should notice that while some vector points sit directly on raster cells with high accumulation values, others are located just off, often by only one or two grid cells. These errors could be due to imprecise GPS coordinates, the resolution of the DEM, or limitations of the analytical approach (i.e. flow routing), particularly in areas of low lying topography. To ensure accurate watershed creation, the sites that lie off the stream network need to be correctly repositioned. While it is possible to rectify these errors manually i.e. by checking each point in turn and assigning new coordinates if necessary, this approach would be extremely time consuming with large datasets and could introduce unnecessary bias or human error. One common approach to reposition individual points is known as pour point snapping, which relocates points to the nearest stream or area of high accumulation, within a specified threshold distance. See Lindsay et al. (2008) for an overview of snapping approaches. Whitebox Tools provides two primary methods for snapping: Snap Pour Points, using the wbt_snap_pour_points function, which moves each point to the location coincident with the highest flow accumulation value within a specified distance; Jenson Snap Pour Points, using the wbt_jenson_snap_pour_points function, which moves each point the nearest stream grid cell within a specified distance following Jenson (1991). Of these, the Jenson Snap Pour Points approach (Lindsay et al. (2008)) is more sophisticated and has one clear advantage: when an initial outlet point is located on a stream cell, it will not be moved. For our EA dataset, which contains points of variable accuracy, this is beneficial because correctly placed points will retain their original positions, and only those incorrectly placed will be modified. In contrast, a standard snap pour points approach will often move all the points, even those which were located correctly! However, the Jenson approach requires additional inputs and utilises a streams raster file, rather than an SCA accumulation file. To calculate this, the overall process is as follows: Calculate the Number of Upslope Elements (NUE) using the wbt_d8_flow_accumulation function. Define the stream network using the wbt_extract_streams function. Reposition EA seed points using the wbt_jenson_snap_pour_points function. Save your script before continuing. 9.2.1.1 Number of Upslope Elements Run the wbt_d8_flow_accumulation tool, using the original filled DEM as the input file (pntr = FALSE) and saving with an appropriate name mersey_dem_D8_flow_accumulation_NUE.tif and in the correct location. Set the output type as cells (i.e. the number of upslope cells) and dont log-transform values. 9.2.1.2 Stream network Next, we are going to extract the stream network based on the NUE raster created in Step 1. To extract streams, we first need to set the channelization threshold. This threshold represents the minimum area required to initiate and maintain a channel. Small threshold values will result in more extensive stream networks, while larger threshold values will extract only the largest streams. Unfortunately, there is no widely accepted rule for determining the channelization threshold, which will be unique for each landscape and dataset, and may vary due to raster resolution. As a result, experimentation is required! For our dataset, we want to ensure that each EA measurement site is located on or near a digitised stream, so setting a large channelization threshold and focusing on the largest streams might not be appropriate, as some measurement sites are located on minor streams in small catchments. As such, we need to set the channelization threshold sufficiently low to mitigate against this. Feel free to experiment with this threshold and evaluate its effects on the delineated stream network. For our analysis, a channelization threshold of 200 upslope grid cells would be appropriate. Using the output of Step 1, we are going to extract the stream network using wbt_extract_streams with a channelization threshold of 200 upslope grid cells (threshold = 200). Set the background value to 0 (zero_background = TRUE). As usual, save with an appropriate name (e.g. mersey_dem_streams_act200.tif) and in the correct location. Your output should resemble the following, whether raster cells corresponding to streams are coded as 1 and non-stream regions are coded as 0: 9.2.1.3 Snapping Finally, we are going to reposition the EA seed points to match the stream network. Remember, seed points already located on the steam network will not be moved. Before we execute the tool, we need to decide on the Snap Distance, which is measured in the units of the input data and is the maximum distance that points can be moved to match the stream network. Our raster is measured in metres (m) with a grid resolution of 50 m2. Most points are incorrectly positioned by one to three grid cells (50 - 150 m). However, to ensure we include all points, well set the snap distance to 500 m. Using the output of Step 2 (mersey_dem_streams_act200.tif) and the EA monitoring stations shapefile mersey_EA_sites.shp, run the wbt_jenson_snap_pour_points function, using a snap distance of 500 m (snap_dist = 500). Save with an appropriate name mersey_EA_sites_snapped.shp. Run the following code to inspect the outputs of Jenson Snap Pour Points. The stream network is a Boolean (logical) raster, where stream cells = 1 (yellow) and non-stream cells = 0 (blue). The original pour points are shown in white, with the snapped points in red. # Loads streams raster using the raster and here packages mersey_streams &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_streams_act200.tif&quot;)) # Crops the streams raster based on a defined extent (row and col indexes) mersey_streams_crop &lt;- crop(mersey_streams, extent(mersey_streams, 632, 645, 540, 578)) # Loads original and snapped EA monitoring sites using the st_read function seed_points &lt;- st_read(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites.shp&quot;)) snapped_seed_points &lt;- st_read(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites_snapped.shp&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_streams_crop, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = seed_points, shape = 21, fill = &quot;#FFFFFF&quot;, colour = &quot;black&quot;, size = 3) + # Adds seeds layer annotation_spatial(data = snapped_seed_points, shape = 21, fill = &quot;#FB5858&quot;, colour = &quot;black&quot;, size = 3) + # Adds snapped seeds layer theme_classic() + labs(fill = &quot;Stream network ID&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p 9.3 Task 3: Watershed creation To finish the Week 10 class (Chapter 9), we are going to combine the previous outputs to create unique (non-overlapping) watersheds for each EA seed point. Use the wbt_watershed function with the D8 pointer raster (mersey_dem_D8_pointer.tif) and the snapped pour points vector (mersey_EA_sites_snapped.shp) as inputs. Save in the output/practical_2 directory as mersey_watersheds.tif. Your output should resemble the following: # Loads streams raster using the raster and here packages mersey_watersheds &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.tif&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_watersheds, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = snapped_seed_points, shape = 21, fill = &quot;#FB5858&quot;, colour = &quot;black&quot;, size = 3) + # Adds snapped seeds layer theme_classic() + labs(fill = &quot;Stream network&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p The watersheds are currently stored as rasters, but it is often easier to work with vectors, especially when dealing with attributes and spatial relationships (topology). Use the following code to convert your watersheds raster (.tif) to vector format (.shp) # Converts watershed to vector format (polygon) wbt_raster_to_vector_polygons(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.tif&quot;), here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.shp&quot;)) These delineated watersheds can now be used for further analysis. However, it is important that we carefully assess them for any errors. This typically requires in-depth knowledge to assess whether the correct watershed(s) has been extracted. In turn, the position of seed points is critical, as placing seeds points above/below stream confluences can have a major impact on the extracted watershed(s), as shown in Figure 4: Figure 4: The impact of seed point location on extracted watersheds. (A) In this example, rivers A and B are sourced from distinct watersheds but meet at a confluence (white circle). (B) When seeds points are placed above this confluence, each river has its own individual watershed. (C) When a seed point is placed at or below the confluence, these watersheds are combined as all water falling within both will ultimately drain to the confluence. Prior knowledge of the catchment is required to know if the correct watershed has been extracted. To finish, save your script and compare your outputs to those shown below. If there are any clear differences in your version, repeat the previous tasks and play close attention to the input settings. References "],["mersey_four.html", "Chapter 10 Mersey IV - Surface derivatives 10.1 Task 4: Reclassification 10.2 Task 5: Calculating surface derivatives 10.3 Task 6: Extracting surface derivatives", " Chapter 10 Mersey IV - Surface derivatives 10.1 Task 4: Reclassification We know from the lecture that some of the factors which influence river hydrochemistry include land cover, soil type and bedrock geology. Information on these characteristics for the Mersey region under study is contained in the categorical files mersey_LC (based on LCM2000 data), mersey_HOST (Hydrology of Soil Types) and mersey_bedrock respectively. These datasets contain many different detailed classes, some of which are not applicable to the study region. Therefore, the datasets need to be simplified by aggregating some classes and omitting unnecessary classes. Well illustrate the process for the land cover raster, which you can then repeat for the soil type and bedrock rasters. 10.1.1 Land cover There are 26 classes in LCM2000 data, each with a unique numeric identifier. We are going to simplify these into the following five macro-classes, alongside their numeric identifiers: Arable = 41, 42, 43; Heath = 91, 101, 102; Grassland = 51, 52, 61, 71, 81; Urban = 171, 172; Wetland = 111, 121. To do so, were going to use the fct_collapse function from the forcats package. First, load the land cover raster into R as normal: # Loads land cover raster land_cover &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_LC.tif&quot;)) Because our raster is categorical (rather than continuous), it makes sense to convert the data format to a factor. In R, these are used to represent categorical variables. Convert the land cover raster to a factor as follows: # Converts the land cover raster to a factor, overwriting the original variable land_cover &lt;- as.factor(land_cover) To assess the values stored in the land_cover raster, use the unique function, which should produce the following: ## [1] -9999 0 11 21 41 42 43 51 52 61 71 81 ## [13] 91 101 102 111 121 131 161 171 172 191 211 212 ## [25] 221 As not all categories are applicable to our study area, we are next going to create a data frame of the land cover categories of interest. Inspect the code below. Many of the code elements should be familiar to you. We are creating a vector called categories using the c() function, which contains all the classes of interest c(41, 42, 43, ...)). Next, we have converted that to a data frame using as.data.frame() and assigned a column name (ID). When you understand what is happening, add to your script and run. # Categories of interest categories &lt;- as.data.frame(c(41, 42, 43, 91, 101, 102, 51, 52, 61, 71, 81, 171, 172, 111, 121)) colnames(categories) &lt;- &quot;ID&quot; If you want to inspect the output, you can use head(categories) to print out the first 6 rows: ## ID ## 1 41 ## 2 42 ## 3 43 ## 4 91 ## 5 101 ## 6 102 Using this new data frame, we are going to create a new column called name, which corresponds to the name of the land cover class (e.g. \\(Arable = 41\\)) # Collapse categories into groups based on ID categories$name &lt;- fct_collapse(as.factor(categories$ID), &quot;Arable&quot; = c(&quot;41&quot;, &quot;42&quot;, &quot;43&quot;), &quot;Heath&quot; = c(&quot;91&quot;, &quot;101&quot;, &quot;102&quot;), &quot;Grassland&quot; = c(&quot;51&quot;, &quot;52&quot;, &quot;61&quot;, &quot;71&quot;, &quot;81&quot;), &quot;Urban&quot; = c(&quot;171&quot;, &quot;172&quot;), &quot;Wetland&quot; = c(&quot;111&quot;, &quot;121&quot;)) Inspect the above code. The syntax is reasonably complex, but you should understand what is happening if you inspect the output: # Prints categories data frame categories ## ID name ## 1 41 Arable ## 2 42 Arable ## 3 43 Arable ## 4 91 Heath ## 5 101 Heath ## 6 102 Heath ## 7 51 Grassland ## 8 52 Grassland ## 9 61 Grassland ## 10 71 Grassland ## 11 81 Grassland ## 12 171 Urban ## 13 172 Urban ## 14 111 Wetland ## 15 121 Wetland Finally, we can use this updated data frame to replace (or substitute) values in the land cover raster (i.e. \\(41, 91, ...\\)) with the land cover class it represents. In this case, values are stored numerically (i.e. \\(Arable =1,Heath =2, ...\\)). One way to achieve this is using the subs function from the raster package. Inspect the following code, which substitutes (reclassifies) the raster layer and saves to a new raster (.tif) using the writeRaster function. # Substitutes raster values with new categories land_cover_classified &lt;- subs(land_cover, categories) # Write to new raster writeRaster(land_cover_classified, here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;)) When you understand it, run the code, load the new raster into R and plot. Use the code below to visualise the reclassified raster, taking note of the updated fill aesthetic (scale_fill_distiller): # Loads land cover raster using the raster and here packages mersey_land_cover &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_land_cover, aes(fill = stat(band1))) + # Adds raster layer theme_classic() + labs(fill = &quot;Land cover class&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_distiller(palette = &quot;RdYlBu&quot;, na.value = NA) + # Updated fill aesthetic theme(legend.position = &quot;top&quot;) p In the above image, the land cover classes are shown as follows: Arable = 1 (dark blue) Heath = 2 (light blue) Grassland = 3 (yellow) Urban = 4 (orange) Wetland = 5 (red) First, save your script before continuing. Next, using the methodology outlined above, repeat this process for the soil type (mersey_HOST) and bedrock geology rasters (mersey_bedrock). Make sure to use a consistent approach to file naming e.g. mersey_HOST_reclass.tif and mersey_bedrock_reclass.tif. 10.1.2 Hydrology of Soil Types (HOST) There are 29 classes in HOST, each with a unique numeric identifier. Reclassify these into the following four new classes: Permeable = 1, 3, 4, 5, 6, 7, 15, 16; Impermeable = 12, 17, 18, 19, 21, 22; Gleyed = 8, 9, 13, 24; Peats = 10, 11, 14, 25, 26, 27, 29. Utilising the order above, the values of the output raster should be as follows: Permeable = 1 Impermeable = 2 Gleyed = 3 Peats = 4 10.1.3 Bedrock geology There are 34 bedrock geology classes in Mersey Basin region, each with a unique numeric identifier. Reclassify into the following three new classes listed below: Sands_and_Muds (sands and muds) = 5, 16, 18, 24, 28, 34; Limestone = 10, 11, 17, 19; Coal = 9, 15, 22. Utilising the order above, the values of the output raster should be as follows: Sands_and_Muds = 1 Limestone = 2 Coal = 3 When complete, save your script before continuing. 10.2 Task 5: Calculating surface derivatives As well as the factors outlined above, other catchment characteristics may affect river hydrochemistry. Here, we are interested in the effects of elevation (the raw data from mersey_DEM_fill) and rainfall (mersey_rainfall), as well as topographic slope and aspect. These are known as surface derivatives as they are calculated (derived) from the DEM. To calculate slope and aspect rasters, use the wbt_slope and wbt_aspect functions, using the original filled DEM as the input data (mersey_dem_fill.tif) and using appropriate output names (e.g. mersey_dem_slope and mersey_dem_aspect). Your outputs should resemble the following: 10.3 Task 6: Extracting surface derivatives At this stage of the analysis we have all the relevant spatial datasets compiled. The next step is to derive the characteristics for each of the 70 catchments in the mersey_watersheds file (.shp), so we can relate these to the water quality data collected at each of the 70 monitoring sites. For each catchment we want to extract the: The area (km2); The number of raster cells; Average elevation; Average slope; Average aspect; Average rainfall; Percentage of the each of the five land cover classes present; Percentage of the each of the four soil types present; Percentage of the each of the three bedrock geology types present. Overall, we will calculate the average of the continuous datasets (elevation, slope, aspect, rainfall), and percentages of the categorical datasets (land cover, soil types, geology). First, however, we need to link our mersey_watersheds file with the measurements of water quality, currently stored in a comma-separated file (mersey_EA_chemisty.csv). 10.3.1 Water quality measurements To begin: Load the mersey_watersheds.shp file into R using the st_read function, storing in a variable called watersheds. Next, print out attribute names for the shapefile as follows: colnames(watersheds) ## [1] &quot;FID&quot; &quot;VALUE&quot; &quot;geometry&quot; For our analysis, the attribute of interest is VALUE, which contains the unique Environment Agency ID for each watershed. Importantly, this is also found in the mersey_EA_chemisty.csv file. This will enable us to join the two datasets, populating the attribute table of the watersheds variable with the water quality measurements stored in the csv. To simplify this, use the following code to replace the column name VALUE with a new name Seed_Point_ID. The latter is used in the mersey_EA_chemisty.csv. # Replaces column name &#39;VALUE&#39; with &#39;SEED_Point_ID&#39; names(watersheds)[names(watersheds) == &#39;VALUE&#39;] &lt;- &#39;Seed_Point_ID&#39; You can re-use the colnames function to check it worked correctly: With this updated: We can now load the Environment Agency data using read.csv(), as shown in Chapter 3: # Loads csv using read.csv ea_data &lt;- read.csv(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_chemistry.csv&quot;)) and merge using the merge function: # Merge based upon matching Seed_Point_IDs watersheds_ea &lt;- merge(watersheds, ea_data, by = &quot;Seed_Point_ID&quot;) Use the head() function to inspect the first few rows of our new data frame. ## Simple feature collection with 6 features and 14 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 343660.8 ymin: 382963.7 xmax: 353552.6 ymax: 394555.6 ## Projected CRS: OSGB 1936 / British National Grid ## Seed_Point_ID FID EA_ID Group Ph SSC Ca Mg NH4 NO3 NO2 ## 1 1 54 1940214 Training 7.79 21.37 60.82 11.12 0.24 2.64 0.08 ## 2 2 42 1941025 Testing 7.79 33.52 75.45 17.52 4.46 3.24 0.11 ## 3 3 40 1941017 Training 8.55 11.69 58.54 20.50 0.25 0.83 0.02 ## 4 4 47 1941007 Training 7.71 34.06 96.83 46.33 0.24 3.65 0.07 ## 5 5 45 1941002 Training 8.08 70.81 141.98 85.85 0.40 4.47 0.05 ## 6 6 44 1941003 Training 8.12 34.00 174.49 86.27 0.21 2.69 0.06 ## TON PO4 Zn geometry ## 1 2.73 0.34 50.00 MULTIPOLYGON (((345159.5 38... ## 2 3.35 0.99 20.51 MULTIPOLYGON (((348107.1 39... ## 3 0.84 0.07 35.23 MULTIPOLYGON (((346108.7 39... ## 4 3.73 0.21 74.26 MULTIPOLYGON (((350505.1 39... ## 5 4.52 0.14 20.16 MULTIPOLYGON (((351854 3906... ## 6 2.75 0.15 18.27 MULTIPOLYGON (((352903.1 39... Save your script before continuing. 10.3.2 Spatial areas With our datasets now linked, we may want to calculate the area of our watersheds, expressed as either km2 or as a count of raster cells. To calculate the km2 area, we can use the st_area() function. At its most simple, it could be written as follows: # Calculates area geometry using st_area() watersheds_ea$area &lt;- st_area(watersheds_ea) However, because our vector data is measured in metres (due to the British National Grid), our calculated area would also be in metres. Given the size of the watersheds, this could result in large, unwieldy values. To simplify, well use the set_units function from the units package as follows, which will store our area in a more manageable km2 format: # Calculates area geometry using st_area(), converting to km^2 using the units package watersheds_ea$area &lt;- set_units(st_area(watersheds_ea), km^2) Read the above code, add to your script and run. The procedure for calculating the count of raster cells is slightly more complicated, because this depends on the spatial resolution of the raster layer. Our raster layer contains cells of approximately ~50 m2. Here we can use the extract function from the raster package, which is described here. Well be using this function on a number of occasions in the remainder of this practical, so its important that you understand what its doing. Broadly, the function extracts values from a raster object at the locations of spatial vector data, where the value of interest is user-defined. For example, this could be the mean (e.g. the average elevation of a DEM within a vector polygon), the count (e.g. the number of cells within a vector polygon), or a minimum or maximum (e.g. the maximum elevation within a vector polygon). To calculate the number of raster cells within each watershed, we first need to load the DEM into R: # Load elevation raster mersey_dem &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_dem_fill.tif&quot;)) before using the extract function as follows. This takes in both raster (mersey_dem) and vector input data (watersheds_ea), where the value of interest is determined by the fun parameter (i.e. a function). Normally, we can specify an existing base R function (e.g. mean) but here we are using a user-defined function fun=function(x, ...) length(x) to count the length (or number) of raster cells for each watershed. # Calculates the number of raster cells per watershed watersheds_ea$count &lt;- extract(mersey_dem, watersheds_ea, fun=function(x, ...) length(x)) Run the above code, which should create a new attribute column called count. This can be previewed using head(), specifying the column of interest ($count): 10.3.3 Continuous derivatives Before we move on to extract our continuous derivatives (average elevation, rainfall, slope and aspect), it is worth noting that R variables can be removed from the environment as follows: # Removes object(s) from memory rm(mersey_dem) This can be useful if R is running slowly. To extract continuous derivatives, we are going to use the extract function again. First, ensure the DEM, rainfall, slope and aspect rasters are loaded into R and stored with sensible variable names (e.g. mersey_dem, mersey_rainfall, mersey_slope, mersey_aspect). The code to extract the average elevation for each watershed is shown here, using the function mean: # Extracts raster values for each watershed, calculates mean (fun=mean), and stores in attribute table ($average_elevation) watersheds_ea$average_elevation &lt;- extract(mersey_dem, watersheds_ea, fun=mean, na.rm=TRUE) When youre happy you understand it, copy to your script and run, before repeating the process for the other continuous variables, and storing the data using sensible attribute names e.g. watersheds_ea$average_elevation, $average_rainfall, $average_slope, $average_aspect. Use the head() function to inspect the output. 10.3.4 Calculating categorical derivatives As we approach the end of this part of Practical 2, we are going to extract and normalise (convert to %) the categorical derivatives (land cover, soil types, bedrock). First, ensure the reclassified land cover, soil type and bedrock rasters are loaded into R and stored with sensible variable names (e.g. land_cover, soils, bedrock). Next, we are going to use the extract function again, but this time returning the count of each category (e.g. Arable, Heath, Grassland, Urban, Wetland) for each watershed area, as shown here: # Extract land cover counts (5 classes so levels = 1:5) land_cover_classes &lt;- extract(land_cover, watersheds_ea, fun=function(i,...) table(factor(i, levels = 1:5))) Rather than trying to decipher the code straight away, copy to your script, run and use head() to inspect the output, which should be as follows. This may take a little while (~30 seconds on a i7 computer with 16 Gb of RAM): head(land_cover_classes) ## 1 2 3 4 5 ## [1,] 379 0 340 785 0 ## [2,] 1832 0 984 861 0 ## [3,] 42 0 491 524 0 ## [4,] 177 0 49 249 0 ## [5,] 185 0 55 57 0 ## [6,] 154 0 61 246 0 We have produced a data frame with 5 columns (representing the 5 land cover classes) and 70 rows (representing the 70 watersheds), where the row-column values represent the number of raster cells corresponding to each land cover class. This is based upon a user-defined function, incorporating the table function from the data.table package. Important: as the land cover dataset contains 5 classes, the function splits the underlying data into five groups using levels = 1:5. This needs to be updated when applying to the soils and bedrock datasets. To improve the readability of the data frame, update its column names as follows: colnames(land_cover_classes) &lt;- c(&quot;Arable&quot;, &quot;Heath&quot;, &quot;Grassland&quot;, &quot;Urban&quot;, &quot;Wetland&quot;) Use head() to inspect the output: head(land_cover_classes) ## Arable Heath Grassland Urban Wetland ## [1,] 379 0 340 785 0 ## [2,] 1832 0 984 861 0 ## [3,] 42 0 491 524 0 ## [4,] 177 0 49 249 0 ## [5,] 185 0 55 57 0 ## [6,] 154 0 61 246 0 Replicate this approach for the soil type and bedrock datasets, using sensible variable names (e.g. soils_classes and bedrock_classes), remembering to update the extract function used (i.e. levels = 1:4 for the soil data and levels = 1:3 for the bedrock data). When complete, you should have the following data frames in your R environment: watersheds_ea: containing the water quality measurements, the spatial areas and the continuous derivatives; land_cover_classes: containing the count (number of cells) for each land cover class; soils_classes: containing the count (number of cells) for each soil type; bedrock_classes: containing the count (number of cells) for each bedrock type; To simplify your R environment, remove all other variables using the rm() function. To merge the remaining files, we can use cbind(), which binds data frames together based on their columns: # Combines watersheds data frame with categorical counts watersheds_ea &lt;- cbind(watersheds_ea, land_cover_classes, soils_classes, bedrock_classes) 10.3.5 Normalising categorical derivatives In the final step of the practical, we are going to normalise our categorical derivatives i.e. to establish the percentage cover of each category, rather than a raw count of raster cells. The former is more informative as it allows us to compare watersheds of differing sizes. This is a relatively simple calculation and involves dividing the total number of raster cells in each watershed (stored in $count) by the number corresponding to each category (stored in $Arable, $Heath, $Grassland, ), before multiplying by 100. However, we have 12 categorical variables to normalise. We could type out each calculation manually e.g. # Normalising categorical variables watersheds_ea$Arable_percent &lt;- watersheds_ea$Arable/watersheds_ea$count*100 watersheds_ea$Heath_percent &lt;- watersheds_ea$Heath/watersheds_ea$count*100 atersheds_ea$Grassland_percent &lt;- watersheds_ea$Grassland/watersheds_ea$count*100 ... However, we dont want to waste time or effort if we could perform these calculations iteratively. To that end, well start by: Creating a vector of the column names we want to normalise: # Creates vector of categorical variables categorical_names &lt;- c(&quot;Arable&quot;, &quot;Heath&quot;, &quot;Grassland&quot;, &quot;Urban&quot;, &quot;Wetland&quot;, &quot;Permeable&quot;, &quot;Impermeable&quot;, &quot;Gleyed&quot;, &quot;Peats&quot;, &quot;Sands_and_Muds&quot;, &quot;Limestone&quot;, &quot;Coal&quot;) # Prints vector categorical_names ## [1] &quot;Arable&quot; &quot;Heath&quot; &quot;Grassland&quot; &quot;Urban&quot; ## [5] &quot;Wetland&quot; &quot;Permeable&quot; &quot;Impermeable&quot; &quot;Gleyed&quot; ## [9] &quot;Peats&quot; &quot;Sands_and_Muds&quot; &quot;Limestone&quot; &quot;Coal&quot; Next, we are going to iterate through this vector using a for loop. This was illustrated briefly in Chapter 3 here, but well explain it more fully now: Copy the following code to your script and run: # Loops through each element of categorical_names and stores it in variable &quot;i&quot; for (i in categorical_names){ # Prints element stored in i print(i) } ## [1] &quot;Arable&quot; ## [1] &quot;Heath&quot; ## [1] &quot;Grassland&quot; ## [1] &quot;Urban&quot; ## [1] &quot;Wetland&quot; ## [1] &quot;Permeable&quot; ## [1] &quot;Impermeable&quot; ## [1] &quot;Gleyed&quot; ## [1] &quot;Peats&quot; ## [1] &quot;Sands_and_Muds&quot; ## [1] &quot;Limestone&quot; ## [1] &quot;Coal&quot; The code works by looping (or iterating) through each element of the categorical_names vector. On every iteration, the variable i (short for iterator) is updated with the next element of categorical_names. On the first iteration, i = \"Arable\", on the second iteration i = \"Heath\", and so on. Now copy and run the following updated version: # Loops through each element of categorical_names and stores it in variable &quot;i&quot; for (i in categorical_names){ # Creates a new column name using the variable &quot;i&quot; and the string &quot;percent&quot;, separated by an underscore. col &lt;- paste(i, &quot;percent&quot;, sep=&quot;_&quot;) # Prints new column name print(col) } ## [1] &quot;Arable_percent&quot; ## [1] &quot;Heath_percent&quot; ## [1] &quot;Grassland_percent&quot; ## [1] &quot;Urban_percent&quot; ## [1] &quot;Wetland_percent&quot; ## [1] &quot;Permeable_percent&quot; ## [1] &quot;Impermeable_percent&quot; ## [1] &quot;Gleyed_percent&quot; ## [1] &quot;Peats_percent&quot; ## [1] &quot;Sands_and_Muds_percent&quot; ## [1] &quot;Limestone_percent&quot; ## [1] &quot;Coal_percent&quot; Here we are iteratively creating a column name based on the element stored in the variable i i.e. \"Arable_percent\", \"Heath_percent\", \"Grassland_percent\". In this final version of the for loop, we are creating new columns in the watersheds_ea data frame based on the column name (col) and the normalisation approach described above (i.e. Arable/count*100). However, rather than having to specify the input and output columns manually, this is handled for us iteratively using the col and i variables: # Loops through each element of categorical_names and stores it in variable &quot;i&quot; for (i in categorical_names){ # Creates a new column name using the variable &quot;i&quot; and the string &quot;percent&quot;, separated by an underscore. col &lt;- paste(i, &quot;percent&quot;, sep=&quot;_&quot;) # Updates watersheds_ea with the percentage cover of each category watersheds_ea[col] &lt;- as.numeric(watersheds_ea[[i]]/watersheds_ea$count*100) } When youre happy you understand the above code, run it. To finish the practical, were going to remove the geometry stored in the data frame (not required for subsequent analysis) before saving as a comma-separated file: # Drops geometry attribute from watersheds_ea watersheds_ea &lt;- st_drop_geometry(watersheds_ea) # Writes data frame to comma-separated file write.csv(x = watersheds_ea, here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds_ea.csv&quot;), row.names=FALSE) Use head() to inspect the final output, which should resemble the following: ## Seed_Point_ID FID EA_ID Group Ph SSC Ca Mg NH4 NO3 NO2 ## 1 1 54 1940214 Training 7.79 21.37 60.82 11.12 0.24 2.64 0.08 ## 2 2 42 1941025 Testing 7.79 33.52 75.45 17.52 4.46 3.24 0.11 ## 3 3 40 1941017 Training 8.55 11.69 58.54 20.50 0.25 0.83 0.02 ## 4 4 47 1941007 Training 7.71 34.06 96.83 46.33 0.24 3.65 0.07 ## 5 5 45 1941002 Training 8.08 70.81 141.98 85.85 0.40 4.47 0.05 ## 6 6 44 1941003 Training 8.12 34.00 174.49 86.27 0.21 2.69 0.06 ## TON PO4 Zn area count average_elevation average_rainfall ## 1 2.73 0.34 50.00 3.9614467 [km^2] 1587 21.51344 539.8778 ## 2 3.35 0.99 20.51 10.2618193 [km^2] 4111 39.95542 581.1832 ## 3 0.84 0.07 35.23 3.6619044 [km^2] 1467 66.75006 591.3545 ## 4 3.73 0.21 74.26 1.2131462 [km^2] 486 44.82042 549.0123 ## 5 4.52 0.14 20.16 0.7413671 [km^2] 297 43.72755 557.1751 ## 6 2.75 0.15 18.27 1.4028564 [km^2] 562 49.89805 562.9039 ## average_slope average_aspect Arable Heath Grassland Urban Wetland Permeable ## 1 0.4947966 94.18135 379 0 340 785 0 0 ## 2 1.2605843 205.45723 1832 0 984 861 0 271 ## 3 1.9887265 195.48206 42 0 491 524 0 292 ## 4 0.5363760 140.63359 177 0 49 249 0 0 ## 5 1.6656744 120.61153 185 0 55 57 0 0 ## 6 1.1076461 118.44821 154 0 61 246 0 0 ## Impermeable Gleyed Peats Sands_and_Muds Limestone Coal Arable_percent ## 1 0 1030 557 1414 0 0 23.881537 ## 2 0 3463 377 4111 0 0 44.563367 ## 3 0 1175 0 1467 0 0 2.862986 ## 4 0 486 0 486 0 0 36.419753 ## 5 0 297 0 297 0 0 62.289562 ## 6 0 562 0 562 0 0 27.402135 ## Heath_percent Grassland_percent Urban_percent Wetland_percent ## 1 0 21.42407 49.46440 0 ## 2 0 23.93578 20.94381 0 ## 3 0 33.46967 35.71915 0 ## 4 0 10.08230 51.23457 0 ## 5 0 18.51852 19.19192 0 ## 6 0 10.85409 43.77224 0 ## Permeable_percent Impermeable_percent Gleyed_percent Peats_percent ## 1 0.00000 0 64.90233 35.097669 ## 2 6.59207 0 84.23741 9.170518 ## 3 19.90457 0 80.09543 0.000000 ## 4 0.00000 0 100.00000 0.000000 ## 5 0.00000 0 100.00000 0.000000 ## 6 0.00000 0 100.00000 0.000000 ## Sands_and_Muds_percent Limestone_percent Coal_percent ## 1 89.09893 0 0 ## 2 100.00000 0 0 ## 3 100.00000 0 0 ## 4 100.00000 0 0 ## 5 100.00000 0 0 ## 6 100.00000 0 0 In the final week of Environmental Modelling and Monitoring Concepts (Week 12, Mersey V), well use this data frame to evaluate the environmental controls on water quality. "],["mersey_five.html", "Chapter 11 Mersey V - Statistical analysis 11.1 Task 7: Model building 11.2 Task 8: Model evaluation", " Chapter 11 Mersey V - Statistical analysis In this final chapter, we will compare the information about catchment characteristics with the water quality data collected at each of the 70 monitoring stations. To begin, load the csv file created at the end of Task 6 (mersey_watersheds_ea.csv), saving to a new variable called watersheds_df: # Reads completed file from csv watersheds_df &lt;- read.csv(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds_ea.csv&quot;)) If you have any other variables in your R environment, these can be removed using rm(). 11.1 Task 7: Model building This data frame should contain the following 10 water quality indicators for each watershed: pH: acidity/alkalinity; SSC: suspended solids concentration; Ca: calcium; Mg: magnesium; NH4: ammonium; NO3: nitrate; NO2: nitrite; TON: total oxidised nitrogen; PO4: phosphate; Zn: zinc. It should also contain the continuous derivatives (e.g. average elevation) and categorical derivatives (e.g. land cover percentage) for each watershed. Note: some of your calculated percentages may not add up to 100%. In Task 4, we reclassified only the most important categorical variables. These are known to have the greatest impact of river hydrochemistry (e.g. urban areas, farmland). While other land cover categories are found within each watershed, these typically account for only a small percentage of the total area and have a limited effect on the river environment. These categories have been excluded to simplify the analysis. 11.1.1 An introduction to linear models in R It is now time to examine the relationships between river water quality and catchment metrics. The key model outputs that are ultimately required for the assessment are: Regression equations for each water quality variable (dependent variable; n = 10) and the key explanatory catchment characteristics (independent variables; n = 16). Associated model values (R2, p value). Remember, you dont have to run every code block shown below, but you can do so if it would help your understanding. The simplest way to run a linear regression in R is to use the lm() function, an example of which is shown below, storing the output in model (you can change this name to reflect the input variables): # Fits a linear model model &lt;- lm(formula = NO2 ~ average_elevation, data = watersheds_df) We have defined the data frame being used (data = watersheds_df) and the input variables from that data frame. This is achieved by including their column names, shown here: colnames(watersheds_df) ## [1] &quot;Seed_Point_ID&quot; &quot;FID&quot; &quot;EA_ID&quot; ## [4] &quot;Group&quot; &quot;Ph&quot; &quot;SSC&quot; ## [7] &quot;Ca&quot; &quot;Mg&quot; &quot;NH4&quot; ## [10] &quot;NO3&quot; &quot;NO2&quot; &quot;TON&quot; ## [13] &quot;PO4&quot; &quot;Zn&quot; &quot;area&quot; ## [16] &quot;count&quot; &quot;average_elevation&quot; &quot;average_rainfall&quot; ## [19] &quot;average_slope&quot; &quot;average_aspect&quot; &quot;Arable&quot; ## [22] &quot;Heath&quot; &quot;Grassland&quot; &quot;Urban&quot; ## [25] &quot;Wetland&quot; &quot;Permeable&quot; &quot;Impermeable&quot; ## [28] &quot;Gleyed&quot; &quot;Peats&quot; &quot;Sands_and_Muds&quot; ## [31] &quot;Limestone&quot; &quot;Coal&quot; &quot;Arable_percent&quot; ## [34] &quot;Heath_percent&quot; &quot;Grassland_percent&quot; &quot;Urban_percent&quot; ## [37] &quot;Wetland_percent&quot; &quot;Permeable_percent&quot; &quot;Impermeable_percent&quot; ## [40] &quot;Gleyed_percent&quot; &quot;Peats_percent&quot; &quot;Sands_and_Muds_percent&quot; ## [43] &quot;Limestone_percent&quot; &quot;Coal_percent&quot; Input variables in the formula are separated by ~, where the variable to the left is the dependent variable (NO2) and the variable to the right is an independent variable (average_elevation). We can, however, include multiple independent variables to perform multiple linear regression. This is achieved as follows, where additional independent variables are separated by +: # Fits a linear model model &lt;- lm(formula = NO2 ~ average_elevation + average_rainfall, data = watersheds_df) We can then assess the model output using the summary function: summary(model) ## ## Call: ## lm(formula = NO2 ~ average_elevation + average_rainfall, data = watersheds_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.059950 -0.015188 -0.010499 0.002269 0.226625 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.525e-02 3.198e-02 2.978 0.00403 ** ## average_elevation -2.096e-04 8.951e-05 -2.341 0.02220 * ## average_rainfall -8.358e-06 5.450e-05 -0.153 0.87857 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.04402 on 67 degrees of freedom ## Multiple R-squared: 0.3109, Adjusted R-squared: 0.2903 ## F-statistic: 15.11 on 2 and 67 DF, p-value: 3.828e-06 For this set of independent variables, we have an R2 of 0.31 (Multiple R-squared: 0.3109) and a model p value of &lt; 0.01 (p-value: 3.828e-06). The model coefficients for the independent variables are described above, where * denotes p values &lt; 0.05 (95% probability) and ** denotes p values &lt; 0.01 (99% probability). As the coefficients are very small, they are presented in scientific notation. These can be converted to numeric (non-scientific) format using the following code: format(-2.096e-04, scientific = FALSE) ## [1] &quot;-0.0002096&quot; We can supply multiple values to the format function by creating a vector: format(c(-2.096e-04, -8.358e-06, ...) , scientific = FALSE) When youre happy you understanding the formatting of the lm function, move on to the next section. 11.1.2 Training vs. Testing One limitation of the above approach is that our dataframe (watersheds_df) contains observations from all 70 EA monitoring stations. When performing statistical analysis, it is common practice to split any dataset into: a training subset, which is used to create the model(s). a testing subset, which is used to evaluate the model(s). Subsetting our data in this way allows models to be evaluated more rigorously. Many models perform well in-sample but poorly out-of-sample when evaluated against independent data (i.e. the testing subset). This is commonly referred to as over-fitting. Training and testing subsets are usually defined randomly, with an approximate ratio of 70:30 (although this varies). However, and to ensure reproducibility, this step has been completed for you: the watersheds_df dataframe contains a group variable denoting which monitoring sites belong to the training and testing subsets. Run the code above to create training and testing dataframes: # Extracts training dataset, comprising 50 observations (~70%) training &lt;- subset(watersheds_df, Group == &quot;Training&quot;) # Extracts training dataset, comprising 20 observations (~30%) testing &lt;- subset(watersheds_df, Group == &quot;Testing&quot;) Before you move on to the next section, can you think of any limitations of this approach? Hints: How important is the training-testing ratio? How are training-testing subsets created? 11.1.3 Variable selection strategies An addition weakness of the above approach is that we have manually defined the independent variables of interest (average_elevation + average_rainfall). For exploratory analysis, however, we may not know which are the most important variables. Perhaps there is a combination of independent variables which produces a better model fit (e.g. R2 &gt; 0.31)? Determining which variables to include/exclude from a model is a very difficult problem, which has resulted in many different variable selection strategies. Common approaches include expert opinion and/or theory, partial least squares (PLS) regression, implemented in PLS, Least Absolute Shrinkage and Selection Operator (LASSO), implemented in glmnet and LARS, as well as elastic net methods and ridge regression, also implemented in glmnet. You may want to explore some of these more complex approaches for your dissertation. For our analysis, we are going to use a relatively simple method known as Stepwise Regression, implemented in the MASS package. This works by including all the relevant independent variables in the analysis and then selecting those with the greatest explanatory power. However, we dont necessarily want to test all model variables. We would probably want to exclude the categorical counts (e.g. Arable, Heath, ) as these factors are already represented by the normalised variables (e.g. Arable_percent, Heath_percent, ), as well as any IDs or geometry variables (area). In general, we are only interested in testing the continuous derivatives (column names starting with average_) and the normalised categorical derivatives (column names ending in _percent). Rather than typing out the columns of interest manually, we are going to use the select function from the dplyr package to do so: # Creates a vector of column names, including only those which contain &quot;average&quot; or &quot;percent&quot; factors &lt;- colnames(watersheds_df %&gt;% dplyr::select(contains(c(&quot;average&quot;, &quot;percent&quot;)))) # Prints to console factors ## [1] &quot;average_elevation&quot; &quot;average_rainfall&quot; &quot;average_slope&quot; ## [4] &quot;average_aspect&quot; &quot;Arable_percent&quot; &quot;Heath_percent&quot; ## [7] &quot;Grassland_percent&quot; &quot;Urban_percent&quot; &quot;Wetland_percent&quot; ## [10] &quot;Permeable_percent&quot; &quot;Impermeable_percent&quot; &quot;Gleyed_percent&quot; ## [13] &quot;Peats_percent&quot; &quot;Sands_and_Muds_percent&quot; &quot;Limestone_percent&quot; ## [16] &quot;Coal_percent&quot; Run the above code. Note, the formatting of dplyr::select may be slightly confusing but it is necessary because there is also a select function in the MASS package. Here, we are telling R to use select from dplyr. Using this vector of column names, we are going to create a new data frame (called variables) containing only the independent variables of interest. Crucially, this is only for the training dataset: variables &lt;- training[factors] Run the above code and use head() to inspect the results. Next, we are going to combine this data frame (cbind) with a dependent variable of interest; we will use NO2 as an example. Our new dataframe will be called model_df as it contains all the variables (dependent + independent) required for multiple linear regression. Note: by default, cbind will (somewhat unhelpfully) rename input column names e.g. NO2 will become watersheds_df$NO2. The code below specifies the new column name as NO2 (NO2 =) for readability: # Column bind the NO2 column with the independent variables from the training dataset model_df &lt;- cbind(NO2 = training$NO2, variables) When complete, we can then run a new model, making sure to update the data frame used (data = model_df) and updating the formula to NO2 ~ .. This denotes that all other data frame columns will be included as independent variables (a useful time saver!): # Fits a linear model, including all other columns (~.) as independent variables no2_model &lt;- lm(formula = NO2 ~ ., data = model_df) When youre happy you understand the lm syntax, combine the two dataframes, run the linear model and inspect the output using summary(). This should resemble the following: ## ## Call: ## lm(formula = NO2 ~ ., data = model_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.050772 -0.015450 -0.000014 0.011843 0.078422 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.529e-01 1.541e+00 -0.099 0.9215 ## average_elevation 2.021e-04 2.073e-04 0.975 0.3367 ## average_rainfall 7.315e-05 8.250e-05 0.887 0.3817 ## average_slope -1.371e-02 7.519e-03 -1.823 0.0774 . ## average_aspect 9.968e-05 1.795e-04 0.555 0.5824 ## Arable_percent -1.306e-05 8.699e-04 -0.015 0.9881 ## Heath_percent -7.314e-04 1.155e-03 -0.633 0.5308 ## Grassland_percent -6.684e-04 8.739e-04 -0.765 0.4498 ## Urban_percent 5.588e-04 9.223e-04 0.606 0.5488 ## Wetland_percent -6.225e-04 1.020e-03 -0.610 0.5459 ## Permeable_percent 2.592e-03 1.495e-02 0.173 0.8634 ## Impermeable_percent 1.826e-03 1.489e-02 0.123 0.9032 ## Gleyed_percent 2.285e-03 1.494e-02 0.153 0.8794 ## Peats_percent 2.176e-03 1.503e-02 0.145 0.8858 ## Sands_and_Muds_percent -5.747e-04 3.790e-03 -0.152 0.8804 ## Limestone_percent 2.227e-05 3.986e-03 0.006 0.9956 ## Coal_percent -8.088e-04 3.795e-03 -0.213 0.8326 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0351 on 33 degrees of freedom ## Multiple R-squared: 0.5483, Adjusted R-squared: 0.3292 ## F-statistic: 2.503 on 16 and 33 DF, p-value: 0.01265 Our overall model fit (R2) is 0.55 which indicates that the independent variables explain ~55% of variability in the dependent variable. However, the model contains many independent variables which are not statistically significant, here defined as having a p value &gt; 0.05. This number represents the probability that the result has occurred by chance. When values are very small (e.g. p &lt; 0.0005), we would typically present these as a discrete value e.g. p &lt; 0.05, &lt; 0.01, &lt; 0.001. Generally, we only use models in which we can be 95% confident or higher (i.e. significance level of 0.05 or less). However, it is important to note that p values should be not be considered in isolation and need to be interpreted carefully. For statistical reviews of using and interpreting p values, see Goodman (2008) and Andrade (2019). For a broader overview, see the Nature commentary by Amrhein et al. (2019), as well as a summary article by Vox. To filter our independent variables to include only the most important, we can use the step.AIC function from the MASS library as follows: # Stepwise regression model step.model &lt;- stepAIC(no2_model, # Input linear model direction = &quot;both&quot;, trace = FALSE, # Print out intermediate results? k = 1) Helpfully, this takes the output of the lm model (no2_model) with no need for any additional data wrangling. The following are important parameters: direction = \"both\": Determines the method used, either forward or backward stepwise regression, or a mixture of both. Forward begins with a model with no variables and then starts adding the most significant variables, stopping when there are no more significant variables. Backward begins with a model with all variables and then starts removing the least significant variables, stopping when only significant variables are remaining. Both includes both of the above, allowing for variables to be added/removed at each step. k = 1: The number of degrees of freedom used for the penalty i.e. for determining whether variables are significant or not. Run the above model (direction = \"both\" and k = 1) and print the output using summary(): ## ## Call: ## lm(formula = NO2 ~ average_elevation + average_rainfall + average_slope + ## Urban_percent + Permeable_percent + Coal_percent, data = model_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.055180 -0.015245 -0.002834 0.015796 0.084547 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.858e-04 3.930e-02 0.023 0.982122 ## average_elevation 1.547e-04 1.487e-04 1.040 0.304078 ## average_rainfall 6.188e-05 6.636e-05 0.933 0.356265 ## average_slope -1.354e-02 6.091e-03 -2.223 0.031519 * ## Urban_percent 1.121e-03 3.095e-04 3.624 0.000763 *** ## Permeable_percent 3.445e-04 2.074e-04 1.661 0.103993 ## Coal_percent -2.604e-04 1.850e-04 -1.408 0.166415 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.03152 on 43 degrees of freedom ## Multiple R-squared: 0.5253, Adjusted R-squared: 0.459 ## F-statistic: 7.93 on 6 and 43 DF, p-value: 8.737e-06 As you can see above, using a low threshold for the degrees of freedom (k = 1) means we still have many non-significant variables remaining (p &gt; 0.05) Re-run the above model, but increasing the value of k in intervals of 1 until all the independent variables are significant at p = 0.05 (denoted by *): ## ## Call: ## lm(formula = NO2 ~ average_slope + Urban_percent, data = model_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.054086 -0.020359 -0.004355 0.012888 0.098635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0435410 0.0126033 3.455 0.00118 ** ## average_slope -0.0039045 0.0016317 -2.393 0.02076 * ## Urban_percent 0.0009823 0.0002980 3.297 0.00187 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0317 on 47 degrees of freedom ## Multiple R-squared: 0.4753, Adjusted R-squared: 0.4529 ## F-statistic: 21.28 on 2 and 47 DF, p-value: 2.622e-07 In general, we prefer models with the minimum number of parameters (independent variables). They require fewer assumptions, less intensive data collection and can be applied more confidently to new data sets/locations. This principle of model parsimony is based upon Occams Razor: other things being equal, simpler explanations are generally better than more complex ones. Our original model, based upon 16 independent variables had an R2 of 0.55. This new model, based upon just 2 independent variables (average_slope + Urban_percent) has an R2 of 0.48; a relatively minor reduction in explanatory power given the removal of 14 (arguably unimportant) additional variables. Our model coefficients are now as follows: intercept = 0.0435188, p = 0.00115 (p &lt; 0.01) average_slope = -0.0040031, p = 0.02040 (p &lt; 0.05) Urban_percent = 0.0009819, p = 0.00186 (p &lt; 0.01) Coefficients are important because they are used in regression equations, which can then be used to predict values. The general format for a regression equation is as follows: \\[ y = a + (b_1 \\cdot x_1) + (b_2 \\cdot x_2) + (b_n \\cdot x_n) \\] where a is the constant (intercept) value, and b is the coefficient of x. For our NO2 model above, we can define our regression equation (presented using sensible data precision) as: \\[ NO_2 = 0.044 + (0.001 \\cdot Urban \\: percent) + (-0.004 \\cdot Average \\: slope) \\] Well done! You have now calculated a regression which links the dependent variable (NO2) to a set of independent variables, in the case the average slope of the watershed and the percentage urban land cover. For your assessment, we would like you to explain the regression results, linking to hydrological processes and literature. Think about specific sources of pollution, transport pathways, types of flow 11.2 Task 8: Model evaluation Having created a statistical model, it is necessary to evaluate its performance. Comparison plots of measured vs. modelled (or predicted) values are one common way to assess model quality, alongside other metrics such as root-mean-square error (RMSE), normalised root-mean-square-error (nRMSE), Q-Q plots, or histograms of model residuals. You may want to explore some of these for the assessment. To calculate modelled values, we can use the predict() function, taking the model variable (step.model) as the input, rather than re-creating the above equation manually in code, and using our testing dataframe for the newdata argument: # Predict NO2 values based upon stepwise model, saving to testing dataframe testing$predicted_no2 &lt;- predict(step.model, newdata = testing) Ifnew_data is not defined, the predict function uses the fitted values for prediction i.e. the training data used to construct the model (see here). Run the above code block to predict NO2 concentrations in the testing dataset, based on the regression model produced from the training dataset. These values could be used to calculate RMSE or other metrics (nRMSE) using your own code or additional packages (e.g. Metrics); \\[ RMSE = \\sqrt{mean(measured\\:values - modelled\\:values)^2} \\] Plots of measured vs. modelled values (as well as Q-Q plots and histograms) can be created in ggplot2. Here is an example: # ggplot of measured vs. modelled (predicted) NO2 values no2_plot &lt;- ggplot(data = testing, aes(x = NO2, y = predicted_no2)) + # Adding a linear regression (&quot;lm&quot;), removing standard error bars (se = FALSE) geom_smooth(method = &quot;lm&quot;, se = FALSE, colour=&quot;#FF953C&quot;) + # Adds a 1:1 line for comparison geom_abline(intercept = 0, slope = 1, lty = &quot;dashed&quot;) + # Adds the point data, modifying the shape, size, colour and fill geom_point(shape = 21, colour = &quot;white&quot;, fill = &quot;#5695FF&quot;, size = 2.5) + # Setting the theme and aspect ratio theme_classic() + theme(aspect.ratio = 1) + # Axis limits scale_x_continuous(limits = c(0,0.3)) + scale_y_continuous(limits = c(0,0.3)) + # Add axis labels and a title labs(x = Measured~NO[2], y = Modelled~NO[2], title = Plot~of~measured~vs.~modelled~NO[2]~values) no2_plot Does the regression line match the 1:1 line? Is there any evidence of under- or over-prediction? Are there any outliers? What types of errors can you identify? You could also assess this relationship statistically, using linear regression: # Linear regression of measured vs. modelled NO2 values prediction_model &lt;- lm(formula = NO2 ~ predicted_no2, data = testing) # Print summary statistics summary(prediction_model) ## ## Call: ## lm(formula = NO2 ~ predicted_no2, data = testing) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.084258 -0.027100 0.000118 0.011208 0.152154 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.01551 0.02322 -0.668 0.51280 ## predicted_no2 1.79536 0.47103 3.812 0.00128 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05421 on 18 degrees of freedom ## Multiple R-squared: 0.4466, Adjusted R-squared: 0.4159 ## F-statistic: 14.53 on 1 and 18 DF, p-value: 0.001278 How well does our NO2 model perform on the testing dataset, based on the above graphs/statistics? Is out-of-sample performance comparable to in-sample performance? To finish the practical and to prepare for the assessment: Replicating the above approaches, calculate regression equations based on stepwise linear regression for all 10 water quality indicators (NO2, pH, SSC, Ca, Mg, NH4, NO3, TON, PO4, Zn). Use the same approach to create new data frames for each indicator, remembering to update the k parameter in the step.AIC function (beginning at k = 1) to determine the statistically significant variables. Save the relevant model coefficients and the R2 and p values for each equation. These should be stored in a single table for the assessment. References "],["Assessment.html", "Chapter 12 Assessment", " Chapter 12 Assessment Title: Modelling River Hydrochemistry Word Limit: 1500. The guidelines provided in the CO2 flux practical, which outline what is and what is not included in the word count, as well penalties for exceeding the word count, also apply here. Intended Learning Outcomes: see Mersey I - Introduction Assessment Criteria: see the Assessment Guidance section on Blackboard, which includes a marking rubric and hints-and-tips presentation. Format: Report Structure: Your report should be a coherent document, as opposed to a list of Q&amp;As as with the CO2 flux practical. Within this, how you structure the report is your decision. However, there are certain items that must be included in the main body of the report. These are the regression equations (and associated information i.e. significance values, R2 etc) for all ten water quality indicators. This could be presented as a table, perhaps in the format shown below: Indicator Equation R2 p value Additional columns as required NO2 \\(NO_2 = 0.044 + (0.001 \\cdot Urban \\: percent) + (-0.004 \\cdot Average \\: slope)\\) 0.48 &lt; 0.01 As well as these important items, the following points should be covered in the report: What are the relationships between water quality and catchment characteristics? Are there any consistent predictors of water quality? How good are the regression models? What sort of errors may occur during the derivation of catchment characteristics? How could these errors be minimised? How could you validate the models? Your regression models could be used to predict water quality in different catchments or different sub-basins of the Mersey. Can you think of any possible errors / limitations when making predictions of this type? Your report should not just be a reworked version of the practical instructions. Do not include a methodology section at all, although you may wish to reflect on the methods used when considering the points above. You may find it useful to refer to the my learning essential resources on report writing. In addition to the above, we would like you to produce three output maps to visualise the practical steps. These should be included in an Appendix at the back of the report (ideally two images per page). These should include the layers listed below but can include others to improve map design (e.g. an elevation background). a map showing the flow accumulation network (mersey_dem_flow_accumulation.tif), using log-transformed values and specific contributing areas; a map showing the watersheds in the Mersey Basin (mersey_watersheds.shp), derived from the 70 modified Environment Agency seed points; a map showing the reclassified soils raster (mersey_HOST_reclass.tif). Map design: Normally, all maps should have a scale bar and north arrow. However, because were using projected data (British National Grid) and because ggpsatial automatically adds informative latitude/longitude axes to your plots, you dont need to include either of these elements here. For reference, these can be added using annotation_scale() and annotation_north_arrow() respectively. Maps should, however, include informative legends, titles and axis labels, and should use colour-blind friendly and perceptually-uniform palettes. Referencing: The report should be fully referenced, using the Harvard style. You should not just adapt the text from the Rothwell et al. (2010) paper, substituting key values for the data here. This will result in a high similarity index and you may face a penalty due to poor academic practice. You must write up the report in your own words, making sure you paraphrase the ideas of others, and reference fully throughout. Submission deadline: 05/01/2023, 14:00 "],["Hints.html", "Chapter 13 Handy Hints", " Chapter 13 Handy Hints This chapter contains a few handy hints which may be useful if youre having any issues loading R packages or running Whitebox tools. We covered most of these during our session in Week 9 but these are included here for reference. If there are any questions regarding Assessment 2 or the practical instructions, please do get in touch at my @manchester email. Tip 1: Make sure to initialise an R project for your work (Section 5.4) and ensure this is created for your GEOG70581 directory. This should resemble the following, with the R project file and a separate R script for the Eskdale and Mersey Basin practicals: Tip 2: Remove spaces in directories or file paths. As a rule, avoid using spaces when naming files or folders as this can cause issues, particularly when coding: P:/GIS and Environmental Applications/GEOG70581/ Bad P:/GIS_and_Environmental_Applications/GEOG70581/ Good Tip 3: Make sure to read error messages (red text in the R console). If this appears, it usually means that something has gone wrong! Tip 4: Make sure to inspect output files. When you run a command (e.g. a Whitebox function), check that it produces the intended output (e.g. a raster file (.tif) in the output directory). If there is an error or the output hasnt been created, subsequent code will fail. Tip 5: If youre having difficulty loading the raster or sf packages on a University managed computer, this is due to multiple package installs. Go to the packages window, identify the older version of each package, and tick the check box to load it into your R library. Tip 6: While you will not be assessed on the quality of your code (you do not have to submit your code for the assessment), remember that well-formatted code (with comments and good structure) is easier to read and understand and will be less prone to error. Inspect the code below: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) This code includes comments for each main code block, line spaces to distinguish different parts of the code, and is written in a logical order (e.g. first loading packages, then loading/selecting data, running tools). Any non-essential code has been removed e.g. additional calls to install.packages() or library(). If there are any other questions, please do get in touch. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
