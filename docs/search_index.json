[["index.html", "Chapter 1 Introduction 1.1 Practical outline 1.2 Objectives 1.3 Schedule", " Hydrological analysis in R Chapter 1 Introduction 1.1 Practical outline In this set of practicals, we’ll be using R, RStudio and WhiteboxTools to perform hydrological analysis. In Chapters 2 and 3, we’ll install and get to grips with the required programs. In Practical 1 (Chapters 4 - 6), which uses data for the Eskdale watershed, we’ll introduce you to some of the key approaches, including DEM pre-processing, calculating flow parameters and comparing flow algorithms. In Practical 2 (Chapters 7 - 9), we’ll utilise these approaches to investigate the water quality of the Mersey Basin. This forms the basis for your assessment. Note: the relevant chapters will be made available as we progress through the semester. Prior to the semester, only the introductory page will be visible. 1.2 Objectives After completing these practicals, you will: have gained experience with new tools (e.g. R, RStudio, WhiteboxTools); become familiar with hydrological approaches used to simulate overland flow from digital elevation models (DEMs); utilised statistical analysis to investigate the relationships between water quality indicators and catchment characteristics; improved your understanding of the hydrological processes influencing water quality. 1.3 Schedule These practicals will be completed across Weeks 8 to 12 as follows: Week 8: Introductory lecture (~1 hr) Complete Chapters 1 - 3: An introduction to R (~2 hr) Weeks 9: Lecture on Hydrological and Geomorphological models (~1 hr) Complete Chapters 4 - 6: Eskdale practical (~2 hr, plus independent work) Weeks 10 - 12: Lecture on Water Quality Modelling (~1 hr) Complete Chapters 7 - 10: Mersey practical (~6 hr, plus independent work) The assessment, based upon Practical 2 (Mersey), is detailed in full on Canvas here. "],["Installation.html", "Chapter 2 Installation 2.1 R 2.2 R Studio 2.3 Course materials", " Chapter 2 Installation During this practical you will be using WhiteboxTools, an open-source GIS software package for hydro-geomorphic applications. While it contains much of the functionality found in a standard GIS software package, it also includes novel functionality specific to hydrological analysis. The tools are also computationally efficient (i.e. fast!), taking advantage of parallel processing and other advancements in computing power. Finally, the tools are developed for cross-platform use, so can be accessed through code (e.g. R, Python) or other GUIs (e.g. QGIS, ArcGIS) if required. To run WhiteboxTools functions (WBT), we’ll be using R as a front-end interface. This will allow us to access and run all of the relevant hydrological functions from WBT, while utilising the statistical and visualisation capabilities of R. 2.1 R If you’re working on a university-managed computer (in HBS 2.2, HBS 2.88 or Mansfield Cooper 2.01), both R and RStudio (described below) should already be installed or can be accessed through the Software Centre. To Install R on a personal computer: Open an internet browser and go to https://www.r-project.org/. Click the “download R” link in the middle of the page under “Getting Started.” Select a CRAN location (a mirror site) and click the corresponding link e.g. the site provided by Imperial College London. Click on the “Download R for Windows” link, or the relevant link for your operating system (macOS, Linux). Click on ‘base’ and then Download R [Version Number] for Windows. The current version is 4.5.1. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.2 R Studio While R is now installed, it is good practice to use an integrated development environment (IDE) to access and run R functions. The most well known IDE for R is RStudio, which includes a console, higher quality graphics, as well as tools for plotting, history, debugging and workspace management. Note: R may be used without RStudio, but RStudio cannot be used without R. To Install RStudio on a personal computer: Go to https://www.rstudio.com/products/rstudio/download/ and click on Download RStudio for Windows. Installers for macOS and other operating systems are available below. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.3 Course materials All the data required for this sets of practicals are available on Canvas here. Download the folder, save it to an appropriate location and extract the contents (unzip). The directory structure is as follows: GEOG70581 Data Practical_1 Practical_2 Images Practical_1 Practical_2 Output Practical_1 Practical_2 All the required data files for Practicals 1 and 2 are stored in the associated sub-directory within Data. As we move through the practicals, we’ll be creating a number of additional GIS files and we encourage you to save these within the corresponding sub-directory within Output. Finally, maps and figures will be required for the assessment. These can be exported directly from R and stored in Images. "],["Intro_to_R.html", "Chapter 3 A (brief) introduction to R 3.1 Overview 3.2 Loading R 3.3 Objects 3.4 Data structures 3.5 Scripts 3.6 Loading packages 3.7 Loading data 3.8 Plotting 3.9 Formative task", " Chapter 3 A (brief) introduction to R 3.1 Overview The following chapter provides a basic introduction to coding in R. If you already have experience with coding, perhaps from Understanding GIS, then the following content should be familiar to you. For those of you with little or no experience with code, the content builds from the simple (using R as a calculator) to the complex (loading packages and data, plotting) and will equip you with the skills needed to complete Practicals 1 (Eskdale) and 2 (Mersey) You should aim to work through this Chapter by the end of Week 8, making sure you understand the concepts and code before moving on to the next sub-section. To test your understanding, there is a short formative task (non-assessed) at the end of this chapter. This covers many of the techniques covered here, as well as unfamiliar ones, so you may have to do some independent research to complete it. I’ll post the solution after the class so you can check your working. 3.1.1 What is R? R was originally developed in the 1990s for statistical data analysis. While it is still optimized for statistical analysis and data visualization, it has been utilised for a much wider range of applications. As a high-level programming language, it is less complicated and more user friendly than lower-level languages (e.g. C, C++, Java, Rust), but this comes at the expense of being slightly slower to execute. Importantly, R is free and open-source and its core functionality can be extended using packages. These are available through the Comprehensive R Archive Network (CRAN) and allow the user to perform a range of tasks (e.g. reading and writing files, rendering images), without the requirement to delve into the underlying source code. While there are a myriad of free online resources available to help you learn R, including R for Data Science and YaRrr! The Pirate’s Guide to R, as well as paid options from Data Camp, this chapter and the subsequent practicals will equip you with all the skills required to complete the course. 3.1.2 Why code? For those of you familiar with desktop GIS (e.g. ArcGIS, QGIS) or other statistical programs (e.g. Excel, SPSS), the transition to using code can be a frightening experience! However, there are a several reasons why it is worth persevering with. First, and with career progression in mind, the ability to code is becoming increasingly important, particularly for those of you who want to progress down GIS or environmental career paths. Getting to grips with R and understanding the fundamentals of coding will equip you to learn new skills and new coding languages which may be required by a future employer (e.g. Python, MATLAB, Julia, Ruby, …). Second, performing your analysis using code ensures that your outputs are reproducible. If you provide someone else with the original data files and the corresponding code i.e. a list of operations carried out by the computer (e.g. read data, modify, plot, perform statistical test, save), they should be able to reproduce your outputs exactly. Third, code is repeatable. In standard GIS software (for example), most functions allow you to perform a single task on a single data file e.g. clipping a digital elevation model to the geometry of a shapefile. By comparison, if that same functionality can be implemented in code, it can be applied as easily to 10 data files, or 1000, or 10,000. Finally, code is easily modifiable. Code can be tweaked, adapted, or completely rewritten to produce the desired outputs. In R, a wide range of functions are already available in packages, but you can write your own functions or combine existing functions to suit your analysis. This functionality is often not the case in other software, which are generally more complicated to modify and typically provide Black Box solutions, where we can inspect the inputs and outputs, but typically have limited understanding of the steps in between. Black Box programming 3.1.3 A quick note on the practicals As we progress through this course, make sure to read the content carefully. However, you do not have to run every bit of code in this document, although have a go where you feel it would help your understanding. If I explicitly want you to do something, I will write an instruction that looks like this: This is an instruction that tells you something to either think about, or do. 3.1.4 Dealing with errors Before we load R and take our first steps towards coding excellence, it is worth noting that errors are an inevitable aspect of coding. Irrespective of how good you are, you will spend more time fixing your code (i.e. debugging) than writing it! Luckily, when R encounters an error, it will try and help you out. For example, shown below is some simple R code. Here, we are tying to create a new object called a, which is the product (+) of object b and the numeric value 1: a &lt;- b + 1 ## Error: object &#39;b&#39; not found In this example, the code fails and prints the error message object 'b' not found because the object b does not exist! We could solve this by defining b beforehand as follows: b &lt;- 3 a &lt;- b + 1 a ## [1] 4 These error messages are important and contain helpful information - do not ignore them! If you’ve struggled to resolve the problem, you should then Google it (normally by just typing “R” followed by the error message into Google), e.g. R Error in eval(expr, envir, enclos): object 'b' not found. Another useful source of information is StackOverflow which is a public collection of coding questions and answers. If you’ve come across a challenging error, there’s a high probability that someone else has also encountered the same error, and there may be a solution already available. Understanding, interpreting and fixing error messages is a key programming skill, so read the error messages carefully and use the above resources (Google, StackOverflow). If you need any additional help, staff and TAs will be happy to help during the practicals. 3.2 Loading R Now that we understand what R is, why it’s useful and what to do when something goes wrong, let’s begin. Open RStudio. The RStudio user-interface should resemble the following, although the exact arrangement of the windows may differ from that below: This contains the following primary windows, which are described below: In the console window, commands can be typed and results returned. For example, typing the following command into the console: 1 + 2 produces: ## [1] 3 Copy and paste the above code into the console and press Enter to run. Does it produce the correct result? In the Workspace window, you can see all the objects that you have created in the current R session (Environment tab; currently empty) as well as a record of the commands you’ve used (History tab; 1 + 2). At it’s most basic, R can operate as a powerful calculator. We can add +, subtract -, multiply * and divide /, take the exponent ^, calculate the square root sqrt() or the logarithm of a number log10(), or melt our computers by using the factorial() function. Operators can also be combined to produce more complicated mathematical functions. 3.3 Objects One of the most important things to know about in R is the variable or object. In this set of practicals, we use the term “object”, although variable is also used commonly across different programming languages. Objects are containers that you can store values in, and then refer to again later. R creates objects for you automatically, so all that is required is to name it and then use the assignment operator &lt;- to assign a value to it. As a general rule (and for historical and other complicated reasons), use &lt;- for assignment, as shown below, and don’t use =: b &lt;- 3 Run the above code. This assigns the value of “3” to an object named “b”: You should now see that under the Workspace window, the object b has been added to the Environment tab: The object b can now be called again by the user. Run the following in the console. Does it work as expected? b + 5 There are number of benefits to using objects. For example, take the following: a &lt;- 5 b &lt;- 10 result &lt;- (sqrt(a) + log(b))^a + 2*b result # Typing the name of an object (&#39;result&#39;) prints it in the Console Window ## [1] 1945.905 This is a relatively complicated mathematical formula! It uses a square root sqrt() and logarithm log() function, as well as multiplication * and powers ^. We could replicate this result by substituting each value of a for 5 and each value of b for 10, but this would be time consuming and prone to error, and would be very frustrating if we wanted to re-calculate the result for a = 6 and b = 11! Using objects allows us to use and modify values multiple times within the code. Modify the code above to test for a = 6 and b = 11. Compare your answer with a friend in class. Did you get the same result? Using objects is also important for repeatability. Let’s say we want to calculate the sum of the squares from 1 to 5 i.e. \\(1^2 + 2^2 + 3^2 + 4^2 +5^2\\). We could write this numerically in R as follows: 1^2 + 2^2 + 3^2 + 4^2 + 5^2 This produces the correct result: ## [1] 55 However, what if wanted to perform this calculation for all the numbers from 1 to 50 i.e. \\(1^2 + 2^2 + 3^2 + ... + 50^2\\)? What if we wanted to cube each value (i.e. \\(n^3\\)), rather than squaring? Either change would require a great deal of manual editing, with lots of potential for error. By comparison, using objects allows us to loop (or iterate) through a series of calculations. The code below creates a object called result and loops through a sequence of numbers using seq() to perform the above calculation, where the object i is updated each iteration. # Initialises an object called result, with a value of 0 result &lt;- 0 # Loops through a sequence from 1 to 5 in increments of 1 (i.e. 1, 2, 3, 4, 5) for(i in seq(from = 1, to = 5, by = 1)){ # Squares each number (i) and adds to result result &lt;- result + i^2 } # Prints object &#39;result&#39; to the Console Window result ## [1] 55 Run the above code in the console. Try to modify the code to perform the calculation for 1 - 50; this should equal \\(42925\\) At this stage, don’t worry about understanding the syntax of the code above. The important thing to remember is that using objects allows to us to perform more complex and repeatable analyses. 3.3.1 Object names When defining a object name, you can’t use spaces or characters that are not a-z, A-Z or 0-9. As a general rule, objects in R should be written in “snake case” (as opposed to “upper case” or “lower case”) where all words are in lower case and are separated by underscores (_). All of these are valid object names in snake case: snake_case a_really_long_snake_case_name snake s Other popular naming conventions include camelCase, UPPER_CASE or hyphen-case (among many others). Pick one and use it consistently. 3.3.2 Object types Each object will have a particular type, which specifies what kind of a value it can hold. Whilst this is handled automatically for you by R, it is important that you understand what the different types are. Here are some common examples: # Character i.e. a string of letters, numbers and/or characters a &lt;- &quot;Bill&quot; # Numeric i.e. a number with decimals b &lt;- 3.567 # Integer c &lt;- 3 # Logical i.e. TRUE or FALSE d &lt;- TRUE Run the above code. If successful, your workspace should now include the objects a, b, c, d. Some data types are compatible e.g.  # Numeric + integer b + c ## [1] 6.567 But others are not! # Numeric + character b + a ## Error in b + a: non-numeric argument to binary operator Can you understand the error message above? Why does b + a produces an error? 3.4 Data structures In the previous examples, each object has consisted of a single element. This could be a name (bill), a decimal number (3.567), an integer (3), and so on. However, R can store data in a range of different structures, some of which you will encounter as we progress through the course. 3.4.1 Vectors One of the simplest structures is the vector, which contains multiple elements of the same type. These are typically created using the command c(): # Creating a vector (v) of the numbers 1 - 5 v &lt;- c(1, 2, 3, 4, 5) v ## [1] 1 2 3 4 5 As a vector must have elements of the same type, R will try and coerce elements to the same type: # Creating a mixed vector (m) containing numeric (1,2), character (&quot;Bill&quot;, &quot;Ben&quot;) and logical elements (TRUE) m &lt;- c(1, 2, &quot;Bill&quot;, &quot;Ben&quot;, TRUE) m ## [1] &quot;1&quot; &quot;2&quot; &quot;Bill&quot; &quot;Ben&quot; &quot;TRUE&quot; In the above example, the numeric and logical elements have been coerced to characters, as signified by these elements being enclosed by quotation marks \" \". We can also create vectors programmatically, rather than manually defining each element. For example, we can generate sequences using seq() or repetitive sequences using rep(): # Creating a sequence from 1 to 5, in increments of 1 s &lt;- seq(from = 1, to = 5, by = 1) s ## [1] 1 2 3 4 5 # Creating a vector of length 5, consisting of the value 10 r &lt;- rep(10, times = 5) r ## [1] 10 10 10 10 10 3.4.2 Data frames Other common R data structures include matrices, lists and data frames. The data frame is the most commonly used structure for tabular data and will be familiar to you, as this similiar in structure to a Microsoft Excel spreadsheet (containing rows and columns of data). Here is an example data frame, which is created by combining four existing vectors of the same length (name, height, nationality, bereft_of_life): # Create numeric, character and logical vectors name &lt;- c(&quot;John&quot;, &quot;Eric&quot;, &quot;Michael&quot;, &quot;Graham&quot;, &quot;Terry&quot;, &quot;Terry&quot;) height &lt;- c(1.96, 1.85, 1.78, 1.88, 1.75, 1.73) nationality &lt;- c(&quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;American&quot;, &quot;British&quot;) bereft_of_life &lt;- c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE) # Combines into a data frame df &lt;- data.frame(name, height, nationality, bereft_of_life) df ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 5 Terry 1.75 American FALSE ## 6 Terry 1.73 British TRUE Run the above code in your R console We’ll be working with data frames throughout the practicals, so it will be useful to know that we can access data frame elements in a variety of different ways. We can access individual columns of a data frame using the $ operator, for example: # Extract the &quot;name&quot; column df$name ## [1] &quot;John&quot; &quot;Eric&quot; &quot;Michael&quot; &quot;Graham&quot; &quot;Terry&quot; &quot;Terry&quot; We can also extract using indexing, which requires us to provide the row and columns indexes in the following format: df[row.index, column.index] # Extract the element at row 3, column 4 df[3,4] ## [1] FALSE # Extract rows 1 to 4, all columns df[1:4,] ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE # Extracts all rows, the second and third columns df[,2:3] ## height nationality ## 1 1.96 British ## 2 1.85 British ## 3 1.78 British ## 4 1.88 British ## 5 1.75 American ## 6 1.73 British We can also extract rows and columns based on the values within the data frame, for example using the subset() function. This can be used to select and exclude observations as follows: # Extract data frame rows where height is more than or equal to (&gt;=) 1.8 m. subset(df, height &gt;= 1.8) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 4 Graham 1.88 British TRUE # Extract data frame rows where nationality is not equal to (!=) American subset(df, nationality != &quot;American&quot;) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 6 Terry 1.73 British TRUE # Extract data frame rows where bereft_of_life is equal to (==) FALSE subset(df, bereft_of_life == FALSE) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 5 Terry 1.75 American FALSE Using the above code as an example, extract all the rows where name is equal to Terry. Assign the result to a new object. You can calculate the number of rows using the function nrow(). Did you get the following answer? ## [1] 2 The sections above have given you a crash course in some of the fundamentals of R. Learning to code (or learning a new code syntax for those of your familiar with Python, MATLAB, Java, …) is similar to learning a new language. Don’t worry if it seems complicated! You can always refer back to these instructions if you get stuck, or ask for guidance. To finish this Chapter, we’ll provide a brief overview of some other key skills, including writing R scripts, loading data and packages, and plotting. This should prepare you for the formative task. This is not an exhaustive list of R functions, and we’ll cover other functionality in Practicals 1 and 2 (e.g. loading GIS raster and vector data, creating maps and figures, performing statistical tests). However, the R functionality outlined here will cover all the steps required for the assessment and should equip you with the core skills required to progress further with R in the future e.g. as the basis for your dissertation. 3.5 Scripts So far we’ve been typing commands into the console. This works as expected; we can print results, store objects in the workspace and create plots in the plots window. However, it is often more convenient to store our code in a script, which is simply a file containing a set of commands and comments. For example, a single R script could contain commands to load data, perform statistical analysis, and output the results to a new file. This can be run in a single keystroke, which is much simpler than typing each command in one after another. To create a new R script: Navigate to File, New File and R Script. This should open a new window, which is an untitled R script: To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. learning_R) One of the easiest ways to run an entire script is to use Ctrl + Shift + Enter. However, you can also run aspects of a script by clicking on the individual line you want to run and using Ctrl + Enter. The same command applies if you highlight (select) multiple lines of code. 3.5.1 Comments When writing code, it is good practice to include comments which describe what the code does. As your code becomes more complex and as time passes, it is very easy to forget what your code does! Commenting your code is especially important if your code is being used by other people or even by yourself weeks/months/years in the future; save yourself time and effort by describing your code fully as you write it. Comments can be written in R using the # symbol. Any line of code which begins with # is ignored by R when the script is run and are used solely to improve the readability of the code. To add a comment, insert the # symbol and write some text. Below is some uncommented code. Copy and paste into your script and add some comments to describe what is taking place. a &lt;- seq(10, 100, 10) b &lt;- a/10 c &lt;- b*a plot(c, a) The above code is our first encounter with plot() which (as you may have guessed), is used to plot objects in base R 1. The new plot you’ve created should now appear in the plots window and should resemble the following figure. There are lots of more exciting ways to plot data in R, some of which are described below. As a general rule, well-presented code would typically comprise: 1/3 code; 1/3 empty space; 1/3 comments. This is known as the Rule of Thirds. 3.6 Loading packages Now that we have our script set up, we need to understand is how to install and load packages in the R environment. To begin, remove all the previous code from your script learning_R.R and save. For the remainder of this chapter, we are going to use just one package as an example, but more will be required for Practicals 1 and 2. The package we are going to use is ggplot2, which is a widely used visualization package for R. If there’s a figure, map or graphic you want to make, it’s likely that ggplot2 can do it for you. It’s now even used by the BBC! BBC graphics created using ggplot2. See here for details. We can install the package using the helpfully named function install.packages() as follows: # Function to install packages. Name of package enclosed by quotation marks. install.packages(&quot;ggplot2&quot;) This function installs the package to your computer, but before we can use it, it needs to be activated within your current R session. This can be achieved by adding it to the library() as follows: # Function to add package to the library (no quotation marks) library(ggplot2) Copy and paste the two code snippets above into your script and run to install and load ggplot2. This is the standard approach to installing and loading packages in R. However, one of the weaknesses of this approach is that R will try and install and load each package every time the script is run. This is unnecessary; the package is already loaded! Try to re-install ggplot2 using the code above. The following pop-up should appear: Press No. The following message will appear in the console: Warning in install.packages : package ‘ggplot2’ is in use and will not be installed We can address this issue by using a user-defined function (called check.packages) to check whether a package is already installed. If it is installed, R will skip the installation. If it is not installed, R will install as normal. This function is similar to the functions we’ve been using before, such as sqrt() or seq(), which are available by default in base R; the difference is it’s been written from scratch! # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) Copy and paste the above code at the start of your R run script, removing the previous install.packages() and library() commands. Re-run the code. In future, you can add new packages by simply adding the package name to the packages vector. This works in exactly the same way as described previously, with the vector created using the c() command. For example, we could install packages as follows: packages &lt;- c(\"ggplot2\", \"dyplr\", \"raster\", \"sf\"). A full list of the available packages is here (&gt;18,000 and counting…). Finally, packages should typically be installed and loaded at the start of any R script. If your code requires a package to run and it has not yet been installed, the code will fail. Installing and loading all the required packages at the start of the script avoids this problem. 3.7 Loading data In the final two sections of this chapter, we will load some data from a external file and plot using ggplot2. In the previous examples, we’ve been creating objects, vectors and data frames from scratch, but most data you will work with will be saved in another file format. This could be a comma-separated file (.csv), tab-delimited (.txt) or in an Excel format (.xls), or could be a spatial data type, such as a raster (.tif, .asc, .img) or vector (.shp). Being able to load and manipulate different data types is a key programming skill. We’ll use one of the most well-used formats as an example; the comma-separated file (.csv). This consists of rows and columns of data, where columns are delineated by commas. For example, here is an example of some comma-separated data: name,age,height,year_of_birth,instrument stewart,70,1.87,1952,drums andy,79,1.70,1942,guitar gordon,70,1.81,1951,vocals When read by R, it is interpreted as follows, splitting columns based on the occurrence of commas, and using the first row as column names: name age height year_of_birth instrument stewart 70 1.87 1952 drums andy 79 1.70 1942 guitar gordon 70 1.81 1951 vocals To load a .csv file into R, we can use the command read.csv(). The most basic way to load a file is to write the absolute file path. This is the location on your computer where the file is saved. For example, a file saved on your C drive, within your Documents folder, and within a folder called Manchester would be formatted as: C:\\Documents\\Manchester To determine the absolute file path, navigate in your file explorer (e.g. Windows Explorer, finder on macOS) to the data sub-directory within GEOG70581: For the final two sections of this Chapter (and the formative task), we’ll using a data file called flow_data.csv which contains the average daily river flow (m3 per second) from the National River Flow Archive and the UK Centre for Ecology &amp; Hydrology. The data are from monitoring station 90003 on the River Nevis, which drains Ben Nevis, the highest mountain in the UK, and cover the period from the 1st October 1982 to the 30th September 2019. You can explore the Glen Nevis catchment below: The file path to your file might read as follows (if your name was Bill): C:\\Users\\Bill\\GEOG-70581\\data\\flow_data.csv To load this file into R: Copy and paste this file path into your script, enclosing the file path within quotation marks. Importantly, R uses forward slashes within file paths /, whereas the file path in your file explorer probably uses backslashes \\, so these will need to be swapped. # Loading data (ensure file path is updated) df &lt;- read.csv(&quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot;) If this has worked successfully, a new object df will have been loaded into the workspace: As you may have noticed, this is not the most elegant way to load data, as it requires us to copy, paste and edit the full file path each time. This would quickly become tiring if we wanted to load multiple data sets. In Practical 1, we’ll guide you through a solution using R projects and the here package but as ever, it is important to understand the basics before moving on to more sophisticated solutions. We can use a variety of commands to inspect this new data set, including: head(df) colnames(df) str(df) Run the above code. What do each of the commands do? head(), colnames(), str() What data types do we have in each column? See here for a reminder. What data structure are we working with? As outlined previously, we can access individual columns of a data frame using the $ operator and could start to do some simple analysis using base R functions, such as: # What is the average daily flow? mean(df$flow) # What is the maximum daily flow? max(df$flow) # What is the minimum daily flow? min(df$flow) Question: What is the range of daily flow measurements? As outlined previously, we can also subset our data based on the values within the data frame, based on columns names ($) or using indexing. To simplify our subsequent analysis, we’re just going to work with data collected in 2019, although we’ll ask you to extract a different year for the formative task. The data frame rows which correspond to 2019 are from the 1st January [13242] to the 30th September [13514]. Extract these rows and all columns using indexing; see here for a reminder if you get stuck. You may also have noticed from the str() command that our column of dates (day/month/year) were stored as characters e.g. $ date: chr \"01/10/1982\". Working with dates and times is a complicated task (see this Tom Scott video for a light introduction to the problem), so to simplify plotting, we’ll convert these characters e.g. \"01/10/1982\" into a date format. This can be achieved using the following code, which overwrites the date column in the data frame: df$date &lt;- and uses the function as.Date() to define a date format. In this case, our numbers are listed in day, month and then year, which is formatted in R as \"%d/%m/%Y\": # Converts dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) Once you’ve extracted the flow data corresponding to 2019, run the above code. 3.8 Plotting In the final task, we’re going to plot these data using ggplot2. One of the challenges of ggplot2 is its steep learning curve, but we’ll walk through this example to show how we can progress from the raw data to high quality figures. These approaches will be useful for your assessment. To finish this chapter, copy and paste the following code elements into your R script in turn and run them. Make sure to read and understand what each step is doing. To begin, we’re going to create an empty ggplot canvas i.e. the blank canvas upon which we can add ggplot elements, such as axes, legends, data, scale bars etc. # Initialising an empty canvas ggplot() Next, we specify which data set we are going to plot. Note: ggplot2 works almost exclusively with data frames: # Which data frame do we want to use? ggplot(data = df) With the data frame defined, we now need to specify which columns of date we wish to plot. The code below uses the function mapping=aes()to do so, where the date variable (within the df object) is assigned to the x-axis x=date and the flow rate variable is assigned to the y-axis y=flow using their column names: # Which data frame columns do we want to plot? ggplot(data = df, mapping=aes(x=date, y=flow)) However, our data are still not visible because we need to select a geom type, which is a complicated way of saying the type of geometric object we want to plot. We could plot our data as points, lines or polygons; we could choose a histogram, bar plot, or box and whiskers plot; there are many to choose from! In this example, we are going to plot our data as points, which is done using the geom_point() command. As you can see from the code below, the structure differs slightly from normal code, in that ggplot elements are separated by the + operator. This allows the user to add each ggplot element on a separate line of code, thereby improving readability. # What type of geom do we want? ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() In the same way, we could also plot the same data as a line, using geom_line(). In this example, both geoms are using the same data frame as inputs x=date, y=flow. This is because the data frame and columns have been specified in the initial call to ggplot here: data = df, mapping=aes(x=date, y=flow). All subsequent geom objects (e.g. points, lines, bar plots…) will use these inputs, unless an alternative is specified. # Adding a line geom ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() + geom_line() The above code covers the basics of ggplot, but we can start to improve the design of our plots. For example, we can choose one of the available themes. We could re-order our geoms to show the data points above the line, while adding some colour using HEX codes. We could change the aspect ratio to produce a square plot and maybe add some axis labels: # Specifying data frame and columns ggplot(data = df, mapping=aes(x=date, y=flow)) + # Specifying colours geom_line(colour = &quot;#56ACFF&quot;) + geom_point(colour = &quot;#767676&quot;) + # Adding a ggplot theme theme_classic() + # Setting the aspect ratio theme(aspect.ratio = 1) + # Updating axis labels (including superscript elements) labs(x = &quot;Date (January - October 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) Using the above plot, we could start to identify periods of above or below average flow. Adding a trend line could also be useful to identify patterns in the data. Add a trendline to your plot using the geom_smooth() argument. You can experiment with different smoothing methods e.g. method = “lm”, “glm”, “gam” or “loess”. As a final step, we will probably want to export our figure so that it can be used elsewhere e.g. in the assessment. To do so, we simply assign our ggplot object to an object using the assignment operator: &lt;- as normal: # Assign our ggplot object to the object &quot;g&quot; g &lt;- ggplot(data = df, mapping=aes(x=date, y=flow)) + ... Finally, we can save our ggplot object using the ggsave() function, as shown below. This takes in the name of the plot (g), as well as the intended name, extension and file path e.g. \"C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png\". In this example, we are saving our output within the images sub-directory of GEOG-70581. We have also defined the name of the file flow_data_2019 and specified its file type: .png (portable network graphics file). We can also modify other settings, such as the dpi which determines the plot resolution. A list of ggsave options can be found here. # Saves to a png ggsave(plot = g, &quot;C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png&quot;, dpi = 150) ## Saving 7 x 5 in image Run the above code to assign your plot to the object g and export it to your images folder, making sure to update the file path! We have now exported our plot and should be able to see it in the images sub-directory of GEOG-70581. Importantly, and for future plots, you don’t need to run the commands in the sequence shown above e.g. first creating a canvas ggplot(), then defining a data frame ggplot(data = df), and so on. This sequence was used to show you how we can add elements to the ggplot object, building from the blank canvas to exporting the final figure. 3.9 Formative task Congratulations! You have now completed the guided portion of Chapter 3: the (not-so) brief introduction to R. While this may have been challenging, we have covered most of the core skills which you will need. You should now have an appreciation of the different data types and structures that are available in R, experience running code in the console and within scripts, and the ability to install packages, load data and create plots. We’ll use many of these techniques in Practicals 1 and 2. To finish the practical, please complete the formative task outlined below. Create an R Script which does the following: Installs the required packages. Loads the flow data for Glen Nevis. Extracts all rows corresponding to the years 2000 to 2019, making sure to convert to date format. Finds the average daily river flow (m3 per second) for each month and plots the result. Questions: on average, which month had the highest average daily flow and which had the lowest? Hints and Tips: For general guidance on using ggplot2, see here. To extract the month from each record, you may want to use the months() function from base R, while aggregate() (described here) can be used to compute summary statistics (i.e. the mean) based on groups. 3.9.1 Formative solution A code solution to the formative task will be uploaded here after the class. Don’t worry if your solution is slightly different from mine! Note If you’re working on a personal computer with Chinese language settings, you may need to run Sys.setlocale(\"LC_TIME\", \"English\") at the start of your code to fix language-related errors the in the as.Date() function. # Solution to the Chapter 3 formative task # Author: matt-tomkins # Date of upload: 21-11-2025 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) ## ggplot2 ## TRUE # Load data from csv (personal file path - update to your own) df &lt;- read.csv(&quot;C:/Users/44797/Desktop/Repo/GEOG-70581/data/flow_data.csv&quot;) # Convert dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) # Extracts post-2000 river flow df_post2000 &lt;- subset(df, date &gt;= as.Date(&quot;2000-01-01&quot;)) # Create a new column, corresponding to the month of measurement, using the months() function df_post2000$month &lt;- months(df_post2000$date) # Summarise by month, using the aggregate() function and &#39;mean&#39; summary &lt;- aggregate(flow ~ month, df_post2000, mean) # Determines plotting order by converting the month column to a &#39;factor&#39; # The factor levels are determined by the R month.name function summary$month &lt;- factor(summary$month, levels = month.name) # Plotting using ggplot2 g &lt;- ggplot(data = summary, mapping=aes(x=month, y=flow)) + # Stat = &quot;identity&quot; is used when the values of x and y are known geom_bar(fill = &quot;#3EBBFB&quot;, stat=&quot;identity&quot;) + # Sets the theme theme_classic() + # Add x and y-axis labels labs(x = &quot;Month (2000 - 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) + # Tight fitting y-axis scale_y_continuous(expand = c(0,0)) + # Adjusts angle of x-axis labels theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # Displays figure g # Prints highest and lowest average daily flows print(paste0(&quot;The month with the highest average daily flow is &quot;, summary$month[summary$flow == max(summary$flow)])) ## [1] &quot;The month with the highest average daily flow is January&quot; print(paste0(&quot;The month with the lowest average daily flow is &quot;, summary$month[summary$flow == min(summary$flow)])) ## [1] &quot;The month with the lowest average daily flow is July&quot; Base R is used to refer to the functions that are part of the core packages deployed with R, such as plot(), sqrt() or seq(). As we install and utilise user-created packages, we are moving away from base R.↩︎ "],["FirstPractical.html", "Chapter 4 Eskdale I - Introduction 4.1 Overview", " Chapter 4 Eskdale I - Introduction It should be evident from the lectures that digital elevation models (DEMs) have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions that (1) we can trace the path that water will take from any point in an elevation grid to a basin outlet, and (2) the discharge of water from a particular location is directly related to its catchment area. This is a fairly reasonable assumption in most drainage basins. 4.1 Overview In this lab exercise, we will use a DEM of the Upper Eskdale catchment to model flow-related phenomena, and will have gained experience in: DEM pre-processing; Calculating flow parameters (e.g. pointers and contributing areas); Comparing flow algorithms. Make sure to save your R script regularly! Upper Eskdale catchment 4.1.1 Intended learning outcomes In Chapter 5, we’ll download the data and install the required programs and packages, and will combine these in Chapter 6 to: gain experience with WhiteboxTools, a specialist GIS specifically designed for digital terrain analysis (DTA); become familiar with some of the most commonly used flow algorithms used to simulate overland flow from digital elevation models (DEMs); consider the utility of DTA for routing applications; 4.1.2 Assessment This practical is formative and as such will not count towards your final mark for GEOG 70581. However, you are encouraged to complete the practical to the best of your ability as the skills developed here will be used throughout the assessed Practical 2. As with the Introduction to R practical, I will upload a code solution after the class so you can check your working. "],["Eskdale_set_up.html", "Chapter 5 Eskdale II - Set-up 5.1 Install programs 5.2 Download data 5.3 Open RStudio 5.4 Initialise an R project 5.5 Creating an R script 5.6 Install WhiteboxTools 5.7 Loading packages", " Chapter 5 Eskdale II - Set-up 5.1 Install programs If you haven’t installed R and RStudio, please refer to the instructions here before continuing. 5.2 Download data If you’ve not already done so, the data for this practical can be downloaded here. The directory structure is outlined in Chapter 2. 5.3 Open RStudio To begin, open RStudio. 5.4 Initialise an R project In the previous chapter, we typed commands into the console and used R scripts to store commands and comments. For both Practicals 1 (Eskdale) and 2 (Mersey), we are going to expand on these by also utilising R projects. Projects make life simpler by allowing us to access file locations (e.g. for loading and writing data) using relative file paths, rather than using the clunky and prone-to-breaking absolute file paths. Relative file paths work by setting the working directory, which is simply the the default location where R will look for files you want to load and where it will put any files you save. We can do this manually by using the setwd() function, but a simpler approach is to use an R project, which sets the working directory for your automatically. Rather than the complicated absolute file paths: &quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot; which indicates that the file is located in data, which is within GEOG-70581, which is within Documents, which is within Bill, which is within Users, which is on the C drive, we can set the working directory to GEOG-70581 (for example) and then access the file using the here package: here(&quot;data&quot;, &quot;flow_data.csv&quot;) This approach not only makes it easier to access files, but also improves reproducibility. Using projects ensures that our work is self-contained (the project directory contains all the data, scripts and outputs) and portable. Transferring the project directory to another user, to a different sub-folder on your computer, or a different drive, would not cause any code to fail. By comparison, scripts featuring absolute file paths or a manually defined working directory would likely fail in any of the above scenarios. To initialise a new R project: Go to File, New Project, and select Existing Directory. Using the Browse icon, set the project working directory to the folder GEOG70581, which you downloaded and unzipped here. Select Create new project If this has been successful, your console should have been updated to include the path to your project working directory as follows: 5.5 Creating an R script Now that we have created our project for GEOG70581, we are going to use scripts to store the code for each practical. We’ll create a script for Practical 1 and a separate script for Practical 2. Important: a script can be understood as a set of instructions. As a result, the order of the code is critical. A useful metaphor may be to think of a script as a recipe. For example, a simple cake recipe might require butter, sugar, eggs, flour, milk, vanilla extract and lemon. These could be combined as follows: Pre-heat oven. Whisk butter and sugar. Crack the eggs in one at a time and whisk. Add the lemon zest, vanilla, flour and milk and whisk. Bake in the oven for 30 mins. In code form, it might resemble the following, with a vector including the ingredients and different functions used to combine the ingredients (whisk), turn on the oven (pre_heat) and bake (bake_mixture): # Vector of ingredients ingredients &lt;- c(butter, sugar, eggs, flour, milk, vanilla, lemon) # Pre-heats oven for 20 minutes pre_heat(20) # Create a mixture of butter and sugar using the whisk function, mixture &lt;- whisk(ingredients$butter, ingredients$sugar) # Iterates (loops) over eggs variable for (i in length(ingredients$eggs)){ # Extracts each egg, one at a time current_egg &lt;- ingredients$eggs[i] # Adds current egg to mixture mixture &lt;- whisk(mixture, current_egg) } # Add remaining ingredients mixture &lt;- whisk(mixture, ingredients$flour, ingredients$milk, ingredients$vanilla, ingredients$lemon) # If oven temperature is 180 degrees of more if(oven_temperature &gt;= 180){ # Bake for 30 minutes cake &lt;- bake(mixture, time = 30) } From the text and code example above, it should be clear that our recipe (and code) would not work correctly if we tried to bake in the oven before pre-heating it, or added the ingredients in the wrong order. In exactly the same way, it is important that your code is formatted in a logical order (e.g. Function 1, Function 2, Function 3, …). Ensure that your future code reflects the recipe above, where individual functions utilise variables created previously, and don’t rely on variables created further down in the code. To create an R script for Practical 1: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-1-Eskdale) This should now resemble the following: 5.6 Install WhiteboxTools For Practical 1, we are going to use the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); These packages can be installed as normal using the install.packages() and library() functions. In previous versions of this course, whitebox was not available (because of complicated reasons) on the Comprehensive R Archive Network (CRAN). Instead, it had to be installed using R-Forge. While CRAN is the official package repository for R and generally features “release” versions of packages, R-Forge provides candidate, beta or “work-in-progress” packages or packages which don’t meet the criteria for inclusion on CRAN. However, as of last year, whitebox is now back on CRAN so we can install as normal using the install.packages() function. It is worth noting, however, that you can install packages from R-Forge by simply specifying the repository (web location) where the package is stored, in this case: http://R-Forge.R-project.org. # Installing a package via R-Forge install.packages(&quot;insert package name here&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) Note You don’t need to run the above code - this is just an example of how to install via an alternative repository. One additional step, which is different from “normal” R package installation, is to use the function whitebox::install_whitebox(), which initialises WhiteboxTools: # Initialise WBT whitebox::install_whitebox() This should print the following to your console: Performing one-time download of WhiteboxTools binary from https://jblindsay.github.io/ghrg/WhiteboxTools/WhiteboxTools_win_amd64.zip (This could take a few minutes, please be patient...) WhiteboxTools binary is located at: C:/Users/44797/OneDrive/Documents/R/win-library/4.0/whitebox/WBT/whitebox_tools.exe You can now start using whitebox &gt; library(whitebox) &gt; wbt_version() This code checks if a suitable WhiteboxTools executable is present (.exe) and installs if missing. The WhiteboxTools executable contains all the code used for geospatial analysis, which we then access using R via the whitebox R package. This is a slightly different configuration from normal R packages but enables WhiteboxTools to be used cross-platform i.e. WhiteboxTools functionality stored in the executable can be assessed through R, Python, ArcGIS or QGIS etc, depending on the user needs. Run the below code to install the whitebox package, initialise WhiteboxTools (which should perform a one-time download of the WhiteboxTools executable) and check it is installed correctly. # Install the whitebox R package install.packages(&quot;whitebox&quot;) # Initialise WhiteboxTools whitebox::install_whitebox() # Add the whitebox R package to library library(whitebox) # Check the version is correct wbt_version() When complete, WhiteboxTools has now been successfully installed and initialised and is almost ready to be used. However, to avoid re-installing and re-initialisng WBT every time we run our script, we can either delete the code or comment-out # each line. Remember that R ignores any line of code that begins with #: # Code to install WhiteboxTools # whitebox::install_whitebox() Either delete or comment-out the previous command (install_whitebox()). Ctrl + Shift + C is a useful shortcut for adding or removing comments from multiple lines of highlighted code. 5.7 Loading packages To finish the set-up for Practical 1, we are going to install the remaining required R packages (described above) and load them into the R library. We’ve already practiced installing and loading the ggplot2 package in Chapter 2 so you are ready for the following: Referring back to the instructions here, install and load ggplot2, here, raster, sf, ggspatial and whitebox. Hint 1 - we’ve already installed whitebox, so this can be excluded if you’re using install.packages() and the library() commands. Hint 2 - the user-defined function check.packages might be useful here. If the package has been installed and loaded correctly, the console should print the following: ggplot2 here raster sf whitebox ggspatial TRUE TRUE TRUE TRUE TRUE TRUE where the logical value TRUE equates to a successful installation (and vice versa for FALSE). "],["Eskdale_flow_algorithms.html", "Chapter 6 Eskdale III - Flow routing 6.1 DEM pre-processing: flow enforcement 6.2 Flow Parameters: Pointers 6.3 Flow Algorithms 6.4 Practical solution", " Chapter 6 Eskdale III - Flow routing Overland and near-surface water flow can be modelled using DEMs if we assume that surface topography is the sole factor which influences the distribution of water. One very simple model routes all water from a particular grid cell in a DEM to a single neighbouring cell (i.e. water is not partitioned between multiple neighbours). This ‘D8’ (8 direction) method sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell. Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). Using this method, water flow is allowed in one of eight possible directions (↑ ↗ → ↘ ↓ ↙ ← ↖), assuming that water will travel along the steepest downslope path. In turn, the method is sometimes referred to as the steepest descent method. Based on the \\(3 * 3\\) cell neighbourhood shown in Figure 1, flow would be directed from the centre cell (8 m elevation) to the southwest cell (4 m elevation). Reminder: you don’t have to run every bit of code in this document. Indented text (such as this) is an instruction that tells you something to either think about, or do. 6.1 DEM pre-processing: flow enforcement One common issue encountered when performing hydrological analyses is the presence of sinks, which interrupt the drainage network. When sinks are encountered, flow direction is undefined when a grid cell, or group of grid cells, is lower than all neighbouring cells (see Figure 2). When sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. These features are referred to as pits if they are a single cell in size, and depressions if they consist of groups of cells. Figure 2: Schematic of a sink in a DEM. In this \\(5 * 5\\) matrix, water is routed into the top right cell (9 m) and then is routed to the lowest elevation cell at each step (9 m → 8 m → 7 m → 6 m → 2 m) using the D8 method (blue cells). However, the drainage network is interrupted by a sink at the 2 m cell (orange), as all neighbouring cells are of higher elevation. Sinks can often be artefacts of the data and should be removed during DEM pre-processing. Pre-processing involves altering the elevations of the DEM in a way that enforces continuous flow-paths. However, it is important to realise that sometimes these ‘digital depressions’ reflect actual features in the landscape, and should be preserved during flow modelling. This is a particular issue for hydrological analysis of karst environments, where water can be routed into dolines and fractures (see Figure 3). However, for our work, we will assume that all depressions in DEMs are artefacts and are justified in being removed. Figure 3: An example of a “true” DEM sink. A sinkhole extending into the limestone karst of Twisleton Scar, Yorkshire Dales, with Ingleborough behind (723 m). How should we model water flow in environments with sinkholes, fractures, dolines and extensive sub-surface water flow? Several methods have been developed for removing depressions from DEMs. These methods vary greatly in terms of their sophistication and impact on the DEM. The two most common depression removal methods (Figure 4) are: depression filling, which raises cells within a depression to the elevation of the outlet cell; depression breaching, which digs a trench from a depression’s bottom to some point downslope. Figure 4: Schematic of depression filling and breaching in a DEM. Using the same values from Figure 2, original values are modified to allow water to escape the sink (orange). Depression filling has raised the value of the sink (2 m → 4 m), while depression breaching has lowered the value of a neighbouring cell (3 m → 1 m). In this simplified example, the outputs of these two distinct approaches are identical but care should be taken when working with real world data as they will often produce different results. Not all interruptions to flow routing are caused by depression cells. Often, DEMs contain extensive flat regions (areas of equal elevation). Flat areas interrupt flow routing in the same way as depressions. Cells within a flat region do not have downslope neighbours, and therefore, flow routing is impossible on flat sites without pre-processing. Correction of flow direction on flat sites typically involves finding an outlet cell, forcing flow from cells adjacent to the outlet to the outlet, and continuing backwards in an iterative manner (e.g., Jenson and Domingue (1988)). 6.1.1 Data description The DEM we are working with is centred on the Upper Eskdale catchment; an upland valley which drains the highest mountain in England (Scafell Pike; 978 m), as shown below: Figure 5: Upper Eskdale panorama, viewed from Harter Fell [Location: 54.386907, -3.205004, Elevation: 649 m]. The catchment ranges in elevation from 978 m (Scafell Pike) to ~160 m at the catchment outlet (white circle), and is ringed by numerous summits with elevations in excess of 800 m (white triangles). The catchment has an area of ~15.7 km2 and all water which falls with the catchment ultimately drains to the Irish Sea via the River Esk. You can explore the catchment below: The DEM we are working with was downloaded from EDINA Digimap, has a cell size of 10 m and uses the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Unfortunately, we don’t have time in this course to delve into the exciting world of map projections, although these are covered in great detail in Understanding GIS. However, it is important to know that different map projections have different uses and work more/less effectively in different regions. As we are working within the UK, it makes sense to use a map projection which is tailored to the UK (e.g. BNG) as this minimises different types of map distortion (length, shape, area). When loaded into R using the raster package and plotted using ggplot2 and ggspatial, our DEM can be visualised as follows, where the outlet point (white circle) and summits (black triangles) match those shown in Figure 5. To accentuate areas of relief, the DEM has been combined with a semi-transparent hillshade layer, which is shown below: 6.1.2 Breaching and filling To evaluate the effects of depression breaching and filling, we’re going to use the whitebox package and the wbt_fill_depressions and wbt_breach_depressions functions. One of the advantages of using whitebox is that functions can be applied to spatial data without having to load files into the R environment (and assign them to objects), which can be an issue when working with large files (e.g. high spatial resolution rasters) or when working on computers with limited available memory. We can call whitebox functions by simply naming the function, specifying the names of the input and output layers, and deciding on parameters (if required). For example, the code to fill depressions, might resemble the following: # Fill depressions wbt_fill_depressions(&quot;name of input file goes here&quot;, &quot;name of output file goes here&quot;) Given the structure of our file directory, this could be written as: # Fill depressions wbt_fill_depressions(&quot;C:/Users/Bill/GEOG-70581/data/practical_1/dem_10m.tif&quot;, &quot;C:/Users/Bill/GEOG-70581/output/practical_1/dem_10m_fill.tif&quot;) Here, we’ve used absolute file paths to set the input DEM as dem_10m.tif, stored in data/practical_1. The function wbt_fill_depressions will fill depressions in the input DEM and save the new DEM as dem_10m_fill.tif, stored in output/practical_1. However, because we’re using an R project we can simplify this further. As we’ll be using the DEM (dem_10m.tif) on multiple occasions, we can store its file path using the here package in the variable dem as follows: # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem Copy and paste the above code into your Practical-1-Eskdale script and run. This should print the absolute file path to dem_10m.tif. With this file path stored, we can run the fill depressions function as follows, using the here package to specify the name and intended location of the output file: # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) Run the above code, which should produce a new raster file in the output/practical_1 folder. Repeat this process to also breach depressions, making sure to update the output file name. The difference between the original DEM and the processed DEM(s) may be very subtle and you probably won’t be able to observe the differences directly. As such, it may be useful to identify the specific areas of the raster which have been modified. This can be achieved by using the subtract function to work out the difference between the original DEM and the processed DEM, as follows: # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file Run the above code. To visualise this new raster file, we’re going to load it into the R environment using the raster and here packages and plot with ggplot2 and ggspatial. The raster can be loaded as follows: # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) It can be plotted as follows: # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g Most of the above should be familiar to you, with a few exceptions: Under scale_fill_continuous(), we’ve set limits for the fill symbology. This now ranges between the maximum value, calculated using max(values(dem)), and the minimum value that is greater than 0, calculated using minimum_value &lt;- min(values(dem)[which(values(dem) &gt; 0)]). The latter is useful because most of the DEM was unmodified by wbt_fill_depressions so most raster cells contain a value of 0. The layer spatial() is a non-standard geom type and is available through the ggspatial package. This simplifies the process of plotting raster data in ggplot2, and this approach should be replicated for future plotting. Run the above code elements to reproduce the plot above. You can remove limits from the scale_fill_continuous() argument to plot the actual distribution of DEM values. Question: to what degree has the original DEM been modified by the wbt_fill_depressions function? Consider the spatial extent and magnitude of the observed differences. Note: For this catchment, the difference between DEM breaching and filling is relatively minor. We’ll use the latter (dem_10m_fill.tif) for all subsequent calculations. As outlined above, the areas which are most affected by the depression filling algorithm are partly determined by local relief i.e. relatives differences in elevation. To help visualise this, let’s create and plot a slope image for the area, where the values in the raster cells represent the gradient of the slope, rather than their raw elevation. This can be achieved using the wbt_slope function. Using the examples above, run wbt_slope, using the original DEM as the input, saving the output file with a logical name (e.g. dem_10m_slope.tif) and to the correct sub-directory (output/practical_1). Load the data into R and plot using ggplot2. Your figure should resemble the following: Compare the ‘slope’ and ‘difference’ images. Hint: you can toggle between plots using the left and right arrows in the plot window. Based on your visual inspection the images, which parts of the landscape appear to have been affected by the depression filling algorithm? Why do you think that this is the case? Before we move on to further analysis, it is worth thinking carefully about the symbology we are using for our raster layers. This is important, because not all colour ramps are created equal! Colour ramps differ in terms of their perceptual uniformity and their accessibility for colour-blind individuals. Save your script before continuing. 6.1.3 Choosing a colour ramp Perceptual uniformity refers to how well a colour ramp represents the underlying data. Some (e.g. rainbow; Hawkins (2015)) can introduce artificial boundaries (Alcalde et al. (2017)) which influence how we assess and then use that data (see here). In the schematic below, there are numerous artificial boundaries in the rainbow colour ramp (e.g. dark blue → light blue, light blue → green, green → yellow, yellow → orange), whereas the monochrome colour ramp smoothly transitions from black → white. The viridis colour ramp we have been using so far is perceptually uniform, but others are not. Inspect Figure 6 below. These show the same slope raster, but one is visualised using rainbow, while the other uses a monochrome (perceptually-uniform) colour ramp. Does this influence how you interpret the data? Figure 6: Perceptual uniformity with rainbow and monochrome colour ramps. In a similar fashion, not all colour ramps are suitable for individuals with various forms of colour-blindness (see Levine (2009) and Geissbuehler and Lasser (2013)), who may have difficulty distinguishing reds from greens, or yellows from blues. Luckily, there are now a range of perceptually uniform and colour-blind friendly colour ramps (e.g. Crameri (2018)), including Scientific Colour Maps 6.0. These can be downloaded and used across a range of GIS programs to suit a range of data types. For now, utilise the colour ramps available in R (i.e. viridis) but bear these issues in mind for future map creation. 6.2 Flow Parameters: Pointers Now that we have a DEM that has had all interruptions to flow removed, we can continue to calculate the data needed to define the drainage network i.e. the topological network describing how each cell in a DEM is connected to all others. One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. Whitebox Tools stores D8 pointers as binary numbers in base-2, as follows: We’ll explore the reason for this in Practical 2. In our earlier example of the D8 flow algorithm (see Figure 1), the flow direction was to the SW. This has a binary value of \\(2^4 = 16\\). Therefore, in the pointer file, the highlighted dark blue cell would be assigned a value of 16. Several sub-programs in Whitebox Tools use pointer files to calculate other terrain data, including the algorithms used to calculate catchment area (see below). However, sub-programs rarely require a pointer file as an input, as WBT will automatically generate a pointer layer from the input DEM when required, before discarding the file afterwards. While this processing step is not readily apparent to the user, it is important to realise that it occurs, and it is often good practice to generate a pointer file manually. This can be achieved using the wbt_d8_pointer function. Using the understanding gained above, run the wbt_d8_pointer function, using the correct input files and output file name and directory. Load the raster into R and plot. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. While this is an acceptable plot, it is actually slightly misleading, because the scale bar is continuous, despite the fact that our underlying data are discrete i.e. there are only eight possible values, which are listed below: \\(2^0 = 1\\) \\(2^1 = 2\\) \\(2^2 = 4\\) \\(2^3 = 8\\) \\(2^4 = 16\\) \\(2^5 = 32\\) \\(2^6 = 64\\) \\(2^7 = 128\\) Here is the same data but using a discrete scale: # Manually defined colours based on &quot;RdYlBu&quot; colour palette colours &lt;- c(&quot;1&quot; = &quot;#D73027&quot;, &quot;2&quot; = &quot;#F46D43&quot;, &quot;4&quot; = &quot;#FDAE61&quot;, &quot;8&quot; = &quot;#FEE090&quot;, &quot;16&quot; = &quot;#E0F3F8&quot;, &quot;32&quot; = &quot;#ABD9E9&quot;, &quot;64&quot; = &quot;#74ADD1&quot;, &quot;128&quot; = &quot;#4575B4&quot;) # Plotting p &lt;- ggplot() + layer_spatial(eskdale_pointer, aes(fill = factor(stat(band1)))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_manual(values = colours, na.value = NA) p which is described here: Question: Can you think of any issues with symbolising pointer data? Save your script before continuing. 6.3 Flow Algorithms Once you have determined how each grid cell in a DEM is connected to its neighbours, it is possible to use this pointer information to calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. Measures of catchment area are often used for stream network extraction and other terrain indices. There are three primary measures of catchment area: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment. Unlike the NUE and UCA, SCA is not affected by the grid resolution of the DEM. To calculate the upslope catchment area for each raster cell, we can use the wbt_d8_flow_accumulation function. Run the above function, using the filled DEM as the input file and the output type (out_type) as ‘specific contributing area’ (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. dem_10m_flow_accumulation.tif) and to the correct sub-directory (output/practical_1). Load the raster to R and plot, and your output should resemble the following: Inspect the map output. Question: Which parts of the landscape have low/high values of SCA, respectively? 6.3.1 Comparing flow algorithms So far we have used the most simplistic flow algorithm (D8) to model specific catchment area (SCA). However, there are many flow algorithms available (e.g. Fairfield and Leymarie (1991), Seibert and McGlynn (2007), Grabs et al. (2010)) and each algorithm results in slightly different spatial patterns of water flow. As flow routing is fundamental to the modelling of surface water hydrology, it is important to familiarise yourself with several commonly used methods, although it can be difficult to decide when or where it is appropriate to use a particular routing algorithm. In this section of the practical, we will compare the output of three flow routing algorithms including: D8: see O’Callaghan and Mark (1984); FD8: see Freeman (1991); D-infinity: see Tarboton (1997); Flow algorithms differ in the way in which they: calculate flow direction; model flow divergence, i.e., partition flow between neighbouring downslope cells; handle streams. The D8 algorithm is incapable of modelling flow divergence (or dispersion) because the flow from a particular grid cell is directed to a single downslope neighbour. FD8 (i.e. fractional D8; Freeman (1991), Quinn et al. (1991)) divides flow to each downslope neighbour using a partitioning scheme that is based on the slope angle to neighbouring cells. There are later modifications to this algorithm by Quinn et al. (1991) and Quinn et al. (1995) which increase the degree of flow convergence from the catchment divide to the channel head. You could evaluate the effects of these in Whitebox tools using wbt_quinn_flow_accumulation(). Finally, D-infinity divide flows between a maximum of two downslope neighbours based on the direction of maximum downward slope. Of these, FD8 (Freeman (1991)), and its later modifications (e.g., Quinn et al. (1995)), is the only algorithm of the three that explicitly recognises that flow on hillslopes should be divergent while flow along valley bottoms should be convergent. It is worth noting that there are many other single-flow direction (SLF) and multiple-flow direction (MFD) algorithms that can be used, see Li et al. (2020). In the final part of the practical, we will assess the specific catchment areas (SCA) derived from each of the flow algorithms discussed above. 6.3.2 FD8 catchment area To calculate the catchment area using the FD8 algorithm (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)), use the wbt_fd8_flow_accumulation function. Use the wbt_fd8_flow_accumulation function, making sure to select ‘specific contributing area’ and remembering to log-transform the values. The exponent parameter should be set to 1.1 (exponent = 1.1), as suggested by Freeman (1991). Save with an appropriate name (e.g. dem_10m_flow_accumulation_fd8.tif). Load the raster in R and plot. 6.3.3 D-infinity catchment area To calculate the catchment area using the D-infinity algorithm of Tarboton (1997): Use wbt_d_inf_flow_accumulation. Use the same input parameters as above (i.e. SCA, log-transformed values). Save with an appropriate name (e.g. dem_10m_flow_accumulation_d_inf.tif). Load the raster in R and plot. 6.3.4 Comparing the outputs The output of the algorithms should resemble the following: Figure 7: Flow algorithm comparison. Results from the D8, FD8 and D-Infinity algorithms. Plot created using ggplot2 and the patchwork package (useful for combining plots). When water flows over a hillslope, you would expect there to be some degree of divergence depending on the topography (i.e. some hillslopes are convergent and some are divergent). However, once water becomes channelised, you would no longer expect flow to be divergent. Once water is in a channel it stays there until it reaches a lake or ocean. Branching in channels is rather rare. So let’s assume that the pattern of SCA should display some degree of dispersion on hillslopes and no dispersion at all along valley bottoms (i.e. in streams). Flow divergence or dispersion is apparent in SCA images by a ‘fuzzy’ quality, almost like the SCA has been smudged. You can zoom into individual plots using the Zoom button in the Plot Window or edit the x- and y-axis limits of your plots by adding the following to your ggplot2 call: # Sets x- and y-axis limits using coord_sf() + coord_sf(ylim=c(504063,506727), xlim = c(320705, 322255)) The above code uses British National Grid coordinates (m), rather than decimal degrees. To finish Practical 1, save your script and complete the following: Compare the D8, FD8 and D-Inf outputs. Either cycle between the plots using the plot window or combine them using the patchwork package. This would need to be installed and loaded to the library. Which algorithms do a good job modelling the pattern of contributing area on hillslopes? Can you see signs of flow divergence along streams in any of the SCA images? Which algorithms perform best for modelling the pattern of SCA in fluvial (stream) environments? Do any of the algorithms do a good job for both hillslopes and streams? Congratulations! You have now completed the Practical 1 (Eskdale; Chapters 4 - 6). You now have considerable experience running whitebox functions as well as visualising spatial data using ggplot2. 6.4 Practical solution As with the introduction to R (Chapter 2 - 3), I will provide a code solution to the Eskdale practical (Chapters 4 - 6), outlining the major analytical steps. This will be released prior to the Week 10 class. # Solution to Practical 1 (Eskdale) # Author: matt-tomkins # Date of upload: xx-xx-xxxx File handling "],["Hints.html", "Handy Hints", " Handy Hints This chapter contains a few handy hints which may be useful if you’re having any issues loading R packages or running Whitebox tools. Dealing with errors Errors are an inevitable aspect of coding, irrespective of how experienced you are! As we gain programming experience, these errors increase in difficulty from initially simple problems, such as incorrect file paths or errors in code syntax, to the more complex. Being able to deal with errors is therefore an essential programming skill. My top tips for fixing errors are: Read the error message (red text in the R console). If this appears, it usually means that something has gone wrong! Error messages can vary in quality, but at the very least they should tell you: where the error was encountered i.e. which line of code failed. some indication as to why the error occurred e.g. No such file or directory If the error message doesn’t provide a simple solution to the problem, then the next step should be to work through your code step-by-step, testing each line or code block to ensure it is working correctly (usually using print() statements). To illustrate this, here is some code which has failed: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;) check.packages(packages) # Fill depressions in a DEM wbt_fill_depressions(dem = here(&quot;data&quot;, &quot;practical_2&quot;, &quot;dem_10m.tif&quot;), output = here(&quot;output&quot;, &quot;practical_2&quot;, &quot;dem_10m_fill.tif&quot;)) # Loads raster filled_dem &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;dem_10m_fill.tif&quot;)) # Plots using ggplot g &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(filed_dem, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Elevation (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot; + theme(legend.position = &quot;top&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot; na.value = NA) # Shows plot g I would first inspect the error message (No such file or directory) but if I didn’t understand it, I would then troubleshoot by: First running the check.packages(packages) function. Were all the packages installed correctly i.e. TRUE? Then checking the wbt_fill_depressions() function. Does this produce an error? Does it correctly produce an output raster in the intended location? Does the output look sensible? Plotting the raster might be useful here to check. Then running the raster() function to load the raster into R. Here the code would fail and I could then start to investigate further: Are there any syntax errors? e.g. use of commas, correct number of brackets, spelling errors? Are the inputs defined correctly? In the above example, the code fails because the raster package is trying to load dem_10m_fill.tif from data/practical_1 when it was actually saved in output/practical_1. Running the code again might produce another error, in which case I would repeat the above process, checking the code step-by-step until the error can be located and fixed. Can you spot any errors in the code below? There are three to find… # Plots using ggplot g &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(filed_dem, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Elevation (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot; + theme(legend.position = &quot;top&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot; na.value = NA) TLDR: the best way to fix errors is to be systematic in your approach. Work your way through the code step-by-step, checking it is producing the intended outputs at each stage - just because the code doesn’t produce an error, it doesn’t mean it is producing the output that you want! Whitebox functions vs. the raster package So far, there has been some confusion regarding the role of whitebox functions, the raster package and ggplot2. Each of these has a unique role, as visualised below, while analysis needs to be conducted in the correct order to avoid errors. Analysis structure. [1] Create output files using whitebox [2] Load outputs into R using raster [3] Plot them using ggplot To illustrate this, below we have an example of the wbt_d8_pointer function, which creates a D8 pointer file based on an input digital elevation model (dem): # Creates a D8 pointer file (.tif) at the output location, based on the input dem wbt_d8_pointer(dem = here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_dem_fill.tif&quot;), output = here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_D8_pointer.tif&quot;)) This uses the here package for locating the input file (mersey_dem_fill.tif), which is stored in data/practical_2, and for saving the output, which is stored as mersey_dem_D8_pointer.tif in output/practical_2. If this runs successfully (make sure to carefully check file paths, file names, use of commas), then an output raster will be created at the specified location. Crucially, this does not load the raster into the R environment, so any attempt to plot the raster using ggplot2 would fail. To achieve this, a raster must be first loaded into the R environment as follows: # Loads D8 pointer raster using the raster and here packages mersey_pointer &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_D8_pointer.tif&quot;)) Again, we have determined the input file location using here package (output/practical_2/mersey_dem_D8_pointer.tif) and stored the raster as an object in the R environment called mersey_pointer. With this stored, it can then be plotted using ggplot, setting layer_spatial to the mersey_pointer object name: # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_pointer, aes(fill = stat(band1))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) p TLDR: for subsequent analysis, make sure to adhere to the following structure: Create output files using whitebox functions. Load those output files into R using the raster package (stored as objects) Plot the R objects using ggplot2 Any deviation from this order (e.g. plotting without loading using raster, loading files that have not yet been created using whitebox) will cause errors! R Projects Make sure to initialise an R project for your work (Section 5.4) and ensure this is created for your GEOG70581 directory. This should resemble the following, with the R project file and a separate R script for the Eskdale and Mersey Basin practicals: File paths Remove spaces in directories or file paths. As a rule, avoid using spaces when naming files or folders as this can cause issues, particularly when coding: “P:/Environmental Monitoring Concepts/GEOG70581/…” Bad “P:/Environmental_Monitoring_Concepts/GEOG70581/…” Good Output files Make sure to inspect output files. When you run a command (e.g. a Whitebox function), check that it produces the intended output (e.g. a raster file (.tif) in the output directory). If there is an error or the output hasn’t been created, subsequent code will fail. Packages If you’re having difficulty loading the raster or sf packages on a University managed computer, this is due to multiple package installs. Go to the packages window, identify the older version of each package, and tick the check box to load it into your R library. Code structure While you will not be assessed on the quality of your code (you do not have to submit your code for the assessment), remember that well-formatted code (with comments and good structure) is easier to read and understand and will be less prone to error. Inspect the code below: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) This code includes comments for each main code block, line spaces to distinguish different parts of the code, and is written in a logical order (e.g. first loading packages, then loading/selecting data, running tools). Any non-essential code has been removed e.g. additional calls to install.packages() or library(). Errors with R projects and the here package If you’re working on a university-managed computer, you may have encountered errors while trying to use whitebox functions, such as wbt_fill_depressions(), or accessing files using relative file paths via the here package. This problem actually originates with our use of R projects (described here). R projects are designed to make accessing files simple and to ensure our analyses are self-contained and portable. They do this by automatically setting the working directory in RStudio. You can test this by using the getwd() function, which should print your working directory to the console. If the R project is working, this should set your working directory to the directory where your R project is saved e.g., an R project file stored in C:/Users/Bob/Documents/GEOG70581 should have an identical working directory. Unfortunately, R projects were not working as intended on university-managed computers in 2023/24: the working directory was not being automatically updated. In most cases, the working directory was stuck at P:/, even if the R project is saved elsewhere! This was a completely new problem for 2023/24, but it is possibly due to changes in the waycomputers are managed after the recent cyber incident. This can lead to difficulties accessing files using here, but also utilising whitebox functions, even when inelegant absolute file paths are used. In both cases, R is looking in the wrong place, either for the data files, or the WBT executable. This issue should be solved for 2024/25, but if it arises again, there are two options to fix this: The simplest approach is to start from scratch, downloading the course data, and creating a new R project in the same directory. If successful, it should resemble the following: The next step is to set the working directory manually using the setwd() function (see below for an example). This should update the working directory to the correct location, such as C:/Users/Bob/Documents/GEOG70581 or P:/GEOG70581 depending on where you data are saved, and should be done before installing packages. The practical steps can then be following as normal and the whitebox functions should work as intended. Try running the code below, making sure to update the path to the correct working directory. # FIRST, set your working directory manually setwd(---YOUR PATH GOES HERE---) # e.g., &quot;C:/Users/Bob/Documents/GEOG70581&quot; # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;sf&quot;, &quot;ggspatial&quot;) check.packages(packages) # Initialise whitebox tools executable whitebox::install_whitebox() # Check whitebox is in the library and the version library(whitebox) wbt_version() # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g If you want to continue with an existing directory/R Project, then you will need to set the working directory manually (setwd()) and set the origin for the here package, as shown below: # Set the working directory manually setwd(&quot;C:/Users/Bob/Documents/GEOG70581&quot;) # UPDATE TO YOUR DIRECTORY # Manually set the origin for the &#39;here&#39; package here::i_am(&quot;practical-1-eskdale.R&quot;) # UPDATE IF A DIFFERENT FILE NAME USED The above solutions have been tested on a range of university-managed computers, working on both the P: and C: drive. For other installation issues, please contact me on my @manchester email. For other R issues, please post on the forum. Catchment characteristics from categorical data In Task 4, we calculated some important catchment characteristics from the continuous datasets (e.g., average_elevation, average_rainfall, average_slope, average_aspect). To simplify the analysis, and to give you more time to focus on statistical analysis and interpretation, I dealt with the categorical datasets (land cover, soil type, bedrock type) and provided the summary values in mersey_EA_characteristics.csv. The following instructions outline how this was completed. While you do not need to run this code as part of the practical, it is useful for you to understand the process, especially if you wanted to use this or similar techniques in future e.g., for your dissertations. Reclassification The categorical files mersey_LC (based on LCM2000 data), mersey_HOST (Hydrology of Soil Types) and mersey_bedrock contain many different detailed classes, some of which are not applicable to the study region. Therefore, the datasets need to be simplified by aggregating some classes and omitting unnecessary classes. For example, there are 26 classes in LCM2000 data, each with a unique numeric identifier. These were simplified into the following five macro-classes, alongside their numeric identifiers: Arable = 41, 42, 43; Heath = 91, 101, 102; Grassland = 51, 52, 61, 71, 81; Urban = 171, 172; Wetland = 111, 121. This was achieved using the fct_collapse function from the forcats package, using the following approach, which can you replicate: First, load the land cover raster into R as normal: # Loads land cover raster land_cover &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_LC.tif&quot;)) Because our raster is categorical (rather than continuous), it makes sense to convert the data format to a factor. In R, these are used to represent categorical variables. Next, convert the land cover raster to a factor: # Converts the land cover raster to a factor, overwriting the original variable land_cover &lt;- as.factor(land_cover) To assess the values stored in the land_cover raster, use the unique function, which should produce the following: ## [1] -9999 0 11 21 41 42 43 51 52 61 71 81 91 101 102 ## [16] 111 121 131 161 171 172 191 211 212 221 As not all categories are applicable to our study area, we are next going to create a data frame of the land cover categories of interest. Inspect the code below. Many of the code elements should be familiar to you. We are creating a vector called categories using the c() function, which contains all the classes of interest c(41, 42, 43, ...)). Next, we have converted that to a data frame using as.data.frame() and assigned a column name (ID). When you understand what is happening, add to your script and run. # Categories of interest categories &lt;- as.data.frame(c(41, 42, 43, 91, 101, 102, 51, 52, 61, 71, 81, 171, 172, 111, 121)) colnames(categories) &lt;- &quot;ID&quot; If you want to inspect the output, you can use head(categories) to print out the first 6 rows: ## ID ## 1 41 ## 2 42 ## 3 43 ## 4 91 ## 5 101 ## 6 102 Using this new data frame, we are going to create a new column called name, which corresponds to the name of the land cover class (e.g. \\(Arable = 41\\)) # Collapse categories into groups based on ID categories$name &lt;- fct_collapse(as.factor(categories$ID), &quot;Arable&quot; = c(&quot;41&quot;, &quot;42&quot;, &quot;43&quot;), &quot;Heath&quot; = c(&quot;91&quot;, &quot;101&quot;, &quot;102&quot;), &quot;Grassland&quot; = c(&quot;51&quot;, &quot;52&quot;, &quot;61&quot;, &quot;71&quot;, &quot;81&quot;), &quot;Urban&quot; = c(&quot;171&quot;, &quot;172&quot;), &quot;Wetland&quot; = c(&quot;111&quot;, &quot;121&quot;)) Inspect the above code. The syntax is reasonably complex, but you should understand what is happening if you inspect the output: # Prints categories data frame categories ## ID name ## 1 41 Arable ## 2 42 Arable ## 3 43 Arable ## 4 91 Heath ## 5 101 Heath ## 6 102 Heath ## 7 51 Grassland ## 8 52 Grassland ## 9 61 Grassland ## 10 71 Grassland ## 11 81 Grassland ## 12 171 Urban ## 13 172 Urban ## 14 111 Wetland ## 15 121 Wetland Finally, we can use this updated data frame to replace (or substitute) values in the land cover raster (i.e. \\(41, 91, ...\\)) with the land cover class it represents. In this case, values are stored numerically (i.e. \\(Arable =1,Heath =2, ...\\)). One way to achieve this is using the subs function from the raster package. Inspect the following code, which substitutes (reclassifies) the raster layer and saves to a new raster (.tif) using the writeRaster function. # Substitutes raster values with new categories land_cover_classified &lt;- subs(land_cover, categories) # Write to new raster writeRaster(land_cover_classified, here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;), overwrite = TRUE) When you understand it, run the code, load the new raster into R and plot. Use the code below to visualise the reclassified raster, taking note of the manual fill aesthetic (scale_fill_manual): # Loads land cover raster using the raster and here packages mersey_land_cover &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;)) # Manually defined colours for land cover categories # (1=Arable, 2=Heath, 3=Grassland, 4=Urban, 5=Wetland) colours &lt;- c(&quot;1&quot; = &quot;#D1F5D8&quot;, &quot;2&quot; = &quot;#87C292&quot;, &quot;3&quot; = &quot;#498955&quot;, &quot;4&quot; = &quot;#FFA888&quot;, &quot;5&quot; = &quot;#4575B4&quot;) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_land_cover, aes(fill = factor(after_stat(band1)))) + # Adds raster layer theme_classic() + labs(fill = &quot;Land cover class&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_manual(values = colours, na.value = NA, # Updated fill aesthetic labels=c(&quot;Arable&quot;, &quot;Heath&quot;, &quot;Grassland&quot;, &quot;Urban&quot;, &quot;Wetland&quot;)) + # Manual legend labels theme(legend.position = &quot;top&quot;) p Using the methodology outlined above, this process can be repeated for the soil type (mersey_HOST) and bedrock geology rasters (mersey_bedrock). Make sure to use a consistent approach to file naming e.g. mersey_HOST_reclass.tif and mersey_bedrock_reclass.tif. There are 29 classes in HOST, each with a unique numeric identifier. Reclassify these into the following four new classes: Permeable = 1, 3, 4, 5, 6, 7, 15, 16; Impermeable = 12, 17, 18, 19, 21, 22; Gleyed = 8, 9, 13, 24; Peats = 10, 11, 14, 25, 26, 27, 29. Utilising the order above, the values of the output raster should be as follows: Permeable = 1 Impermeable = 2 Gleyed = 3 Peats = 4 There are 34 bedrock geology classes in Mersey Basin region, each with a unique numeric identifier. Reclassify into the following three new classes listed below: Sands_and_Muds (sands and muds) = 5, 16, 18, 24, 28, 34; Limestone = 10, 11, 17, 19; Coal = 9, 15, 22. Utilising the order above, the values of the output raster should be as follows: Sands_and_Muds = 1 Limestone = 2 Coal = 3 When complete, save your script before continuing. You should have three reclassified rasters in your output directory i.e., mersey_LC_reclass.tif,mersey_HOST_reclass.tif,mersey_bedrock_reclass.tif`. Extracting-normalising surface derivatives In the next phase, we want to count the number of cells for each class per watershed i.e., number of arable cells, number of grassland cells, number of limestone cells. If we know the total number of cells per watershed, we can calculate the proportion i.e., arable %, grassland %, limestone %. First, ensure the reclassified land cover, soil type and bedrock rasters are loaded into R and stored with sensible variable names (e.g. land_cover, soils, bedrock). # Load raster data land_cover &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_LC_reclass.tif&quot;)) soils &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_HOST_reclass.tif&quot;)) bedrock &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_bedrock_reclass.tif&quot;)) Next, we are going to use the extract function again, but this time returning the count of each category (e.g. Arable, Heath, Grassland, Urban, Wetland) for each watershed area, as shown here: # Load watersheds (vector, sf, VALUE column is important) and renames watersheds_ea &lt;- st_read(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.shp&quot;)) # Extract land cover counts (5 classes so levels = 1:5) land_cover_classes &lt;- extract(land_cover, watersheds_ea, fun=function(i,...) table(factor(i, levels = 1:5))) Rather than trying to decipher the code straight away, copy to your script, run and use head() to inspect the output, which should be as follows. This may take a little while (~30 seconds on a i7 computer with 16 Gb of RAM): head(land_cover_classes) ## 1 2 3 4 5 ## [1,] 18 41 2779 88 0 ## [2,] 28 73 4727 223 21 ## [3,] 16 108 1333 0 221 ## [4,] 90 61 1544 669 0 ## [5,] 0 90 1008 13 221 ## [6,] 16 49 1230 6 18 We have produced a data frame with 5 columns (representing the 5 land cover classes) and 70 rows (representing the 70 watersheds), where the row-column values represent the number of raster cells corresponding to each land cover class. This is based upon a user-defined function, incorporating the table function from the data.table package. Important: as the land cover dataset contains 5 classes, the function splits the underlying data into five groups using levels = 1:5. This needs to be updated when applying to the soils and bedrock datasets. While this information is useful, it complicates our statistical analysis and our comparison between watersheds, as there are a different number of cells per watershed, reflecting their varying sizes. To convert these to percentages, there are a couple of possible approaches. The first is to use the rowSums function, which is available in base R and (as the name suggests) calculates the sum of each row within a matrix or dataframe. Note that we use the form base:: to specify we are using the base package, as rowSums is also the name of a function in other packages, such as raster, see here. You can test the code below, which calculates the land cover percentage (proportion * 100) # Land cover proportion as a function of the total number of land cover cells (rowSums), converted to a percentage (* 100) land_cover_classes &lt;- land_cover_classes / base::rowSums(land_cover_classes) * 100 The above approach calculates the percentage per class (arable, heath, grassland, urban, wetland) as a function of the total number of cells in those five classes. In our reclassification we focused on the macro-classes which are known to have the greatest impact of river hydrochemistry (e.g. urban areas, farmland). However, other land cover categories are found within each watershed, but these typically account for only a small percentage of the total area and are assumed to have a limited effect on the river environment. While these categories have been excluded to simplify the analysis, this does mean that the total number of cells in the watershed will be greater than the number of cells in the five macro classes, which will influence our percentage calculation. To reflect this, we might want to calculate the class percentage as a function of the total watershed area. This can be completed using the extract function, this time using a custom function (fun=function(x, ...) length(x)) to count the number of cells within each watershed: # Total number of cells within each watershed land_cover_count = extract(land_cover, watersheds_ea, fun=function(x, ...) length(x)) # Calculate proportion using sweep, convert to percentage land_cover_classes &lt;- sweep(land_cover_classes, 1, land_cover_count, FUN = &quot;/&quot;) * 100 To improve the readability of the data frame, update its column names as follows: colnames(land_cover_classes) &lt;- c(&quot;Arable_percent&quot;, &quot;Heath_percent&quot;, &quot;Grassland_percent&quot;, &quot;Urban_percent&quot;, &quot;Wetland_percent&quot;) Use head() to inspect the output: head(land_cover_classes) ## Arable_percent Heath_percent Grassland_percent Urban_percent Wetland_percent ## [1,] 0.5584859 1.272107 86.22401 2.7303754 0.000000 ## [2,] 0.5217067 1.360164 88.07527 4.1550214 0.391280 ## [3,] 0.8311688 5.610390 69.24675 0.0000000 11.480519 ## [4,] 3.5074045 2.377241 60.17147 26.0717069 0.000000 ## [5,] 0.0000000 5.579665 62.49225 0.8059516 13.701178 ## [6,] 1.1644833 3.566230 89.51965 0.4366812 1.310044 Replicate this approach for the soil type and bedrock datasets, using sensible variable names (e.g. soils_classes and bedrock_classes), remembering to update the extract function used (i.e. levels = 1:4 for the soil data and levels = 1:3 for the bedrock data). When complete, you should have the following data frames in your R environment: watersheds_ea: containing the water quality measurements, the spatial areas and the continuous derivatives; land_cover_classes: containing the proportion for each land cover class; soils_classes: containing the proportion for each soil type; bedrock_classes: containing the proportion for each bedrock type; To simplify your R environment, remove all other variables using the rm() function. To merge the remaining files, we can use cbind(), which binds data frames together based on their columns: # Combines watersheds data frame with proportions watersheds_ea &lt;- cbind(watersheds_ea, land_cover_classes, soils_classes, bedrock_classes) "],["file-handling.html", "File handling", " File handling "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
