[["index.html", "Chapter 1 Introduction 1.1 Practical outline 1.2 Objectives 1.3 Schedule", " Hydrological analysis in R Chapter 1 Introduction 1.1 Practical outline In this set of practicals, we’ll be using R, RStudio and WhiteboxTools to perform hydrological analysis. In Chapters 2 and 3, we’ll install and get to grips with the required programs. In Practical 1 (Chapters 4 - 6), which uses data for the Eskdale watershed, we’ll introduce you to some of the key approaches, including DEM pre-processing, calculating flow parameters and comparing flow algorithms. In Practical 2 (Chapters 7 - 11), we’ll utilise these approaches to investigate the water quality of the Mersey Basin. This forms the basis for your assessment. Note: the relevant chapters will be made available as we progress through the semester, hence why Practicals 2 and the assessment information are not currently visible. Practical 2 will be released prior to the Week 10 lecture. 1.2 Objectives After completing these practicals, you will: have gained experience with new tools (e.g. R, RStudio, WhiteboxTools); become familiar with hydrological approaches used to simulate overland flow from digital elevation models (DEMs); utilised statistical analysis to investigate the relationships between water quality indicators and catchment characteristics; improved your understanding of the hydrological processes influencing water quality. 1.3 Schedule These practicals will be completed across Weeks 8 to 12 as follows: Week 8: Introductory lecture (~1 hr) Complete Chapters 1 - 3: An introduction to R (~2 hr) Weeks 9: Lecture on Hydrological and Geomorphological models (~1 hr) Complete Chapters 4 - 6: Eskdale practical (~2 hr, plus independent work) Weeks 10 - 12: Lecture on Water Quality Modelling (~1 hr) Complete Chapters 7 - 11: Mersey practical (~6 hr, plus independent work) The assessment, based upon Practical 2 (Mersey), is due 09/01/2025 (14:00). "],["Installation.html", "Chapter 2 Installation 2.1 R 2.2 R Studio 2.3 Course materials", " Chapter 2 Installation During this practical you will be using WhiteboxTools, an open-source GIS software package for hydro-geomorphic applications. While it contains much of the functionality found in a standard GIS software package, it also includes novel functionality specific to hydrological analysis. The tools are also computationally efficient (i.e. fast!), taking advantage of parallel processing and other advancements in computing power. Finally, the tools are developed for cross-platform use, so can be accessed through code (e.g. R, Python) or other GUIs (e.g. QGIS, ArcGIS) if required. To run WhiteboxTools functions (WBT), we’ll be using R as a front-end interface. This will allow us to access and run all of the relevant hydrological functions from WBT, while utilising the statistical and visualisation capabilities of R. 2.1 R If you’re working on a university-managed computer (in HBS 2.2, HBS 2.88 or Mansfield Cooper 2.01) or accessing the course via Citrix, both R and RStudio (described below) should already be installed or can be accessed through the Software Centre. To Install R on a personal computer: Open an internet browser and go to https://www.r-project.org/. Click the “download R” link in the middle of the page under “Getting Started.” Select a CRAN location (a mirror site) and click the corresponding link e.g. the site provided by Imperial College London. Click on the “Download R for Windows” link, or the relevant link for your operating system (macOS, Linux). Click on ‘base’ and then Download R [Version Number] for Windows. The current version is 4.2.2. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.2 R Studio While R is now installed, it is good practice to use an integrated development environment (IDE) to access and run R functions. The most well known IDE for R is RStudio, which includes a console, higher quality graphics, as well as tools for plotting, history, debugging and workspace management. Note: R may be used without RStudio, but RStudio cannot be used without R. To Install RStudio on a personal computer: Go to https://www.rstudio.com/products/rstudio/download/ and click on Download RStudio for Windows. Installers for macOS and other operating systems are available below. Once downloaded, double click the application file (.exe) and follow the installation instructions. 2.3 Course materials All the data required for this sets of practicals are available on Blackboard under Course Content → Week 8 to 12. Download the folder, save it to an appropriate location and extract the contents (unzip). The directory structure is as follows: GEOG70581 Data Practical_1 Practical_2 Images Practical_1 Practical_2 Output Practical_1 Practical_2 All the required data files for Practicals 1 and 2 are stored in the associated sub-directory within Data. As we move through the practicals, we’ll be creating a number of additional GIS files and we encourage you to save these within the corresponding sub-directory within Output. Finally, maps and figures will be required for the assessment. These can be exported directly from R and stored in Images. "],["Intro_to_R.html", "Chapter 3 A (brief) introduction to R 3.1 Overview 3.2 Loading R 3.3 Objects 3.4 Data structures 3.5 Scripts 3.6 Loading packages 3.7 Loading data 3.8 Plotting 3.9 Formative task", " Chapter 3 A (brief) introduction to R 3.1 Overview The following chapter provides a basic introduction to coding in R. If you already have experience with coding, perhaps from Understanding GIS, then the following content should be familiar to you. For those of you with little or no experience with code, the content builds from the simple (using R as a calculator) to the complex (loading packages and data, plotting) and will equip you with the skills needed to complete Practicals 1 (Eskdale) and 2 (Mersey) You should aim to work through this Chapter by the end of Week 8, making sure you understand the concepts and code before moving on to the next sub-section. To test your understanding, there is a short formative task (non-assessed) at the end of this chapter. This covers many of the techniques covered here, as well as unfamiliar ones, so you may have to do some independent research to complete it. I’ll post the solution after the class so you can check your working. 3.1.1 What is R? R was originally developed in the 1990s for statistical data analysis. While it is still optimized for statistical analysis and data visualization, it has been utilised for a much wider range of applications. As a high-level programming language, it is less complicated and more user friendly than lower-level languages (e.g. C, C++, Java, Rust), but this comes at the expense of being slightly slower to execute. Importantly, R is free and open-source and its core functionality can be extended using packages. These are available through the Comprehensive R Archive Network (CRAN) and allow the user to perform a range of tasks (e.g. reading and writing files, rendering images), without the requirement to delve into the underlying source code. While there are a myriad of free online resources available to help you learn R, including R for Data Science and YaRrr! The Pirate’s Guide to R, as well as paid options from Data Camp, this chapter and the subsequent practicals will equip you with all the skills required to complete the course. 3.1.2 Why code? For those of you familiar with desktop GIS (e.g. ArcGIS, QGIS) or other statistical programs (e.g. Excel, SPSS), the transition to using code can be a frightening experience! However, there are a several reasons why it is worth persevering with. First, and with career progression in mind, the ability to code is becoming increasingly important, particularly for those of you who want to progress down GIS or environmental career paths. Getting to grips with R and understanding the fundamentals of coding will equip you to learn new skills and new coding languages which may be required by a future employer (e.g. Python, MATLAB, Julia, Ruby, …). Second, performing your analysis using code ensures that your outputs are reproducible. If you provide someone else with the original data files and the corresponding code i.e. a list of operations carried out by the computer (e.g. read data, modify, plot, perform statistical test, save), they should be able to reproduce your outputs exactly. Third, code is repeatable. In standard GIS software (for example), most functions allow you to perform a single task on a single data file e.g. clipping a digital elevation model to the geometry of a shapefile. By comparison, if that same functionality can be implemented in code, it can be applied as easily to 10 data files, or 1000, or 10,000. Finally, code is easily modifiable. Code can be tweaked, adapted, or completely rewritten to produce the desired outputs. In R, a wide range of functions are already available in packages, but you can write your own functions or combine existing functions to suit your analysis. This functionality is often not the case in other software, which are generally more complicated to modify and typically provide Black Box solutions, where we can inspect the inputs and outputs, but typically have limited understanding of the steps in between. Black Box programming 3.1.3 A quick note on the practicals As we progress through this course, make sure to read the content carefully. However, you do not have to run every bit of code in this document, although have a go where you feel it would help your understanding. If I explicitly want you to do something, I will write an instruction that looks like this: This is an instruction that tells you something to either think about, or do. 3.1.4 Dealing with errors Before we load R and take our first steps towards coding excellence, it is worth noting that errors are an inevitable aspect of coding. Irrespective of how good you are, you will spend more time fixing your code (i.e. debugging) than writing it! Luckily, when R encounters an error, it will try and help you out. For example, shown below is some simple R code. Here, we are tying to create a new object called a, which is the product (+) of object b and the numeric value 1: a &lt;- b + 1 ## Error in eval(expr, envir, enclos): object &#39;b&#39; not found In this example, the code fails and prints the error message object 'b' not found because the object b does not exist! We could solve this by defining b beforehand as follows: b &lt;- 3 a &lt;- b + 1 a ## [1] 4 These error messages are important and contain helpful information - do not ignore them! If you’ve struggled to resolve the problem, you should then Google it (normally by just typing “R” followed by the error message into Google), e.g. R Error in eval(expr, envir, enclos): object 'b' not found. Another useful source of information is StackOverflow which is a public collection of coding questions and answers. If you’ve come across a challenging error, there’s a high probability that someone else has also encountered the same error, and there may be a solution already available. Understanding, interpreting and fixing error messages is a key programming skill, so read the error messages carefully and use the above resources (Google, StackOverflow). If you need any additional help, staff and TAs will be happy to help during the practicals. 3.2 Loading R Now that we understand what R is, why it’s useful and what to do when something goes wrong, let’s begin. Open RStudio. The RStudio user-interface should resemble the following, although the exact arrangement of the windows may differ from that below: This contains the following primary windows, which are described below: In the console window, commands can be typed and results returned. For example, typing the following command into the console: 1 + 2 produces: ## [1] 3 Copy and paste the above code into the console and press Enter to run. Does it produce the correct result? In the Workspace window, you can see all the objects that you have created in the current R session (Environment tab; currently empty) as well as a record of the commands you’ve used (History tab; 1 + 2). At it’s most basic, R can operate as a powerful calculator. We can add +, subtract -, multiply * and divide /, take the exponent ^, calculate the square root sqrt() or the logarithm of a number log10(), or melt our computers by using the factorial() function. Operators can also be combined to produce more complicated mathematical functions. 3.3 Objects One of the most important things to know about in R is the variable or object. In this set of practicals, we use the term “object”, although variable is also used commonly across different programming languages. Objects are containers that you can store values in, and then refer to again later. R creates objects for you automatically, so all that is required is to name it and then use the assignment operator &lt;- to assign a value to it. As a general rule (and for historical and other complicated reasons), use &lt;- for assignment, as shown below, and don’t use =: b &lt;- 3 Run the above code. This assigns the value of “3” to an object named “b”: You should now see that under the Workspace window, the object b has been added to the Environment tab: The object b can now be called again by the user. Run the following in the console. Does it work as expected? b + 5 There are number of benefits to using objects. For example, take the following: a &lt;- 5 b &lt;- 10 result &lt;- (sqrt(a) + log(b))^a + 2*b result # Typing the name of an object (&#39;result&#39;) prints it in the Console Window ## [1] 1945.905 This is a relatively complicated mathematical formula! It uses a square root sqrt() and logarithm log() function, as well as multiplication * and powers ^. We could replicate this result by substituting each value of a for 5 and each value of b for 10, but this would be time consuming and prone to error, and would be very frustrating if we wanted to re-calculate the result for a = 6 and b = 11! Using objects allows us to use and modify values multiple times within the code. Modify the code above to test for a = 6 and b = 11. Compare your answer with a friend in class. Did you get the same result? Using objects is also important for repeatability. Let’s say we want to calculate the sum of the squares from 1 to 5 i.e. \\(1^2 + 2^2 + 3^2 + 4^2 +5^2\\). We could write this numerically in R as follows: 1^2 + 2^2 + 3^2 + 4^2 + 5^2 This produces the correct result: ## [1] 55 However, what if wanted to perform this calculation for all the numbers from 1 to 50 i.e. \\(1^2 + 2^2 + 3^2 + ... + 50^2\\)? What if we wanted to cube each value (i.e. \\(n^3\\)), rather than squaring? Either change would require a great deal of manual editing, with lots of potential for error. By comparison, using objects allows us to loop (or iterate) through a series of calculations. The code below creates a object called result and loops through a sequence of numbers using seq() to perform the above calculation, where the object i is updated each iteration. # Initialises an object called result, with a value of 0 result &lt;- 0 # Loops through a sequence from 1 to 5 in increments of 1 (i.e. 1, 2, 3, 4, 5) for(i in seq(from = 1, to = 5, by = 1)){ # Squares each number (i) and adds to result result &lt;- result + i^2 } # Prints object &#39;result&#39; to the Console Window result ## [1] 55 Run the above code in the console. Try to modify the code to perform the calculation for 1 - 50; this should equal \\(42925\\) At this stage, don’t worry about understanding the syntax of the code above. The important thing to remember is that using objects allows to us to perform more complex and repeatable analyses. 3.3.1 Object names When defining a object name, you can’t use spaces or characters that are not a-z, A-Z or 0-9. As a general rule, objects in R should be written in “snake case” (as opposed to “upper case” or “lower case”) where all words are in lower case and are separated by underscores (_). All of these are valid object names in snake case: snake_case a_really_long_snake_case_name snake s Other popular naming conventions include camelCase, UPPER_CASE or hyphen-case (among many others). Pick one and use it consistently. 3.3.2 Object types Each object will have a particular type, which specifies what kind of a value it can hold. Whilst this is handled automatically for you by R, it is important that you understand what the different types are. Here are some common examples: # Character i.e. a string of letters, numbers and/or characters a &lt;- &quot;Bill&quot; # Numeric i.e. a number with decimals b &lt;- 3.567 # Integer c &lt;- 3 # Logical i.e. TRUE or FALSE d &lt;- TRUE Run the above code. If successful, your workspace should now include the objects a, b, c, d. Some data types are compatible e.g.  # Numeric + integer b + c ## [1] 6.567 But others are not! # Numeric + character b + a ## Error in b + a: non-numeric argument to binary operator Can you understand the error message above? Why does b + a produces an error? 3.4 Data structures In the previous examples, each object has consisted of a single element. This could be a name (bill), a decimal number (3.567), an integer (3), and so on. However, R can store data in a range of different structures, some of which you will encounter as we progress through the course. 3.4.1 Vectors One of the simplest structures is the vector, which contains multiple elements of the same type. These are typically created using the command c(): # Creating a vector (v) of the numbers 1 - 5 v &lt;- c(1, 2, 3, 4, 5) v ## [1] 1 2 3 4 5 As a vector must have elements of the same type, R will try and coerce elements to the same type: # Creating a mixed vector (m) containing numeric (1,2), character (&quot;Bill&quot;, &quot;Ben&quot;) and logical elements (TRUE) m &lt;- c(1, 2, &quot;Bill&quot;, &quot;Ben&quot;, TRUE) m ## [1] &quot;1&quot; &quot;2&quot; &quot;Bill&quot; &quot;Ben&quot; &quot;TRUE&quot; In the above example, the numeric and logical elements have been coerced to characters, as signified by these elements being enclosed by quotation marks \" \". We can also create vectors programmatically, rather than manually defining each element. For example, we can generate sequences using seq() or repetitive sequences using rep(): # Creating a sequence from 1 to 5, in increments of 1 s &lt;- seq(from = 1, to = 5, by = 1) s ## [1] 1 2 3 4 5 # Creating a vector of length 5, consisting of the value 10 r &lt;- rep(10, times = 5) r ## [1] 10 10 10 10 10 3.4.2 Data frames Other common R data structures include matrices, lists and data frames. The data frame is the most commonly used structure for tabular data and will be familiar to you, as this similiar in structure to a Microsoft Excel spreadsheet (containing rows and columns of data). Here is an example data frame, which is created by combining four existing vectors of the same length (name, height, nationality, bereft_of_life): # Create numeric, character and logical vectors name &lt;- c(&quot;John&quot;, &quot;Eric&quot;, &quot;Michael&quot;, &quot;Graham&quot;, &quot;Terry&quot;, &quot;Terry&quot;) height &lt;- c(1.96, 1.85, 1.78, 1.88, 1.75, 1.73) nationality &lt;- c(&quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;British&quot;, &quot;American&quot;, &quot;British&quot;) bereft_of_life &lt;- c(FALSE, FALSE, FALSE, TRUE, FALSE, TRUE) # Combines into a data frame df &lt;- data.frame(name, height, nationality, bereft_of_life) df ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 5 Terry 1.75 American FALSE ## 6 Terry 1.73 British TRUE Run the above code in your R console We’ll be working with data frames throughout the practicals, so it will be useful to know that we can access data frame elements in a variety of different ways. We can access individual columns of a data frame using the $ operator, for example: # Extract the &quot;name&quot; column df$name ## [1] &quot;John&quot; &quot;Eric&quot; &quot;Michael&quot; &quot;Graham&quot; &quot;Terry&quot; &quot;Terry&quot; We can also extract using indexing, which requires us to provide the row and columns indexes in the following format: df[row.index, column.index] # Extract the element at row 3, column 4 df[3,4] ## [1] FALSE # Extract rows 1 to 4, all columns df[1:4,] ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE # Extracts all rows, the second and third columns df[,2:3] ## height nationality ## 1 1.96 British ## 2 1.85 British ## 3 1.78 British ## 4 1.88 British ## 5 1.75 American ## 6 1.73 British We can also extract rows and columns based on the values within the data frame, for example using the subset() function. This can be used to select and exclude observations as follows: # Extract data frame rows where height is more than or equal to (&gt;=) 1.8 m. subset(df, height &gt;= 1.8) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 4 Graham 1.88 British TRUE # Extract data frame rows where nationality is not equal to (!=) American subset(df, nationality != &quot;American&quot;) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 4 Graham 1.88 British TRUE ## 6 Terry 1.73 British TRUE # Extract data frame rows where bereft_of_life is equal to (==) FALSE subset(df, bereft_of_life == FALSE) ## name height nationality bereft_of_life ## 1 John 1.96 British FALSE ## 2 Eric 1.85 British FALSE ## 3 Michael 1.78 British FALSE ## 5 Terry 1.75 American FALSE Using the above code as an example, extract all the rows where name is equal to Terry. Assign the result to a new object. You can calculate the number of rows using the function nrow(). Did you get the following answer? ## [1] 2 The sections above have given you a crash course in some of the fundamentals of R. Learning to code (or learning a new code syntax for those of your familiar with Python, MATLAB, Java, …) is similar to learning a new language. Don’t worry if it seems complicated! You can always refer back to these instructions if you get stuck, or ask for guidance. To finish this Chapter, we’ll provide a brief overview of some other key skills, including writing R scripts, loading data and packages, and plotting. This should prepare you for the formative task. This is not an exhaustive list of R functions, and we’ll cover other functionality in Practicals 1 and 2 (e.g. loading GIS raster and vector data, creating maps and figures, performing statistical tests). However, the R functionality outlined here will cover all the steps required for the assessment and should equip you with the core skills required to progress further with R in the future e.g. as the basis for your dissertation. 3.5 Scripts So far we’ve been typing commands into the console. This works as expected; we can print results, store objects in the workspace and create plots in the plots window. However, it is often more convenient to store our code in a script, which is simply a file containing a set of commands and comments. For example, a single R script could contain commands to load data, perform statistical analysis, and output the results to a new file. This can be run in a single keystroke, which is much simpler than typing each command in one after another. To create a new R script: Navigate to File, New File and R Script. This should open a new window, which is an untitled R script: To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. learning_R) One of the easiest ways to run an entire script is to use Ctrl + Shift + Enter. However, you can also run aspects of a script by clicking on the individual line you want to run and using Ctrl + Enter. The same command applies if you highlight (select) multiple lines of code. 3.5.1 Comments When writing code, it is good practice to include comments which describe what the code does. As your code becomes more complex and as time passes, it is very easy to forget what your code does! Commenting your code is especially important if your code is being used by other people or even by yourself weeks/months/years in the future; save yourself time and effort by describing your code fully as you write it. Comments can be written in R using the # symbol. Any line of code which begins with # is ignored by R when the script is run and are used solely to improve the readability of the code. To add a comment, insert the # symbol and write some text. Below is some uncommented code. Copy and paste into your script and add some comments to describe what is taking place. a &lt;- seq(10, 100, 10) b &lt;- a/10 c &lt;- b*a plot(c, a) The above code is our first encounter with plot() which (as you may have guessed), is used to plot objects in base R 1. The new plot you’ve created should now appear in the plots window and should resemble the following figure. There are lots of more exciting ways to plot data in R, some of which are described below. As a general rule, well-presented code would typically comprise: 1/3 code; 1/3 empty space; 1/3 comments. This is known as the Rule of Thirds. 3.6 Loading packages Now that we have our script set up, we need to understand is how to install and load packages in the R environment. To begin, remove all the previous code from your script learning_R.R and save. For the remainder of this chapter, we are going to use just one package as an example, but more will be required for Practicals 1 and 2. The package we are going to use is ggplot2, which is a widely used visualization package for R. If there’s a figure, map or graphic you want to make, it’s likely that ggplot2 can do it for you. It’s now even used by the BBC! BBC graphics created using ggplot2. See here for details. We can install the package using the helpfully named function install.packages() as follows: # Function to install packages. Name of package enclosed by quotation marks. install.packages(&quot;ggplot2&quot;) This function installs the package to your computer, but before we can use it, it needs to be activated within your current R session. This can be achieved by adding it to the library() as follows: # Function to add package to the library (no quotation marks) library(ggplot2) Copy and paste the two code snippets above into your script and run to install and load ggplot2. This is the standard approach to installing and loading packages in R. However, one of the weaknesses of this approach is that R will try and install and load each package every time the script is run. This is unnecessary; the package is already loaded! Try to re-install ggplot2 using the code above. The following pop-up should appear: Press No. The following message will appear in the console: Warning in install.packages : package ‘ggplot2’ is in use and will not be installed We can address this issue by using a user-defined function (called check.packages) to check whether a package is already installed. If it is installed, R will skip the installation. If it is not installed, R will install as normal. This function is similar to the functions we’ve been using before, such as sqrt() or seq(), which are available by default in base R; the difference is it’s been written from scratch! # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) Copy and paste the above code at the start of your R run script, removing the previous install.packages() and library() commands. Re-run the code. In future, you can add new packages by simply adding the package name to the packages vector. This works in exactly the same way as described previously, with the vector created using the c() command. For example, we could install packages as follows: packages &lt;- c(\"ggplot2\", \"dyplr\", \"raster\", \"sf\"). A full list of the available packages is here (&gt;18,000 and counting…). Finally, packages should typically be installed and loaded at the start of any R script. If your code requires a package to run and it has not yet been installed, the code will fail. Installing and loading all the required packages at the start of the script avoids this problem. 3.7 Loading data In the final two sections of this chapter, we will load some data from a external file and plot using ggplot2. In the previous examples, we’ve been creating objects, vectors and data frames from scratch, but most data you will work with will be saved in another file format. This could be a comma-separated file (.csv), tab-delimited (.txt) or in an Excel format (.xls), or could be a spatial data type, such as a raster (.tif, .asc, .img) or vector (.shp). Being able to load and manipulate different data types is a key programming skill. We’ll use one of the most well-used formats as an example; the comma-separated file (.csv). This consists of rows and columns of data, where columns are delineated by commas. For example, here is an example of some comma-separated data: name,age,height,year_of_birth,instrument stewart,70,1.87,1952,drums andy,79,1.70,1942,guitar gordon,70,1.81,1951,vocals When read by R, it is interpreted as follows, splitting columns based on the occurrence of commas, and using the first row as column names: name age height year_of_birth instrument stewart 70 1.87 1952 drums andy 79 1.70 1942 guitar gordon 70 1.81 1951 vocals To load a .csv file into R, we can use the command read.csv(). The most basic way to load a file is to write the absolute file path. This is the location on your computer where the file is saved. For example, a file saved on your C drive, within your Documents folder, and within a folder called Manchester would be formatted as: C:\\Documents\\Manchester To determine the absolute file path, navigate in your file explorer (e.g. Windows Explorer, finder on macOS) to the data sub-directory within GEOG70581: For the final two sections of this Chapter (and the formative task), we’ll using a data file called flow_data.csv which contains the average daily river flow (m3 per second) from the National River Flow Archive and the UK Centre for Ecology &amp; Hydrology. The data are from monitoring station 90003 on the River Nevis, which drains Ben Nevis, the highest mountain in the UK, and cover the period from the 1st October 1982 to the 30th September 2019. You can explore the Glen Nevis catchment below: The file path to your file might read as follows (if your name was Bill): C:\\Users\\Bill\\GEOG-70581\\data\\flow_data.csv To load this file into R: Copy and paste this file path into your script, enclosing the file path within quotation marks. Importantly, R uses forward slashes within file paths /, whereas the file path in your file explorer probably uses backslashes \\, so these will need to be swapped. # Loading data (ensure file path is updated) df &lt;- read.csv(&quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot;) If this has worked successfully, a new object df will have been loaded into the workspace: As you may have noticed, this is not the most elegant way to load data, as it requires us to copy, paste and edit the full file path each time. This would quickly become tiring if we wanted to load multiple data sets. In Practical 1, we’ll guide you through a solution using R projects and the here package but as ever, it is important to understand the basics before moving on to more sophisticated solutions. We can use a variety of commands to inspect this new data set, including: head(df) colnames(df) str(df) Run the above code. What do each of the commands do? head(), colnames(), str() What data types do we have in each column? See here for a reminder. What data structure are we working with? As outlined previously, we can access individual columns of a data frame using the $ operator and could start to do some simple analysis using base R functions, such as: # What is the average daily flow? mean(df$flow) # What is the maximum daily flow? max(df$flow) # What is the minimum daily flow? min(df$flow) Question: What is the range of daily flow measurements? As outlined previously, we can also subset our data based on the values within the data frame, based on columns names ($) or using indexing. To simplify our subsequent analysis, we’re just going to work with data collected in 2019, although we’ll ask you to extract a different year for the formative task. The data frame rows which correspond to 2019 are from the 1st January [13242] to the 30th September [13514]. Extract these rows and all columns using indexing; see here for a reminder if you get stuck. You may also have noticed from the str() command that our column of dates (day/month/year) were stored as characters e.g. $ date: chr \"01/10/1982\". Working with dates and times is a complicated task (see this Tom Scott video for a light introduction to the problem), so to simplify plotting, we’ll convert these characters e.g. \"01/10/1982\" into a date format. This can be achieved using the following code, which overwrites the date column in the data frame: df$date &lt;- and uses the function as.Date() to define a date format. In this case, our numbers are listed in day, month and then year, which is formatted in R as \"%d/%m/%Y\": # Converts dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) Once you’ve extracted the flow data corresponding to 2019, run the above code. 3.8 Plotting In the final task, we’re going to plot these data using ggplot2. One of the challenges of ggplot2 is its steep learning curve, but we’ll walk through this example to show how we can progress from the raw data to high quality figures. These approaches will be useful for your assessment. To finish this chapter, copy and paste the following code elements into your R script in turn and run them. Make sure to read and understand what each step is doing. To begin, we’re going to create an empty ggplot canvas i.e. the blank canvas upon which we can add ggplot elements, such as axes, legends, data, scale bars etc. # Initialising an empty canvas ggplot() Next, we specify which data set we are going to plot. Note: ggplot2 works almost exclusively with data frames: # Which data frame do we want to use? ggplot(data = df) With the data frame defined, we now need to specify which columns of date we wish to plot. The code below uses the function mapping=aes()to do so, where the date variable (within the df object) is assigned to the x-axis x=date and the flow rate variable is assigned to the y-axis y=flow using their column names: # Which data frame columns do we want to plot? ggplot(data = df, mapping=aes(x=date, y=flow)) However, our data are still not visible because we need to select a geom type, which is a complicated way of saying the type of geometric object we want to plot. We could plot our data as points, lines or polygons; we could choose a histogram, bar plot, or box and whiskers plot; there are many to choose from! In this example, we are going to plot our data as points, which is done using the geom_point() command. As you can see from the code below, the structure differs slightly from normal code, in that ggplot elements are separated by the + operator. This allows the user to add each ggplot element on a separate line of code, thereby improving readability. # What type of geom do we want? ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() In the same way, we could also plot the same data as a line, using geom_line(). In this example, both geoms are using the same data frame as inputs x=date, y=flow. This is because the data frame and columns have been specified in the initial call to ggplot here: data = df, mapping=aes(x=date, y=flow). All subsequent geom objects (e.g. points, lines, bar plots…) will use these inputs, unless an alternative is specified. # Adding a line geom ggplot(data = df, mapping=aes(x=date, y=flow)) + geom_point() + geom_line() The above code covers the basics of ggplot, but we can start to improve the design of our plots. For example, we can choose one of the available themes. We could re-order our geoms to show the data points above the line, while adding some colour using HEX codes. We could change the aspect ratio to produce a square plot and maybe add some axis labels: # Specifying data frame and columns ggplot(data = df, mapping=aes(x=date, y=flow)) + # Specifying colours geom_line(colour = &quot;#56ACFF&quot;) + geom_point(colour = &quot;#767676&quot;) + # Adding a ggplot theme theme_classic() + # Setting the aspect ratio theme(aspect.ratio = 1) + # Updating axis labels (including superscript elements) labs(x = &quot;Date (January - October 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) Using the above plot, we could start to identify periods of above or below average flow. Adding a trend line could also be useful to identify patterns in the data. Add a trendline to your plot using the geom_smooth() argument. You can experiment with different smoothing methods e.g. method = “lm”, “glm”, “gam” or “loess”. As a final step, we will probably want to export our figure so that it can be used elsewhere e.g. in the assessment. To do so, we simply assign our ggplot object to an object using the assignment operator: &lt;- as normal: # Assign our ggplot object to the object &quot;g&quot; g &lt;- ggplot(data = df, mapping=aes(x=date, y=flow)) + ... Finally, we can save our ggplot object using the ggsave() function, as shown below. This takes in the name of the plot (g), as well as the intended name, extension and file path e.g. \"C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png\". In this example, we are saving our output within the images sub-directory of GEOG-70581. We have also defined the name of the file flow_data_2019 and specified its file type: .png (portable network graphics file). We can also modify other settings, such as the dpi which determines the plot resolution. A list of ggsave options can be found here. # Saves to a png ggsave(plot = g, &quot;C:/Users/Bill/Documents/GEOG-70581/images/flow_data_2019.png&quot;, dpi = 150) ## Saving 7 x 5 in image Run the above code to assign your plot to the object g and export it to your images folder, making sure to update the file path! We have now exported our plot and should be able to see it in the images sub-directory of GEOG-70581. Importantly, and for future plots, you don’t need to run the commands in the sequence shown above e.g. first creating a canvas ggplot(), then defining a data frame ggplot(data = df), and so on. This sequence was used to show you how we can add elements to the ggplot object, building from the blank canvas to exporting the final figure. 3.9 Formative task Congratulations! You have now completed the guided portion of Chapter 3: the (not-so) brief introduction to R. While this may have been challenging, we have covered most of the core skills which you will need. You should now have an appreciation of the different data types and structures that are available in R, experience running code in the console and within scripts, and the ability to install packages, load data and create plots. We’ll use many of these techniques in Practicals 1 and 2. To finish the practical, please complete the formative task outlined below. Create an R Script which does the following: Installs the required packages. Loads the flow data for Glen Nevis. Extracts all rows corresponding to the years 2000 to 2019, making sure to convert to date format. Finds the average daily river flow (m3 per second) for each month and plots the result. Questions: on average, which month had the highest average daily flow and which had the lowest? Hints and Tips: For general guidance on using ggplot2, see here. To extract the month from each record, you may want to use the months() function from base R, while aggregate() (described here) can be used to compute summary statistics (i.e. the mean) based on groups. 3.9.1 Formative solution A code solution to the formative task will be uploaded here after the class. Note If you’re working on a personal computer with Chinese language settings, you may need to run Sys.setlocale(\"LC_TIME\", \"English\") at the start of your code to fix language-related errors the in the as.Date() function. # Solution to the Chapter 3 formative task # Author: matt-tomkins # Date of upload: 19-11-2024 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;) check.packages(packages) ## ggplot2 ## TRUE # Load data from csv (personal file path - update to your own) df &lt;- read.csv(&quot;C:/Users/44797/Desktop/Repo/GEOG-70581/data/flow_data.csv&quot;) # Convert dates (in character format) to date format df$date &lt;- as.Date(df$date, format = &quot;%d/%m/%Y&quot;) # Extracts post-2000 river flow df_post2000 &lt;- subset(df, date &gt;= as.Date(&quot;2000-01-01&quot;)) # Create a new column, corresponding to the month of measurement, using the months() function df_post2000$month &lt;- months(df_post2000$date) # Summarise by month, using the aggregate() function and &#39;mean&#39; summary &lt;- aggregate(flow ~ month, df_post2000, mean) # Determines plotting order by converting the month column to a &#39;factor&#39; # The factor levels are determined by the R month.name function summary$month &lt;- factor(summary$month, levels = month.name) # Plotting using ggplot2 g &lt;- ggplot(data = summary, mapping=aes(x=month, y=flow)) + # Stat = &quot;identity&quot; is used when the values of x and y are known geom_bar(fill = &quot;#3EBBFB&quot;, stat=&quot;identity&quot;) + # Sets the theme theme_classic() + # Add x and y-axis labels labs(x = &quot;Month (2000 - 2019)&quot;, y = bquote(&#39;Average daily flow&#39;~(m^3~&#39;per second&#39;))) + # Tight fitting y-axis scale_y_continuous(expand = c(0,0)) + # Adjusts angle of x-axis labels theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # Displays figure g # Prints highest and lowest average daily flows print(paste0(&quot;The month with the highest average daily flow is &quot;, summary$month[summary$flow == max(summary$flow)])) ## [1] &quot;The month with the highest average daily flow is January&quot; print(paste0(&quot;The month with the lowest average daily flow is &quot;, summary$month[summary$flow == min(summary$flow)])) ## [1] &quot;The month with the lowest average daily flow is July&quot; Base R is used to refer to the functions that are part of the core packages deployed with R, such as plot(), sqrt() or seq(). As we install and utilise user-created packages, we are moving away from base R.↩︎ "],["FirstPractical.html", "Chapter 4 Eskdale I - Introduction 4.1 Overview", " Chapter 4 Eskdale I - Introduction It should be evident from the lectures that digital elevation models (DEMs) have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions that (1) we can trace the path that water will take from any point in an elevation grid to a basin outlet, and (2) the discharge of water from a particular location is directly related to its catchment area. This is a fairly reasonable assumption in most drainage basins. 4.1 Overview In this lab exercise, we will use a DEM of the Upper Eskdale catchment to model flow-related phenomena, and will have gained experience in: DEM pre-processing; Calculating flow parameters (e.g. pointers and contributing areas); Comparing flow algorithms. Make sure to save your R script regularly! Upper Eskdale catchment 4.1.1 Intended learning outcomes In Chapter 5, we’ll download the data and install the required programs and packages, and will combine these in Chapter 6 to: gain experience with WhiteboxTools, a specialist GIS specifically designed for digital terrain analysis (DTA); become familiar with some of the most commonly used flow algorithms used to simulate overland flow from digital elevation models (DEMs); consider the utility of DTA for routing applications; 4.1.2 Assessment This practical is formative and as such will not count towards your final mark for GEOG 70581, but you are encouraged to complete the practical to the best of your ability as the skills developed here will be used throughout the assessed Practical 2. "],["Eskdale_set_up.html", "Chapter 5 Eskdale II - Set-up 5.1 Install programs 5.2 Download data 5.3 Open RStudio 5.4 Initialise an R project 5.5 Creating an R script 5.6 Install WhiteboxTools 5.7 Loading packages 5.8 Errors using R projects", " Chapter 5 Eskdale II - Set-up 5.1 Install programs If you haven’t installed R and RStudio, please refer to the instructions here before continuing. 5.2 Download data If you’ve not already done so, the data for this practical can be downloaded here. The directory structure is outlined in Chapter 2. 5.3 Open RStudio To begin, open RStudio. 5.4 Initialise an R project In the previous chapter, we typed commands into the console and used R scripts to store commands and comments. For both Practicals 1 (Eskdale) and 2 (Mersey), we are going to expand on these by also utilising R projects. Projects make life simpler by allowing us to access file locations (e.g. for loading and writing data) using relative file paths, rather than using the clunky and prone-to-breaking absolute file paths. Relative file paths work by setting the working directory, which is simply the the default location where R will look for files you want to load and where it will put any files you save. We can do this manually by using the setwd() function, but a simpler approach is to use an R project, which sets the working directory for your automatically. Rather than the complicated absolute file paths: &quot;C:/Users/Bill/Documents/GEOG-70581/data/flow_data.csv&quot; which indicates that the file is located in data, which is within GEOG-70581, which is within Documents, which is within Bill, which is within Users, which is on the C drive, we can set the working directory to GEOG-70581 (for example) and then access the file using the here package: here(&quot;data&quot;, &quot;flow_data.csv&quot;) This approach not only makes it easier to access files, but also improves reproducibility. Using projects ensures that our work is self-contained (the project directory contains all the data, scripts and outputs) and portable. Transferring the project directory to another user, to a different sub-folder on your computer, or a different drive, would not cause any code to fail. By comparison, scripts featuring absolute file paths or a manually defined working directory would likely fail in any of the above scenarios. To initialise a new R project: Go to File, New Project, and select Existing Directory. Using the Browse icon, set the project working directory to the folder GEOG70581, which you downloaded and unzipped here. Select Create new project If this has been successful, your console should have been updated to include the path to your project working directory as follows: 5.5 Creating an R script Now that we have created our project for GEOG70581, we are going to use scripts to store the code for each practical. We’ll create a script for Practical 1 and a separate script for Practical 2. Important: a script can be understood as a set of instructions. As a result, the order of the code is critical. A useful metaphor may be to think of a script as a recipe. For example, a simple cake recipe might require butter, sugar, eggs, flour, milk, vanilla extract and lemon. These could be combined as follows: Pre-heat oven. Whisk butter and sugar. Crack the eggs in one at a time and whisk. Add the lemon zest, vanilla, flour and milk and whisk. Bake in the oven for 30 mins. In code form, it might resemble the following, with a vector including the ingredients and different functions used to combine the ingredients (whisk), turn on the oven (pre_heat) and bake (bake_mixture): # Vector of ingredients ingredients &lt;- c(butter, sugar, eggs, flour, milk, vanilla, lemon) # Pre-heats oven for 20 minutes pre_heat(20) # Create a mixture of butter and sugar using the whisk function, mixture &lt;- whisk(ingredients$butter, ingredients$sugar) # Iterates (loops) over eggs variable for (i in length(ingredients$eggs)){ # Extracts each egg, one at a time current_egg &lt;- ingredients$eggs[i] # Adds current egg to mixture mixture &lt;- whisk(mixture, current_egg) } # Add remaining ingredients mixture &lt;- whisk(mixture, ingredients$flour, ingredients$milk, ingredients$vanilla, ingredients$lemon) # If oven temperature is 180 degrees of more if(oven_temperature &gt;= 180){ # Bake for 30 minutes cake &lt;- bake(mixture, time = 30) } From the text and code example above, it should be clear that our recipe (and code) would not work correctly if we tried to bake in the oven before pre-heating it, or added the ingredients in the wrong order. In exactly the same way, it is important that your code is formatted in a logical order (e.g. Function 1, Function 2, Function 3, …). Ensure that your future code reflects the recipe above, where individual functions utilise variables created previously, and don’t rely on variables created further down in the code. To create an R script for Practical 1: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-1-Eskdale) This should now resemble the following: 5.6 Install WhiteboxTools For Practical 1, we are going to use the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); These packages can be installed as normal using the install.packages() and library() functions. In previous versions of this course, whitebox was not available (because of complicated reasons) on the Comprehensive R Archive Network (CRAN). Instead, it had to be installed using R-Forge. While CRAN is the official package repository for R and generally features “release” versions of packages, R-Forge provides candidate, beta or “work-in-progress” packages or packages which don’t meet the criteria for inclusion on CRAN. However, as of last year, whitebox is now back on CRAN so we can install as normal using the install.packages() function. It is worth noting, however, that you can install packages from R-Forge by simply specifying the repository (web location) where the package is stored, in this case: http://R-Forge.R-project.org. # Installing a package via R-Forge install.packages(&quot;insert package name here&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) Note You don’t need to run the above code - this is just an example of how to install via an alternative repository. One additional step, which is different from “normal” R package installation, is to use the function whitebox::install_whitebox(), which initialises WhiteboxTools: # Initialise WBT whitebox::install_whitebox() This should print the following to your console: Performing one-time download of WhiteboxTools binary from https://jblindsay.github.io/ghrg/WhiteboxTools/WhiteboxTools_win_amd64.zip (This could take a few minutes, please be patient...) WhiteboxTools binary is located at: C:/Users/44797/OneDrive/Documents/R/win-library/4.0/whitebox/WBT/whitebox_tools.exe You can now start using whitebox &gt; library(whitebox) &gt; wbt_version() This code checks if a suitable WhiteboxTools executable is present (.exe) and installs if missing. The WhiteboxTools executable contains all the code used for geospatial analysis, which we then access using R via the whitebox R package. This is a slightly different configuration from normal R packages but enables WhiteboxTools to be used cross-platform i.e. WhiteboxTools functionality stored in the executable can be assessed through R, Python, ArcGIS or QGIS etc, depending on the user needs. Run the below code to install the whitebox package, initialise WhiteboxTools (which should perform a one-time download of the WhiteboxTools executable) and check it is installed correctly. # Install the whitebox R package install.packages(&quot;whitebox&quot;) # Initialise WhiteboxTools whitebox::install_whitebox() # Add the whitebox R package to library library(whitebox) # Check the version is correct wbt_version() When complete, WhiteboxTools has now been successfully installed and initialised and is almost ready to be used. However, to avoid re-installing and re-initialisng WBT every time we run our script, we can either delete the code or comment-out # each line. Remember that R ignores any line of code that begins with #: # Code to install WhiteboxTools # whitebox::install_whitebox() Either delete or comment-out the previous command (install_whitebox()). Ctrl + Shift + C is a useful shortcut for adding or removing comments from multiple lines of highlighted code. 5.7 Loading packages To finish the set-up for Practical 1, we are going to install the remaining required R packages (described above) and load them into the R library. We’ve already practiced installing and loading the ggplot2 package in Chapter 2 so you are ready for the following: Referring back to the instructions here, install and load ggplot2, here, raster, sf, ggspatial and whitebox. Hint 1 - we’ve already installed whitebox, so this can be excluded if you’re using install.packages() and the library() commands. Hint 2 - the user-defined function check.packages might be useful here. If the package has been installed and loaded correctly, the console should print the following: ggplot2 here raster sf whitebox ggspatial TRUE TRUE TRUE TRUE TRUE TRUE where the logical value TRUE equates to a successful installation (and vice versa for FALSE). 5.8 Errors using R projects For the 2023/24 iteration of this course, there were a few issues using whitebox functions. It took a while to solve, but I eventually traced the problem back to how R projects and the here package are working (i.e., not working) on university-managed computers. For 2024/25, these issues should now have been fixed, but if you’re having issues using whitebox functions, or are struggling to access files using here, please see the solution on the Hints page. "],["Eskdale_flow_algorithms.html", "Chapter 6 Eskdale III - Flow routing 6.1 DEM pre-processing: flow enforcement 6.2 Flow Parameters: Pointers 6.3 Flow Algorithms 6.4 Practical solution", " Chapter 6 Eskdale III - Flow routing Overland and near-surface water flow can be modelled using DEMs if we assume that surface topography is the sole factor which influences the distribution of water. One very simple model routes all water from a particular grid cell in a DEM to a single neighbouring cell (i.e. water is not partitioned between multiple neighbours). This ‘D8’ (8 direction) method sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell. Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). Using this method, water flow is allowed in one of eight possible directions (↑ ↗ → ↘ ↓ ↙ ← ↖), assuming that water will travel along the steepest downslope path. In turn, the method is sometimes referred to as the steepest descent method. Based on the \\(3 * 3\\) cell neighbourhood shown in Figure 1, flow would be directed from the centre cell (8 m elevation) to the southwest cell (4 m elevation). Reminder: you don’t have to run every bit of code in this document. Indented text (such as this) is an instruction that tells you something to either think about, or do. 6.1 DEM pre-processing: flow enforcement One common issue encountered when performing hydrological analyses is the presence of sinks, which interrupt the drainage network. When sinks are encountered, flow direction is undefined when a grid cell, or group of grid cells, is lower than all neighbouring cells (see Figure 2). When sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. These features are referred to as pits if they are a single cell in size, and depressions if they consist of groups of cells. Figure 2: Schematic of a sink in a DEM. In this \\(5 * 5\\) matrix, water is routed into the top right cell (9 m) and then is routed to the lowest elevation cell at each step (9 m → 8 m → 7 m → 6 m → 2 m) using the D8 method (blue cells). However, the drainage network is interrupted by a sink at the 2 m cell (orange), as all neighbouring cells are of higher elevation. Sinks can often be artefacts of the data and should be removed during DEM pre-processing. Pre-processing involves altering the elevations of the DEM in a way that enforces continuous flow-paths. However, it is important to realise that sometimes these ‘digital depressions’ reflect actual features in the landscape, and should be preserved during flow modelling. This is a particular issue for hydrological analysis of karst environments, where water can be routed into dolines and fractures (see Figure 3). However, for our work, we will assume that all depressions in DEMs are artefacts and are justified in being removed. Figure 3: An example of a “true” DEM sink. A sinkhole extending into the limestone karst of Twisleton Scar, Yorkshire Dales, with Ingleborough behind (723 m). How should we model water flow in environments with sinkholes, fractures, dolines and extensive sub-surface water flow? Several methods have been developed for removing depressions from DEMs. These methods vary greatly in terms of their sophistication and impact on the DEM. The two most common depression removal methods (Figure 4) are: depression filling, which raises cells within a depression to the elevation of the outlet cell; depression breaching, which digs a trench from a depression’s bottom to some point downslope. Figure 4: Schematic of depression filling and breaching in a DEM. Using the same values from Figure 2, original values are modified to allow water to escape the sink (orange). Depression filling has raised the value of the sink (2 m → 4 m), while depression breaching has lowered the value of a neighbouring cell (3 m → 1 m). In this simplified example, the outputs of these two distinct approaches are identical but care should be taken when working with real world data as they will often produce different results. Not all interruptions to flow routing are caused by depression cells. Often, DEMs contain extensive flat regions (areas of equal elevation). Flat areas interrupt flow routing in the same way as depressions. Cells within a flat region do not have downslope neighbours, and therefore, flow routing is impossible on flat sites without pre-processing. Correction of flow direction on flat sites typically involves finding an outlet cell, forcing flow from cells adjacent to the outlet to the outlet, and continuing backwards in an iterative manner (e.g., Jenson and Domingue (1988)). 6.1.1 Data description The DEM we are working with is centred on the Upper Eskdale catchment; an upland valley which drains the highest mountain in England (Scafell Pike; 978 m), as shown below: Figure 5: Upper Eskdale panorama, viewed from Harter Fell [Location: 54.386907, -3.205004, Elevation: 649 m]. The catchment ranges in elevation from 978 m (Scafell Pike) to ~160 m at the catchment outlet (white circle), and is ringed by numerous summits with elevations in excess of 800 m (white triangles). The catchment has an area of ~15.7 km2 and all water which falls with the catchment ultimately drains to the Irish Sea via the River Esk. You can explore the catchment below: The DEM we are working with was downloaded from EDINA Digimap, has a cell size of 10 m and uses the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Unfortunately, we don’t have time in this course to delve into the exciting world of map projections, although these are covered in great detail in Understanding GIS. However, it is important to know that different map projections have different uses and work more/less effectively in different regions. As we are working within the UK, it makes sense to use a map projection which is tailored to the UK (e.g. BNG) as this minimises different types of map distortion (length, shape, area). When loaded into R using the raster package and plotted using ggplot2 and ggspatial, our DEM can be visualised as follows, where the outlet point (white circle) and summits (black triangles) match those shown in Figure 5. To accentuate areas of relief, the DEM has been combined with a semi-transparent hillshade layer, which is shown below: 6.1.2 Breaching and filling To evaluate the effects of depression breaching and filling, we’re going to use the whitebox package and the wbt_fill_depressions and wbt_breach_depressions functions. One of the advantages of using whitebox is that functions can be applied to spatial data without having to load files into the R environment (and assign them to objects), which can be an issue when working with large files (e.g. high spatial resolution rasters) or when working on computers with limited available memory. We can call whitebox functions by simply naming the function, specifying the names of the input and output layers, and deciding on parameters (if required). For example, the code to fill depressions, might resemble the following: # Fill depressions wbt_fill_depressions(&quot;name of input file goes here&quot;, &quot;name of output file goes here&quot;) Given the structure of our file directory, this could be written as: # Fill depressions wbt_fill_depressions(&quot;C:/Users/Bill/GEOG-70581/data/practical_1/dem_10m.tif&quot;, &quot;C:/Users/Bill/GEOG-70581/output/practical_1/dem_10m_fill.tif&quot;) Here, we’ve used absolute file paths to set the input DEM as dem_10m.tif, stored in data/practical_1. The function wbt_fill_depressions will fill depressions in the input DEM and save the new DEM as dem_10m_fill.tif, stored in output/practical_1. However, because we’re using an R project we can simplify this further. As we’ll be using the DEM (dem_10m.tif) on multiple occasions, we can store its file path using the here package in the variable dem as follows: # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem Copy and paste the above code into your Practical-1-Eskdale script and run. This should print the absolute file path to dem_10m.tif. With this file path stored, we can run the fill depressions function as follows, using the here package to specify the name and intended location of the output file: # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) Run the above code, which should produce a new raster file in the output/practical_1 folder. Repeat this process to also breach depressions, making sure to update the output file name. The difference between the original DEM and the processed DEM(s) may be very subtle and you probably won’t be able to observe the differences directly. As such, it may be useful to identify the specific areas of the raster which have been modified. This can be achieved by using the subtract function to work out the difference between the original DEM and the processed DEM, as follows: # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file Run the above code. To visualise this new raster file, we’re going to load it into the R environment using the raster and here packages and plot with ggplot2 and ggspatial. The raster can be loaded as follows: # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) It can be plotted as follows: # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g Most of the above should be familiar to you, with a few exceptions: Under scale_fill_continuous(), we’ve set limits for the fill symbology. This now ranges between the maximum value, calculated using max(values(dem)), and the minimum value that is greater than 0, calculated using minimum_value &lt;- min(values(dem)[which(values(dem) &gt; 0)]). The latter is useful because most of the DEM was unmodified by wbt_fill_depressions so most raster cells contain a value of 0. The layer spatial() is a non-standard geom type and is available through the ggspatial package. This simplifies the process of plotting raster data in ggplot2, and this approach should be replicated for future plotting. Run the above code elements to reproduce the plot above. You can remove limits from the scale_fill_continuous() argument to plot the actual distribution of DEM values. Question: to what degree has the original DEM been modified by the wbt_fill_depressions function? Consider the spatial extent and magnitude of the observed differences. Note: For this catchment, the difference between DEM breaching and filling is relatively minor. We’ll use the latter (dem_10m_fill.tif) for all subsequent calculations. As outlined above, the areas which are most affected by the depression filling algorithm are partly determined by local relief i.e. relatives differences in elevation. To help visualise this, let’s create and plot a slope image for the area, where the values in the raster cells represent the gradient of the slope, rather than their raw elevation. This can be achieved using the wbt_slope function. Using the examples above, run wbt_slope, using the original DEM as the input, saving the output file with a logical name (e.g. dem_10m_slope.tif) and to the correct sub-directory (output/practical_1). Load the data into R and plot using ggplot2. Your figure should resemble the following: Compare the ‘slope’ and ‘difference’ images. Hint: you can toggle between plots using the left and right arrows in the plot window. Based on your visual inspection the images, which parts of the landscape appear to have been affected by the depression filling algorithm? Why do you think that this is the case? Before we move on to further analysis, it is worth thinking carefully about the symbology we are using for our raster layers. This is important, because not all colour ramps are created equal! Colour ramps differ in terms of their perceptual uniformity and their accessibility for colour-blind individuals. Save your script before continuing. 6.1.3 Choosing a colour ramp Perceptual uniformity refers to how well a colour ramp represents the underlying data. Some (e.g. rainbow; Hawkins (2015)) can introduce artificial boundaries (Alcalde et al. (2017)) which influence how we assess and then use that data (see here). In the schematic below, there are numerous artificial boundaries in the rainbow colour ramp (e.g. dark blue → light blue, light blue → green, green → yellow, yellow → orange), whereas the monochrome colour ramp smoothly transitions from black → white. The viridis colour ramp we have been using so far is perceptually uniform, but others are not. Inspect Figure 6 below. These show the same slope raster, but one is visualised using rainbow, while the other uses a monochrome (perceptually-uniform) colour ramp. Does this influence how you interpret the data? Figure 6: Perceptual uniformity with rainbow and monochrome colour ramps. In a similar fashion, not all colour ramps are suitable for individuals with various forms of colour-blindness (see Levine (2009) and Geissbuehler and Lasser (2013)), who may have difficulty distinguishing reds from greens, or yellows from blues. Luckily, there are now a range of perceptually uniform and colour-blind friendly colour ramps (e.g. Crameri (2018)), including Scientific Colour Maps 6.0. These can be downloaded and used across a range of GIS programs to suit a range of data types. For now, utilise the colour ramps available in R (i.e. viridis) but bear these issues in mind for future map creation. 6.2 Flow Parameters: Pointers Now that we have a DEM that has had all interruptions to flow removed, we can continue to calculate the data needed to define the drainage network i.e. the topological network describing how each cell in a DEM is connected to all others. One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. Whitebox Tools stores D8 pointers as binary numbers in base-2, as follows: We’ll explore the reason for this in Practical 2. In our earlier example of the D8 flow algorithm (see Figure 1), the flow direction was to the SW. This has a binary value of \\(2^4 = 16\\). Therefore, in the pointer file, the highlighted dark blue cell would be assigned a value of 16. Several sub-programs in Whitebox Tools use pointer files to calculate other terrain data, including the algorithms used to calculate catchment area (see below). However, sub-programs rarely require a pointer file as an input, as WBT will automatically generate a pointer layer from the input DEM when required, before discarding the file afterwards. While this processing step is not readily apparent to the user, it is important to realise that it occurs, and it is often good practice to generate a pointer file manually. This can be achieved using the wbt_d8_pointer function. Using the understanding gained above, run the wbt_d8_pointer function, using the correct input files and output file name and directory. Load the raster into R and plot. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. While this is an acceptable plot, it is actually slightly misleading, because the scale bar is continuous, despite the fact that our underlying data are discrete i.e. there are only eight possible values, which are listed below: \\(2^0 = 1\\) \\(2^1 = 2\\) \\(2^2 = 4\\) \\(2^3 = 8\\) \\(2^4 = 16\\) \\(2^5 = 32\\) \\(2^6 = 64\\) \\(2^7 = 128\\) Here is the same data but using a discrete scale: # Manually defined colours based on &quot;RdYlBu&quot; colour palette colours &lt;- c(&quot;1&quot; = &quot;#D73027&quot;, &quot;2&quot; = &quot;#F46D43&quot;, &quot;4&quot; = &quot;#FDAE61&quot;, &quot;8&quot; = &quot;#FEE090&quot;, &quot;16&quot; = &quot;#E0F3F8&quot;, &quot;32&quot; = &quot;#ABD9E9&quot;, &quot;64&quot; = &quot;#74ADD1&quot;, &quot;128&quot; = &quot;#4575B4&quot;) # Plotting p &lt;- ggplot() + layer_spatial(eskdale_pointer, aes(fill = factor(stat(band1)))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_manual(values = colours, na.value = NA) p which is described here: Question: Can you think of any issues with symbolising pointer data? Save your script before continuing. 6.3 Flow Algorithms Once you have determined how each grid cell in a DEM is connected to its neighbours, it is possible to use this pointer information to calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. Measures of catchment area are often used for stream network extraction and other terrain indices. There are three primary measures of catchment area: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment. Unlike the NUE and UCA, SCA is not affected by the grid resolution of the DEM. To calculate the upslope catchment area for each raster cell, we can use the wbt_d8_flow_accumulation function. Run the above function, using the filled DEM as the input file and the output type (out_type) as ‘specific contributing area’ (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. dem_10m_flow_accumulation.tif) and to the correct sub-directory (output/practical_1). Load the raster to R and plot, and your output should resemble the following: Inspect the map output. Question: Which parts of the landscape have low/high values of SCA, respectively? 6.3.1 Comparing flow algorithms So far we have used the most simplistic flow algorithm (D8) to model specific catchment area (SCA). However, there are many flow algorithms available (e.g. Fairfield and Leymarie (1991), Seibert and McGlynn (2007), Grabs et al. (2010)) and each algorithm results in slightly different spatial patterns of water flow. As flow routing is fundamental to the modelling of surface water hydrology, it is important to familiarise yourself with several commonly used methods, although it can be difficult to decide when or where it is appropriate to use a particular routing algorithm. In this section of the practical, we will compare the output of three flow routing algorithms including: D8: see O’Callaghan and Mark (1984) FD8: see Freeman (1991), Quinn et al. (1991) and its modification by Quinn et al. (1995) D-infinity: see Tarboton (1997); Flow algorithms differ in the way in which they: calculate flow direction; model flow divergence, i.e., partition flow between neighbouring downslope cells; handle streams. The D8 algorithm is incapable of modelling flow divergence (or dispersion) because the flow from a particular grid cell is directed to a single downslope neighbour. FD8 (i.e. fractional D8; Freeman (1991), Quinn et al. (1991)) divides flow to each downslope neighbour using a partitioning scheme that is based on the slope angle to neighbouring cells, while a later modification by Quinn et al. (1995) increases the degree of flow convergence from the catchment divide to the channel head. Finally, D-infinity divide flows between a maximum of two downslope neighbours based on the direction of maximum downward slope. Of these, FD8-Quinn (Quinn et al. (1995)) is the only algorithm of the three that explicitly recognises that flow on hillslopes should be divergent while flow along valley bottoms should be convergent. In the final part of the practical, we will assess the specific catchment areas (SCA) derived from each of the flow algorithms discussed above. 6.3.2 FD8 catchment area To calculate the catchment area using the FD8 algorithm (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)), use the wbt_fd8_flow_accumulation function. Use the wbt_fd8_flow_accumulation function, making sure to select ‘specific contributing area’ and remembering to log-transform the values. The exponent parameter should be set to 1.1 (exponent = 1.1), as suggested by Freeman (1991). Save with an appropriate name (e.g. dem_10m_flow_accumulation_fd8.tif). Load the raster in R and plot. 6.3.3 D-infinity catchment area To calculate the catchment area using the D-infinity algorithm of Tarboton (1997): Use wbt_d_inf_flow_accumulation. Use the same input parameters as above (i.e. SCA, log-transformed values). Save with an appropriate name (e.g. dem_10m_flow_accumulation_d_inf.tif). Load the raster in R and plot. 6.3.4 Comparing the outputs The output of the algorithms should resemble the following: Figure 7: Flow algorithm comparison. Results from the D8, FD8 and D-Infinity algorithms. Plot created using ggplot2 and the patchwork package (useful for combining plots). When water flows over a hillslope, you would expect there to be some degree of divergence depending on the topography (i.e. some hillslopes are convergent and some are divergent). However, once water becomes channelised, you would no longer expect flow to be divergent. Once water is in a channel it stays there until it reaches a lake or ocean. Branching in channels is rather rare. So let’s assume that the pattern of SCA should display some degree of dispersion on hillslopes and no dispersion at all along valley bottoms (i.e. in streams). Flow divergence or dispersion is apparent in SCA images by a ‘fuzzy’ quality, almost like the SCA has been smudged. You can zoom into individual plots using the Zoom button in the Plot Window or edit the x- and y-axis limits of your plots by adding the following to your ggplot2 call: # Sets x- and y-axis limits using coord_sf() + coord_sf(ylim=c(504063,506727), xlim = c(320705, 322255)) The above code uses British National Grid coordinates (m), rather than decimal degrees. To finish Practical 1, save your script and complete the following: Compare the D8, FD8 and D-Inf outputs. Either cycle between the plots using the plot window or combine them using the patchwork package. This would need to be installed and loaded to the library. Which algorithms do a good job modelling the pattern of contributing area on hillslopes? Can you see signs of flow divergence along streams in any of the SCA images? Which algorithms perform best for modelling the pattern of SCA in fluvial (stream) environments? Do any of the algorithms do a good job for both hillslopes and streams? Congratulations! You have now completed the Practical 1 (Eskdale; Chapters 4 - 6). You now have considerable experience running whitebox functions as well as visualising spatial data using ggplot2. 6.4 Practical solution As with the introduction to R (Chapter 2 - 3), I will provide a code solution to the Eskdale practical (Chapters 4 - 6), outlining the major analytical steps. This will be released prior to the Week 10 class. # Solution to Practical 1 (Eskdale) # Author: matt-tomkins # Date of upload: 26/11/2024 # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) ## ggplot2 sf here raster whitebox ggspatial patchwork ## TRUE TRUE TRUE TRUE TRUE TRUE TRUE # Initialise whitebox tools executable # whitebox::install_whitebox() # Check whitebox is in the library and the version # library(whitebox) # wbt_version() # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) # Calculates FD8 accumulation file (SCA), with log-transformed values and an exponent of 1.1 wbt_fd8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_fd8.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;, exponent = 1.1) # Calculates D-Infinity accumulation file (SCA), with log-transformed values wbt_d_inf_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_d_inf.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) # Loads accumulation files using raster and here packages eskdale_d8 &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;)) eskdale_fd8 &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_fd8.tif&quot;)) eskdale_d_inf &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation_d_inf.tif&quot;)) # Plots D8 using ggplot2 p_d8 &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_d8, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;D8&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Removes legend, sets title size theme(legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) # Plots FD8 using ggplot2 p_fd8 &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_fd8, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;FD8&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Sets legend location and axis labeling theme(legend.position = &quot;bottom&quot;) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), plot.title = element_text(hjust = 0.5)) # Plots D-Infinity using ggplot2 p_dinf &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(eskdale_d_inf, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;, title=&quot;D-Infinity&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + # Sets fill symbology theme(legend.position = &quot;top&quot;) + # Removes legend and sets axis labeling theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = &quot;none&quot;, plot.title = element_text(hjust = 0.5)) # Combines using the patchwork library combined_plot &lt;- p_d8 + p_fd8 + p_dinf # Displays figure combined_plot References "],["SecondPractical.html", "Chapter 7 Mersey I - Introduction 7.1 Overview", " Chapter 7 Mersey I - Introduction Within a Geographical Information System (GIS) framework, readily available spatial datasets, such as land use and geology, have been used to explore the controls on river water quality. Understanding the relationships between catchment characteristics (or metrics) and river water quality provides a base for determining how future changes in both land use and climate will impact river water quality. Therefore, it is important to determine the processes that regulate river water quality in landscapes under increasing pressure from human population, whether from urbanisation or more intensive food production. The Mersey Basin is one of the UK’s most environmentally varied regions, with rich rural landscapes and urban-industrial centres. Industrial prosperity in the region over the last few hundred years resulted in severe pollution of the many rivers in the basin. Although there has been significant improvements in water quality in recent years (see The Mersey Basin Campaign), the rivers in the Mersey Basin are still subjected to a number of sewage and industrial inputs, and agricultural runoff. You can explore the basin below: 7.1 Overview In this practical you will explore the controls on river water quality in the Mersey Basin and develop empirical models of river water quality across the region. 7.1.1 Topics covered in this practical Terrain analysis, including catchment delineation and derivation of catchment characteristics; Writing and execution of scripts; Multiple linear regression to create models of catchment hydrochemistry; Methods for model evaluation. 7.1.2 Intended Learning Outcomes Gain practical experience of modelling environmental phenomena using a Geographical Information Systems (GIS) framework; Gain practical experience of hydrological modelling using GIS; Gain practical experience of manipulating and analysing raster datasets; Gain practical experience of using Map Algebra functions and writing scripts; Gain practical experience of using multivariate analysis; Understand some of the key controls on river water quality; Be able to critically evaluate model outputs. 7.1.3 Assessment This practical is assessed by completion of the tasks outlined in this document, full details of which are provided on Blackboard. You will need to write up the practical in the format of a report. This practical is worth 50% of your overall mark for GEOG70581. As well as the standard SEED PGT criteria, you will also be assessed on your ability to: Effectively follow instructions to complete the practical tasks (technical prowess), i.e. producing the regression equations and output images correctly; Analyse and interpret the outputs in the context of your knowledge of river hydrochemistry (and with reference to appropriate literature); Relate the monitoring and modelling approaches here to theory covered earlier in the course (and with reference to appropriate literature); Explain ideas clearly and concisely; Present figures and data clearly and effectively; Be original and insightful in terms of independent research and additional analysis of the data where appropriate. Use references to relevant academic literature to support your arguments; The deadline for this practical is 14:00 on Thursday 9th January 2025. The word count for this practical is 1,500. "],["Mersy_set_up.html", "Chapter 8 Mersey II - Set-up 8.1 Practical overview 8.2 Install programs 8.3 Download data 8.4 Data description 8.5 Open RStudio 8.6 Projects and Scripts 8.7 Loading packages", " Chapter 8 Mersey II - Set-up 8.1 Practical overview This practical is comprised of eight primary tasks, with three weeks of class time available (Weeks 10 - 12). Each of the steps is described in more detail in the remainder of this document. An outline of the key tasks is as follows: Mersey III (Chapter 9): To complete in class in Week 10, and finish before the class in Week 11: Task 1: Flow routing Task 2: Seed points Task 3: Watershed delineation Mersey IV (Chapter 10): To complete in class in Week 11, and finish before the class in Week 12: Task 4: Reclassification of categorical datasets Task 5: Calculating surface derivatives Task 6: Extracting surface derivatives Mersey V (Chapter 11): To complete in class in Week 12: Task 7: Model building i.e. relating river water quality to catchment metrics Task 8: Model evaluation Any class time remaining can be used to commence your practical report and ask any questions you may have before submission. 8.2 Install programs You should have already installed R and RStudio. If not, please refer to the instructions here before continuing. 8.3 Download data The data for this practical have already been downloaded here and can be found in data/practical_2. The directory structure is outlined in Chapter 2. 8.4 Data description In Practical 1, we used a single raster file (dem_10m.tif) and visualised and assessed the outputs of a number of WBT functions (e.g. fill, breach, slope, pointer, flow accumulation). In Practical 2, we are going to use a much wider range of input data, all of which use the British National Grid (BNG), a projected coordinate Reference System [EPSG:27700]. Alongside a few supplementary files (which you may wish to use for plotting), the key data are described here: Raster data (.tif): mersey_dem_fill a filled digital elevation model of the Mersey Basin; mersey_rainfall a raster of precipitation values; mersey_bedrock a categorical raster of bedrock geology types; mersey_HOST a categorical raster of soil types (Hydrology of Soil Types); mersey_LC a categorical raster of land cover classes, based on LCM2000 data; Vector data (.shp): mersey_EA_sites a point vector representing the locations of water quality monitoring stations. The attribute table contains a unique Environment Agency ID for each (EA_ID): Tables (.csv): mersey_EA_chemisty a comma-delimited table containing measurements for the following water quality indicators, as well as corresponding Environment Agency IDs: pH: acidity/alkalinity; SSC: suspended solids concentration (mg l−1); Ca: calcium (mg l−1); Mg: magnesium (mg l−1); NH4: ammonium (mg-N l−1); NO3: nitrate (mg-N l−1); NO2: nitrite (mg-N l−1); TON: total oxidised nitrogen (mg-N l−1); PO4: phosphate (mg-P l−1); Zn: zinc (μg l−1). Read through the above descriptions carefully, making sure you understand the data we are using before moving on. We’ll be combining the Environment Agency measurements of water quality (e.g. pH, SSC, …) with spatial data representing the catchment (e.g. topography, rainfall, land cover, …) to investigate the controls on water quality across the Mersey Basin. 8.5 Open RStudio To begin, open RStudio. 8.6 Projects and Scripts 8.6.1 Using an existing R project In Practical 2 (Eskdale), we utilised an R Project called GEOG70581. Your GEOG70581 directory should resemble the following: Here we have sub-directories for the original data (data) and any spatial data files or images we might create (output and images). We can also see the GEOG70581 project file and the Practical-1-Eskdale R script. The former is used to improve file access and to ensure our code is reproducible, self-contained and portable (see here for a refresher). The latter contains all our code and comments relating to Practical 1. In this practical, we don’t need to create a new R project. We will simply use the existing R project as follows: Go to File, Open Project in New Session, and select the GEOG70581 project file. If this has been successful, your console should have been updated to include the path to your project working directory as follows: 8.6.2 Creating an R script As shown in the file directory image above, we already have an R script for the Eskdale practical (Practical-1-Eskdale). As we are now working on a separate practical, with different input data and analytical techniques, it makes sense to create a new script to store the code and comments. To create a new script for Practical 2: Navigate to File, New File and R Script. To save the script: Navigate to File and Save As, and save it in the GEOG70581 folder with an appropriate name (e.g. Practical-2-Mersey) This should now resemble the following: 8.7 Loading packages As we’re working in the same R project from Practical 1, we don’t need to re-install already utilised packages (e.g. whitebox). However, we will need to install some new packages and ensure that all packages are loaded into the R environment. Copy and paste the check.packages function into your new script, either from below or from Practical-1-Eskdale.R: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } In Practical 1, we used the following packages: ggplot2 for data visualisation; here to construct paths to your project files; raster for reading, analysing and writing of raster and vector data; sf for simple storage of vector data; ggspatial for simple plotting of raster data in ggplot2; whitebox for geospatial analysis (a front-end for WhiteboxTools); In Practical 2, we are going to use a number of additional packages: data.table for easy manipulation of tables; dplyr for easy manipulation of data frames; forcats for working with categorical variables (factors); MASS for statistical analysis, based upon Venables and Ripley (2002) “Modern Applied Statistics with S”; units for calculation of measurement units; corrplot and olsrr for correlation and regression analysis. To load new packages, you can either use the install.packages() and library() functions or more simply, add package names to the packages vector, as shown here: # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;ggspatial&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;sf&quot;, &quot;whitebox&quot;, # Practical-1 &quot;data.table&quot;, &quot;dplyr&quot;, &quot;forcats&quot;, &quot;MASS&quot;, &quot;units&quot;) # Practical-2 check.packages(packages) Use the above code to install and load the required packages. "],["mersey_three.html", "Chapter 9 Mersey III - Hydrology 9.1 Task 1: Flow routing 9.2 Task 2: Seed points 9.3 Task 3: Watershed creation", " Chapter 9 Mersey III - Hydrology 9.1 Task 1: Flow routing It should be evident from the lecture and the previous practical that DEMs have a great deal of potential for modelling surface water hydrology and related phenomena in drainage basins. Much of this work is based on the assumptions: We can trace the path that water will take from any point in an elevation grid to a basin outlet; The discharge of water and sediment from a particular location is directly related to its catchment area. However, the first assumption is not met when the flow direction is undefined. This occurs when a single grid cell (pit), or group of grid cells (depression), is lower than all neighbouring cells. When these DEM sinks are encountered, and when there are no downslope neighbours, all water that enters a cell is unable to escape. While these sinks can reflect true features of the landscape, they are often DEM artefacts and can be removed using depression filling or breaching. For more information on DEM sinks and the methods used to remove them, see the previous practical. For this practical, we’ll be working with a DEM where sinks have already been removed, as indicated by the file name (mersey_DEM_fill). While it is not always practical to use file names which include all analytical steps, it is good practice to include the key analytical steps (e.g. _fill, _slope, _hillshade). 9.1.1 Creating a D8 pointer One key input for drainage network calculations is a pointer file, which stores the flow direction of each cell in a raster grid, and thus, the topology of the drainage network. The exact nature of these data depends on the method used to derive the flow directions. One common approach is the D8 algorithm, which sets the flow direction toward the lowest of the eight neighbouring cells, in relation to the centre cell (see Figure 1). Figure 1: Schematic of the D8 method. The elevation value of the centre cell is 8 m (dark blue). Under atmospheric pressure, water flows to areas of lower elevation (&lt; 8 m; light blue) and does not flow to areas of higher elevation (&gt;8 m; grey). In this case, the D8 method would route all the water from the centre cell into the bottom left cell, as it has the lowest value (4 m). We also know from the previous practical that Whitebox stores D8 pointers as binary numbers in base-2 (see Figure 2): Figure 2: Pointer data format in Whitebox Tools. Flow directions (NE, E, SE, S, SW, W, NW, N) are stored as binary numbers in base-2 (\\(2^0, 2^1, 2^2, 2^3, 2^4, 2^5, 2^6, 2^7\\)) Initially, this can seem unnecessarily complex, particularly when compared to a simple sequential storage method (e.g. \\(N = 0, NE = 1, E = 2, ...\\)). However, the advantage of this approach is only apparent when multi-directional flow algorithms are considered such as FD8 (Freeman (1991); Quinn et al. (1991); Quinn et al. (1995)) and D-infinity (Tarboton (1997)). While the D8 algorithm assigns all water flow to a single neighbouring cell, multi-directional algorithms split water flow into two or more neighbouring cells. For these methods, a binary base-2 approach enables this information to be stored efficiently as a single number (see Figure 3). With eight neighbouring cells, there are a total of 255 possible combinations2. Figure 3: Comparison of Whitebox Tools and SAGA GIS flow direction formats. In WBT, all combinations of multi-directional water flow can be expressed as a unique number using base-2. In this example, the value of the centre cell (dark blue) is equal to 44, which is the sum of flow to its W (32), S (8) and SE neighbours (4). In SAGA GIS, the same flow distribution is equal to 13, which is the sum of W (6), S (4) and SE flow (3). However, this same value (13) can also be obtained with flow to the NE, NW and SW (among others!). For this practical, we’ll be calculating the pointer file using the simplest D8 algorithm, as shown in Practical 1. Using your experience from Practical 1, run the wbt_d8_pointer function, using mersey_dem_fill.tif as the input file and saving with an appropriate output name (mersey_dem_D8_pointer.tif) and to the correct directory (output/practical_2). Remember: use the here package for accessing file locations. Load the raster into R using the raster package and plot using ggplot2 and ggspatial. Your output should resemble the following, where each cell value has been reclassified according to the D8 flow. Save your script before continuing. 9.1.2 Catchment area Using this pointer information, we can calculate the number of grid cells that are connected to each cell in an upslope (catchment area) and downslope (dispersal area) direction. These data describe the spatial pattern of upslope (or downslope) area draining to (or away from) each pixel in a DEM. For now, we’ll be calculating the Specific Contributing Area (SCA), which utilises the Upslope Contributing Area (UCA) and Number of Upslope Elements (NUE) metrics: Number of Upslope Elements (NUE): the number of cells draining to each grid cell; Upslope Contributing Area (UCA): the NUE multiplied by the cell area; Specific Contributing Area (SCA): the UCA divided by the cell size or grid resolution. The SCA characterises the amount of flow across a unit length of contour in the catchment and is unaffected by the grid resolution of the DEM. In Practical 1, we used the filled DEM to calculate the SCA for each raster cell using the wbt_d8_flow_accumulation function, but we can also use the D8 pointer raster we just created: Run the wbt_d8_flow_accumulation function, using the D8 pointer raster as the input file (mersey_dem_D8_pointer.tif). Important: because we’re working with a pointer raster, rather than a DEM, we need to specify this parameter (pntr = TRUE). If we don’t set this parameter, WBT will assume the DEM values are actually elevations (ranging from 1 - 128 m) and we will obtain spurious results! Make sure to set the output type (out_type) as ‘specific contributing area’ (in quotation marks). As SCA data typically have a very large range of values, we can also log-transform the flow accumulation values (log = \"TRUE\") to improve plot readability. Save the output file with a logical name (e.g. mersey_dem_flow_accumulation.tif) and to the correct sub-directory (output/practical_2). Load the raster to R and plot, and your output should resemble the following: 9.2 Task 2: Seed points The UK Environment Agency (EA) collects river water quality data at a series of fixed locations across the Mersey Basin. The location of each of these sites is recorded using a GPS and locations have a unique identification number. Seed points are sometimes referred to as “pour points”, and are routinely used to delineate watersheds. The locations of the EA monitoring stations are shown below: 9.2.1 Seed repositioning However, before you can delineate the watershed for each of these outlet points, the seed point location must be checked, repositioned if necessary and given its own unique identifier. This is a very important part of any watershed delineation process. Below is a figure showing the flow accumulation raster (mersey_dem_flow_accumulation.tif) and the EA monitoring stations (mersey_EA_sites.shp). Here, we’re using the raster package to load the raster data, and the st_read() function from the sf package (Simple Features) to load the vector data. To add spatial data to our ggplot map, we are using the annotation_spatial() function which is designed for spatial geometries. Helpfully, this works across the main vector types; points, lines, and polygons can all be added using geom_sf(). For design reasons, we’ve modified the shape type as well as the fill, colour and size of the point data. # Loads flow accumulation raster using the raster and here packages mersey_accumulation &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_flow_accumulation.tif&quot;)) # Loads EA monitoring sites using the st_read function seed_points &lt;- st_read(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites.shp&quot;)) ## Reading layer `mersey_EA_sites&#39; from data source ## `C:\\Users\\44797\\Desktop\\Repo\\GEOG-70581\\data\\practical_2\\mersey_EA_sites.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 70 features and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 345751 ymin: 368724 xmax: 404810 ymax: 427178 ## Projected CRS: OSGB36 / British National Grid # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_accumulation, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = seed_points, shape = 21, fill = &quot;#FFFFFF&quot;, colour = &quot;black&quot;, size = 3) + # Adds vector layer theme_classic() + labs(fill = &quot;Log-transformed flow accumulation value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p Inspect the above code and implement it in your R script. At this map scale, it is quite difficult to assess the correspondence between the accumulation raster and the monitoring stations. To address this, add the following code before your ggplot2 call and update the layer_spatial() argument to include this new raster. Here, we are cropping our flow accumulation raster using indexing, extracting rows 632 - 645 and columns 540 - 578. There are, however, more sophisticated approaches to cropping (e.g. using the extent of other spatial layers) but this is reasonably efficient for now. # Crops the accumulation raster based on a defined extent (row and col indexes) mersey_accumulation_crop &lt;- crop(mersey_accumulation, extent(mersey_accumulation, 632, 645, 540, 578)) As the plot extent is determined by the extent of the feature/layer specified in the layer_spatial() argument, your map should now resemble the following: Inspect the map. You should notice that while some vector points sit directly on raster cells with high accumulation values, others are located just off, often by only one or two grid cells. These errors could be due to imprecise GPS coordinates, the resolution of the DEM, or limitations of the analytical approach (i.e. flow routing), particularly in areas of low lying topography. To ensure accurate watershed creation, the sites that lie off the stream network need to be correctly repositioned. While it is possible to rectify these errors manually i.e. by checking each point in turn and assigning new coordinates if necessary, this approach would be extremely time consuming with large datasets and could introduce unnecessary bias or human error. One common approach to reposition individual points is known as pour point snapping, which relocates points to the nearest stream or area of high accumulation, within a specified threshold distance. See Lindsay et al. (2008) for an overview of snapping approaches. Whitebox Tools provides two primary methods for snapping: Snap Pour Points, using the wbt_snap_pour_points function, which moves each point to the location coincident with the highest flow accumulation value within a specified distance; Jenson Snap Pour Points, using the wbt_jenson_snap_pour_points function, which moves each point the nearest stream grid cell within a specified distance following Jenson (1991). Of these, the Jenson Snap Pour Points approach (Lindsay et al. (2008)) is more sophisticated and has one clear advantage: when an initial outlet point is located on a stream cell, it will not be moved. For our EA dataset, which contains points of variable accuracy, this is beneficial because correctly placed points will retain their original positions, and only those incorrectly placed will be modified. In contrast, a standard snap pour points approach will often move all the points, even those which were located correctly! However, the Jenson approach requires additional inputs and utilises a streams raster file, rather than an SCA accumulation file. To calculate this, the overall process is as follows: Calculate the Number of Upslope Elements (NUE) using the wbt_d8_flow_accumulation function. Define the stream network using the wbt_extract_streams function. Reposition EA seed points using the wbt_jenson_snap_pour_points function. Save your script before continuing. 9.2.1.1 Number of Upslope Elements Run the wbt_d8_flow_accumulation tool, using the original filled DEM as the input file (pntr = FALSE) and saving with an appropriate name mersey_dem_D8_flow_accumulation_NUE.tif and in the correct location. Set the output type as cells (i.e. the number of upslope cells) and don’t log-transform values. 9.2.1.2 Stream network Next, we are going to extract the stream network based on the NUE raster created in Step 1. To extract streams, we first need to set the channelization threshold. This threshold represents the minimum area required to initiate and maintain a channel. Small threshold values will result in more extensive stream networks, while larger threshold values will extract only the largest streams. Unfortunately, there is no widely accepted rule for determining the channelization threshold, which will be unique for each landscape and dataset, and may vary due to raster resolution. As a result, experimentation is required! For our dataset, we want to ensure that each EA measurement site is located on or near a digitised stream, so setting a large channelization threshold and focusing on the largest streams might not be appropriate, as some measurement sites are located on minor streams in small catchments. As such, we need to set the channelization threshold sufficiently low to mitigate against this. Feel free to experiment with this threshold and evaluate its effects on the delineated stream network. For our analysis, a channelization threshold of 200 upslope grid cells would be appropriate. Using the output of Step 1, we are going to extract the stream network using wbt_extract_streams with a channelization threshold of 200 upslope grid cells (threshold = 200). Set the background value to 0 (zero_background = TRUE). As usual, save with an appropriate name (e.g. mersey_dem_streams_act200.tif) and in the correct location. Your output should resemble the following, whether raster cells corresponding to streams are coded as “1” and non-stream regions are coded as “0”: 9.2.1.3 Snapping Finally, we are going to reposition the EA seed points to match the stream network. Remember, seed points already located on the steam network will not be moved. Before we execute the tool, we need to decide on the Snap Distance, which is measured in the units of the input data and is the maximum distance that points can be moved to match the stream network. Our raster is measured in metres (m) with a grid resolution of 50 m2. Most points are incorrectly positioned by one to three grid cells (50 - 150 m). However, to ensure we include all points, we’ll set the snap distance to 500 m. Using the output of Step 2 (mersey_dem_streams_act200.tif) and the EA monitoring stations shapefile mersey_EA_sites.shp, run the wbt_jenson_snap_pour_points function, using a snap distance of 500 m (snap_dist = 500). Save with an appropriate name mersey_EA_sites_snapped.shp. As we’re using two inputs to a Whitebox function (rather than just one), it can be useful to specify inputs directly in R e.g. streams = mersey_dem_streams_act200.tif, pour_pts = mersey_EA_sites.shp, output = .... You can check the names of the inputs by reading the documentation, by hovering the mouse over the name of the function, or pressing the tab button. Run the following code to inspect the outputs of Jenson Snap Pour Points. The stream network is a Boolean (logical) raster, where stream cells = 1 (yellow) and non-stream cells = 0 (blue). The original pour points are shown in white, with the snapped points in red. # Loads streams raster using the raster and here packages mersey_streams &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_streams_act200.tif&quot;)) # Crops the streams raster based on a defined extent (row and col indexes) mersey_streams_crop &lt;- crop(mersey_streams, extent(mersey_streams, 632, 645, 540, 578)) # Loads original and snapped EA monitoring sites using the st_read function seed_points &lt;- st_read(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites.shp&quot;)) snapped_seed_points &lt;- st_read(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_EA_sites_snapped.shp&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_streams_crop, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = seed_points, shape = 21, fill = &quot;#FFFFFF&quot;, colour = &quot;black&quot;, size = 3) + # Adds seeds layer annotation_spatial(data = snapped_seed_points, shape = 21, fill = &quot;#FB5858&quot;, colour = &quot;black&quot;, size = 3) + # Adds snapped seeds layer theme_classic() + labs(fill = &quot;Stream network ID&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p 9.3 Task 3: Watershed creation To finish the Week 10 class (Chapter 9), we are going to combine the previous outputs to create unique (non-overlapping) watersheds for each EA seed point. Use the wbt_watershed function with the D8 pointer raster (mersey_dem_D8_pointer.tif) and the snapped pour points vector (mersey_EA_sites_snapped.shp) as inputs. Save in the output/practical_2 directory as mersey_watersheds.tif. Your output should resemble the following: # Loads streams raster using the raster and here packages mersey_watersheds &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.tif&quot;)) # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_watersheds, aes(fill = stat(band1))) + # Adds raster layer annotation_spatial(data = snapped_seed_points, shape = 21, fill = &quot;#FB5858&quot;, colour = &quot;black&quot;, size = 3) + # Adds snapped seeds layer theme_classic() + labs(fill = &quot;Stream network&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) + theme(legend.position = &quot;top&quot;) p The watersheds are currently stored as rasters, but it is often easier to work with vectors, especially when dealing with attributes and spatial relationships (topology). Use the following code to convert your watersheds raster (.tif) to vector format (.shp) # Converts watershed to vector format (polygon) wbt_raster_to_vector_polygons(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.tif&quot;), here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_watersheds.shp&quot;)) These delineated watersheds can now be used for further analysis. However, it is important that we carefully assess them for any errors. This typically requires in-depth knowledge to assess whether the correct watershed(s) has been extracted. In turn, the position of seed points is critical, as placing seeds points above/below stream confluences can have a major impact on the extracted watershed(s), as shown in Figure 4: Figure 4: The impact of seed point location on extracted watersheds. (A) In this example, rivers A and B are sourced from distinct watersheds but meet at a confluence (white circle). (B) When seeds points are placed above this confluence, each river has its own individual watershed. (C) When a seed point is placed at or below the confluence, these watersheds are combined as all water falling within both will ultimately drain to the confluence. Prior knowledge of the catchment is required to know if the correct watershed has been extracted. To finish, save your script and compare your outputs to those shown below. If there are any clear differences in your version, repeat the previous tasks and play close attention to the input settings. References "],["Hints.html", "Chapter 10 Handy Hints", " Chapter 10 Handy Hints This chapter contains a few handy hints which may be useful if you’re having any issues loading R packages or running Whitebox tools. Dealing with errors Errors are an inevitable aspect of coding, irrespective of how experienced you are! As we gain programming experience, these errors increase in difficulty from initially simple problems, such as incorrect file paths or errors in code syntax, to the more complex. Being able to deal with errors is therefore an essential programming skill. My top tips for fixing errors are: Read the error message (red text in the R console). If this appears, it usually means that something has gone wrong! Error messages can vary in quality, but at the very least they should tell you: where the error was encountered i.e. which line of code failed. some indication as to why the error occurred e.g. No such file or directory If the error message doesn’t provide a simple solution to the problem, then the next step should be to work through your code step-by-step, testing each line or code block to ensure it is working correctly (usually using print() statements). To illustrate this, here is some code which has failed: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;) check.packages(packages) # Fill depressions in a DEM wbt_fill_depressions(dem = here(&quot;data&quot;, &quot;practical_2&quot;, &quot;dem_10m.tif&quot;), output = here(&quot;output&quot;, &quot;practical_2&quot;, &quot;dem_10m_fill.tif&quot;)) # Loads raster filled_dem &lt;- raster(here(&quot;data&quot;, &quot;practical_2&quot;, &quot;dem_10m_fill.tif&quot;)) # Plots using ggplot g &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(filed_dem, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Elevation (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot; + theme(legend.position = &quot;top&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot; na.value = NA) # Shows plot g I would first inspect the error message (No such file or directory) but if I didn’t understand it, I would then troubleshoot by: First running the check.packages(packages) function. Were all the packages installed correctly i.e. TRUE? Then checking the wbt_fill_depressions() function. Does this produce an error? Does it correctly produce an output raster in the intended location? Does the output look sensible? Plotting the raster might be useful here to check. Then running the raster() function to load the raster into R. Here the code would fail and I could then start to investigate further: Are there any syntax errors? e.g. use of commas, correct number of brackets, spelling errors? Are the inputs defined correctly? In the above example, the code fails because the raster package is trying to load dem_10m_fill.tif from data/practical_1 when it was actually saved in output/practical_1. Running the code again might produce another error, in which case I would repeat the above process, checking the code step-by-step until the error can be located and fixed. Can you spot any errors in the code below? There are three to find… # Plots using ggplot g &lt;- ggplot() + # Adds raster layer using layer_spatial layer_spatial(filed_dem, aes(fill = after_stat(band1))) + # Sets ggplot theme theme_classic() + # Axis and legend labeling labs(fill = &quot;Elevation (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot; + theme(legend.position = &quot;top&quot;) + # Sets fill symbology scale_fill_continuous(type = &quot;viridis&quot; na.value = NA) TLDR: the best way to fix errors is to be systematic in your approach. Work your way through the code step-by-step, checking it is producing the intended outputs at each stage - just because the code doesn’t produce an error, it doesn’t mean it is producing the output that you want! Whitebox functions vs. the raster package So far, there has been some confusion regarding the role of whitebox functions, the raster package and ggplot2. Each of these has a unique role, as visualised below, while analysis needs to be conducted in the correct order to avoid errors. Analysis structure. [1] Create output files using whitebox [2] Load outputs into R using raster [3] Plot them using ggplot To illustrate this, below we have an example of the wbt_d8_pointer function, which creates a D8 pointer file based on an input digital elevation model (dem): # Creates a D8 pointer file (.tif) at the output location, based on the input dem wbt_d8_pointer(dem = here(&quot;data&quot;, &quot;practical_2&quot;, &quot;mersey_dem_fill.tif&quot;), output = here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_D8_pointer.tif&quot;)) This uses the here package for locating the input file (mersey_dem_fill.tif), which is stored in data/practical_2, and for saving the output, which is stored as mersey_dem_D8_pointer.tif in output/practical_2. If this runs successfully (make sure to carefully check file paths, file names, use of commas), then an output raster will be created at the specified location. Crucially, this does not load the raster into the R environment, so any attempt to plot the raster using ggplot2 would fail. To achieve this, a raster must be first loaded into the R environment as follows: # Loads D8 pointer raster using the raster and here packages mersey_pointer &lt;- raster(here(&quot;output&quot;, &quot;practical_2&quot;, &quot;mersey_dem_D8_pointer.tif&quot;)) Again, we have determined the input file location using here package (output/practical_2/mersey_dem_D8_pointer.tif) and stored the raster as an object in the R environment called mersey_pointer. With this stored, it can then be plotted using ggplot, setting layer_spatial to the mersey_pointer object name: # Plots using ggplot p &lt;- ggplot() + layer_spatial(mersey_pointer, aes(fill = stat(band1))) + theme_classic() + labs(fill = &quot;Pointer value&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA) p TLDR: for subsequent analysis, make sure to adhere to the following structure: Create output files using whitebox functions. Load those output files into R using the raster package (stored as objects) Plot the R objects using ggplot2 Any deviation from this order (e.g. plotting without loading using raster, loading files that have not yet been created using whitebox) will cause errors! R Projects Make sure to initialise an R project for your work (Section 5.4) and ensure this is created for your GEOG70581 directory. This should resemble the following, with the R project file and a separate R script for the Eskdale and Mersey Basin practicals: File paths Remove spaces in directories or file paths. As a rule, avoid using spaces when naming files or folders as this can cause issues, particularly when coding: “P:/Environmental Monitoring Concepts/GEOG70581/…” Bad “P:/Environmental_Monitoring_Concepts/GEOG70581/…” Good Output files Make sure to inspect output files. When you run a command (e.g. a Whitebox function), check that it produces the intended output (e.g. a raster file (.tif) in the output directory). If there is an error or the output hasn’t been created, subsequent code will fail. Packages If you’re having difficulty loading the raster or sf packages on a University managed computer, this is due to multiple package installs. Go to the packages window, identify the older version of each package, and tick the check box to load it into your R library. Code structure While you will not be assessed on the quality of your code (you do not have to submit your code for the assessment), remember that well-formatted code (with comments and good structure) is easier to read and understand and will be less prone to error. Inspect the code below: # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;sf&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;ggspatial&quot;, &quot;patchwork&quot;) check.packages(packages) # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Breach and fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) wbt_breach_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_breach.tif&quot;)) # Calculates D8 pointer wbt_d8_pointer(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_D8_pointer.tif&quot;)) # Calculates D8 accumulation file (SCA), with log-transformed values wbt_d8_flow_accumulation(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_flow_accumulation.tif&quot;), out_type = &quot;specific contributing area&quot;, log = &quot;TRUE&quot;) This code includes comments for each main code block, line spaces to distinguish different parts of the code, and is written in a logical order (e.g. first loading packages, then loading/selecting data, running tools). Any non-essential code has been removed e.g. additional calls to install.packages() or library(). Errors with R projects and the here package If you’re working on a university-managed computer, you may have encountered errors while trying to use whitebox functions, such as wbt_fill_depressions(), or accessing files using relative file paths via the here package. This problem actually originates with our use of R projects (described here). R projects are designed to make accessing files simple and to ensure our analyses are self-contained and portable. They do this by automatically setting the working directory in RStudio. You can test this by using the getwd() function, which should print your working directory to the console. If the R project is working, this should set your working directory to the directory where your R project is saved e.g., an R project file stored in C:/Users/Bob/Documents/GEOG70581 should have an identical working directory. Unfortunately, R projects were not working as intended on university-managed computers in 2023/24: the working directory was not being automatically updated. In most cases, the working directory was stuck at P:/, even if the R project is saved elsewhere! This was a completely new problem for 2023/24, but it is possibly due to changes in the waycomputers are managed after the recent cyber incident. This can lead to difficulties accessing files using here, but also utilising whitebox functions, even when inelegant absolute file paths are used. In both cases, R is looking in the wrong place, either for the data files, or the WBT executable. This issue should be solved for 2024/25, but if it arises again, there are two options to fix this: The simplest approach is to start from scratch, downloading the course data, and creating a new R project in the same directory. If successful, it should resemble the following: The next step is to set the working directory manually using the setwd() function (see below for an example). This should update the working directory to the correct location, such as C:/Users/Bob/Documents/GEOG70581 or P:/GEOG70581 depending on where you data are saved, and should be done before installing packages. The practical steps can then be following as normal and the whitebox functions should work as intended. Try running the code below, making sure to update the path to the correct working directory. # FIRST, set your working directory manually setwd(---YOUR PATH GOES HERE---) # e.g., &quot;C:/Users/Bob/Documents/GEOG70581&quot; # Function to check and install packages check.packages &lt;- function(pkg){ new.pkg &lt;- pkg[!(pkg %in% installed.packages()[, &quot;Package&quot;])] if (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE) sapply(pkg, require, character.only = TRUE) } # Checks and installs packages packages &lt;- c(&quot;ggplot2&quot;, &quot;here&quot;, &quot;raster&quot;, &quot;whitebox&quot;, &quot;sf&quot;, &quot;ggspatial&quot;) check.packages(packages) # Initialise whitebox tools executable whitebox::install_whitebox() # Check whitebox is in the library and the version library(whitebox) wbt_version() # Sets file path for DEM dem &lt;- here(&quot;data&quot;, &quot;practical_1&quot;, &quot;dem_10m.tif&quot;) # Prints the file path to the console window dem # Fills depressions wbt_fill_depressions(dem, here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;)) # Subtract function; the difference between the processed DEM (filled) and the original DEM wbt_subtract(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill.tif&quot;), # Input file 1 dem, # Input file 2 here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Output file # Loads raster using the raster and here packages eskdale_dem &lt;- raster(here(&quot;output&quot;, &quot;practical_1&quot;, &quot;dem_10m_fill_difference.tif&quot;)) # Find minimum and maximum values minimum_value &lt;- min(values(eskdale_dem)[which(values(eskdale_dem) &gt; 0)]) maximum_value &lt;- max(values(eskdale_dem)) # Plots using ggplot g &lt;- ggplot() + layer_spatial(eskdale_dem, aes(fill = after_stat(band1))) + theme_classic() + labs(fill = &quot;Elevation difference (m)&quot;, x = &quot;Easting&quot;, y = &quot;Northing&quot;) + scale_fill_continuous(type = &quot;viridis&quot;, na.value = NA, limits = c(minimum_value, maximum_value)) g If you want to continue with an existing directory/R Project, then you will need to set the working directory manually (setwd()) and set the origin for the here package, as shown below: # Set the working directory manually setwd(&quot;C:/Users/Bob/Documents/GEOG70581&quot;) # UPDATE TO YOUR DIRECTORY # Manually set the origin for the &#39;here&#39; package here::i_am(&quot;practical-1-eskdale.R&quot;) # UPDATE IF A DIFFERENT FILE NAME USED The above solutions have been tested on a range of university-managed computers, working on both the P: and C: drive. For other installation issues, please contact me on my @manchester email. For other R issues, please post on the forum. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
