# Mersey IV - Statistical analysis {#mersey_four}

## Task 4: Extracting catchment characteristics

We know from the lecture that river hydrochemistry can be influenced by a range of catchment characteristics, including land cover, soil type, bedrock geology, topography, and rainfall. Before we undertake statistical analysis, our next step is to extract these characteristics for each of the 70 catchments in the `mersey_watersheds` file (`.shp`), so we can relate these to the Environment Agency water quality data collected at each of the 70 monitoring sites.

Information on these characteristics for the Mersey region can be obtained from the relevant files in the data pack, including:

-   `mersey_LC` [`categorical`], based on LCM2000 data;
-   `mersey_HOST` [`categorical`] i.e., Hydrology of Soil Types;
-   `mersey_bedrock` [`categorical`], sourced from the British Geological Survey;
-   `mersey_dem_fill` [`continuous`], a digital elevation model (50 m spatial resolution);
-   `mersey_rainfall` [`continuous`], annual average rainfall (mm a^-1^).

To simplify our analysis, I am going to walk you through the approach to calculate *some* of these variables, including the average of the continuous datasets (elevation, slope, aspect, rainfall).

For the categorical datasets (e.g., percentage of the each of the land cover / soil / bedrock classes present), I have provided you with the summary values, stored in `data/practical_2/mersey_EA_characteristics.csv`.

While extracting these values is an important part of the data cleaning-preparation process, it is *time-consuming*, requiring reclassification of each categorical raster (e.g., simplifying the number of classes and removing classes that are not present in the study area) and normalisation of the values based on catchment area. Given the focus of EMMC, this part has been removed to give you more time to focus on the **statistical analysis and interpretation** (as requested by previous student feedback), and to relate your results to key environmental **concepts**.

If you are interested in the reclassification approach, I have provided some guidance in the final chapter [here](#Hints).

### Calculating surface derivatives

Before we can extract characteristics for each watershed, we need to produce some additional layers. Here, we are interested in the effects of elevation (the data from `mersey_DEM_fill`) and rainfall (`mersey_rainfall`), as well as topographic **slope** and **aspect**, both of which likely influence the path of water through the landscape. These two attributes (*slope, aspect*) are known as **surface derivatives** as they are calculated (derived) from the DEM.  

To produce **slope** and **aspect** rasters: 

> use the `wbt_slope` and `wbt_aspect` functions, using the original filled DEM as the input data (`mersey_dem_fill.tif`) and using appropriate output names (e.g. `mersey_dem_slope` and `mersey_dem_aspect`). 

Your outputs should resemble the following: 

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Sets file path for DEM
dem <- here("data", "practical_2", "mersey_dem_fill.tif")

# Slope and aspect
wbt_slope(dem, here("output", "practical_2", "mersey_dem_slope.tif")) 
wbt_aspect(dem, here("output", "practical_2", "mersey_dem_aspect.tif")) 

# Loads slope and aspect rasters
mersey_slope <- raster(here("output", "practical_2", "mersey_dem_slope.tif"))
mersey_aspect <- raster(here("output", "practical_2", "mersey_dem_aspect.tif"))

# Creates ggplot for slope
slope <- ggplot() +
  layer_spatial(mersey_slope, aes(fill = stat(band1))) + # Adds raster layer
  #annotation_spatial(data = snapped_seed_points, shape = 21, fill = "#FB5858", colour = "black", size = 3) + # Adds snapped seeds layer
  theme_classic() + 
  labs(fill = "Slope angle", x = "Easting", y = "Northing") +
  scale_fill_continuous(type = "viridis",  na.value = NA) +
  #scale_fill_distiller(palette = "RdYlBu", na.value = NA) +
  theme(legend.position = "top")

# Creates ggplot for aspect
aspect <- ggplot() +
  layer_spatial(mersey_aspect, aes(fill = stat(band1))) + # Adds raster layer
  #annotation_spatial(data = snapped_seed_points, shape = 21, fill = "#FB5858", colour = "black", size = 3) + # Adds snapped seeds layer
  theme_classic() + 
  labs(fill = "Aspect", x = "Easting", y = "Northing") +
  #scale_fill_continuous(type = "viridis",  na.value = NA) +
  scale_fill_distiller(palette = "RdYlBu", na.value = NA) +
  theme(legend.position = "top")


slope+aspect
```

### Extracting continuous characteristics

To begin:

> Load the `mersey_watersheds.shp` file into R (produced in Task 3) using the `st_read` function, storing in a variable called `watersheds`.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Load watersheds (vector, sf, VALUE column is important) and renames
watersheds <- st_read(here("output", "practical_2", "mersey_watersheds.shp"))
```

> Next, print out attribute names for the shapefile as follows:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
colnames(watersheds)
```

For our analysis, the attribute of interest is `VALUE`, which contains the unique Environment Agency ID for each watershed. **Importantly**, this is also found in `mersey_EA_chemisty.csv` file. This will enable us to join the two datasets, populating the attribute table of the `watersheds` variable with the water quality measurements stored in the csv.

> To simplify this, use the following code to replace the column name `VALUE` with a new name `Seed_Point_ID`. The latter is used in the `mersey_EA_chemisty.csv`.

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Replaces column name 'VALUE' with 'SEED_Point_ID'
names(watersheds)[names(watersheds) == 'VALUE'] <- 'Seed_Point_ID'
```

> You can re-use the `colnames` function to check it worked correctly:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
colnames(watersheds)
```

With this updated:

> We can now load the Environment Agency data using `read.csv()`, as shown in [Chapter 3](#Intro_to_R):

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}

# Loads csv using read.csv
ea_data <- read.csv(here("data", "practical_2", "mersey_EA_chemistry.csv"))
```

> and **merge** using the `merge` function:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Merge based upon matching Seed_Point_IDs
watersheds_ea <- merge(watersheds, ea_data, by = "Seed_Point_ID")
```

> Use the `head()` function to inspect the first few rows of our new data frame.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
head(watersheds_ea)
```

> Save your script before continuing.

Before we move on to extract our continuous derivatives (average elevation, rainfall, slope and aspect), it is worth noting that R variables can be removed from the environment as follows:

```{r, echo = TRUE, eval = FALSE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Removes object(s) from memory
rm(mersey_dem)
```

This can be useful if R is running slowly.

To extract continuous derivatives, we are going to use the `extract` function from the **raster** package, which is described [here](https://www.rdocumentation.org/packages/raster/versions/3.5-2/topics/extract). We'll be using this function for each continuous dataset. It is also used for extracting categorical derivatives, as demonstrated [here](#Hints), so it's important that you understand what it's doing.

Broadly, the function extracts values from a raster object at the locations of spatial vector data, where the value of interest is user-defined. For example, this could be the mean (e.g. the average elevation of a DEM within a vector polygon), the count (e.g. the number of cells within a vector polygon), or a minimum or maximum (e.g. the maximum elevation within a vector polygon), as shown in the figure below:

<p align="center">
<a name="Figure_7"></a>![](figures/Practical-2/extract-raster.png){width=50%}
</p>

**Figure 5: Visual example of the `extract` function**. In this example, our raster layer is the slope of the terrain in the Eskdale catchment, studied in [Week 9](#Eskdale_flow_algorithms), with values ranging from 0° (flat) to 76° (very steep). Our vector layer is a series of three $Polygons$, covering three upland locations (other valid vector types include $Points$ and $Lines$). Here, the `extract` function would return the average and minimum-maximum slope for $Polygons$ $A$ and $B$ respectively, and the count (number of raster cells) for $Polygon$ $C$.  

<br/> 

To use the `extract` function, the raster dataset needs to be loaded into R.

> Load the digital elevation model into R and store with a sensible variable name e.g. `mersey_dem`:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Load elevation raster
mersey_dem <- raster(here("data", "practical_2", "mersey_dem_fill.tif"))
```

> Now we can use the `extract` function as follows. This takes in both raster (`mersey_dem`) and vector input data (`watersheds_ea`), where the value of interest is determined by the `fun` parameter (i.e. a function). Here we are using an existing base R function (e.g. mean) but we can use user-defined functions as well. For example, `fun=function(x, ...) length(x)` would **count** the length (or number) of raster cells for each watershed, and `min` or `max` would return the minimum and maximum elevations, respectively. In our case, we just want to calculate the mean (average) elevation, so we can use the approach below:

```{r, echo = TRUE, eval = FALSE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Extracts raster values for each watershed, calculates mean (fun=mean), stores in attribute table ($average_elevation), with NA values removed (na.rm=TRUE)
watersheds_ea$average_elevation <- extract(mersey_dem, watersheds_ea, fun=mean, na.rm=TRUE)
```

> When you're happy you understand the process, load the other continuous rasters into R with sensible variable names (e.g. `mersey_rainfall`, `mersey_slope`, `mersey_aspect`) and use the `extract` function to calculate the relevant attributes e.g. `$average_rainfall`, `$average_slope`, `$average_aspect`.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide'}
# Load raster data
mersey_dem <- raster(here("data", "practical_2", "mersey_dem_fill.tif"))
mersey_rainfall <- raster(here("data", "practical_2", "mersey_rainfall.tif"))
mersey_slope <- raster(here("output", "practical_2", "mersey_dem_slope.tif"))
mersey_aspect <- raster(here("output", "practical_2", "mersey_dem_aspect.tif"))
```

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide',  cache = FALSE}
# Extract derivatives, calculate mean, and store in attribute table
watersheds_ea$average_elevation <- extract(mersey_dem, watersheds_ea, fun=mean, na.rm=TRUE)
watersheds_ea$average_rainfall  <- extract(mersey_rainfall, watersheds_ea, fun=mean, na.rm=TRUE)
watersheds_ea$average_slope <- extract(mersey_slope, watersheds_ea, fun=mean, na.rm=TRUE)
watersheds_ea$average_aspect  <- extract(mersey_aspect, watersheds_ea, fun=mean, na.rm=TRUE)

# Removes object(s) from memory
#rm(dem, rainfall, slope, aspect)

```

> Use the `head()` function to inspect the output.

### Extracting categorical characteristics

As noted above, I have produced some catchments characteristics for you, derived from `mersey_LC`, `mersey_HOST` and `mersey_bedrock`. If you want to learn about the approach, it is detailed [here](#Hints). This information is stored in `mersey_EA_characteristics.csv` and includes:

-   Percentage of the each of the five land cover classes present:
    -   Arable %
    -   Heath %
    -   Grassland %
    -   Urban %
    -   Wetland %
-   Percentage of the each of the four soil types present:
    -   Permeable %
    -   Impermeable %
    -   Gleyed %
    -   Peats %
-   Percentage of the each of the three bedrock geology types present:
    -   Sands_and_Muds %
    -   Limestone %
    -   Coal %

> Load the above dataset using `read.csv()` and then **merge** this dataset with `watersheds_ea` (containing the continuous characteristics calculated above) using the `merge` function and `Seed_Point_ID` as the merge value.

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', results='hide',  cache = FALSE}

# Load characteristics using read.csv
watershed_char <- read.csv(here("data", "practical_2", "mersey_EA_characteristics.csv"))

# Merge based upon matching Seed_Point_IDs
watersheds_ea <- merge(watersheds_ea, watershed_char, by = "Seed_Point_ID")

```

> Again, use the `head()` function to check this worked as expected.

<br/> 

**To finish Task 4:**

> Run the following code, which removes the geometry stored in the data frame (not required for subsequent analysis) and saves our dataframe as a comma-separated file.

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Drops geometry attribute from watersheds_ea
watersheds_ea <- st_drop_geometry(watersheds_ea)

# Writes data frame to comma-separated file
write.csv(x = watersheds_ea, here("output", "practical_2", "mersey_watersheds_ea.csv"), row.names=FALSE)

```

> If you wish, it might be good idea to clean / simplify our R environment before commencing the statistical analysis. To do so, best practice is to restart R (`Ctrl + Shift + F10` on Windows) *or* you can use the following code `rm(list = ls())` to remove all objects from the workspace. While the latter isn't a perfect solution (see [here](https://rstats.wtf/source-and-blank-slates#sec-rm-list-ls)), it can be of value if used prudently.

```{r, echo = TRUE, eval = FALSE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Delete user-created objects from the global workspace
rm(list = ls())
```

## Task 5: Model building

In this next task, we will compare the information about catchment characteristics with the water quality data collected at each of the 70 monitoring stations. To begin, load the csv file created at the end of Task 4 (`mersey_watersheds_ea.csv`), saving to a new variable called `watersheds_df`:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Reads completed file from csv
watersheds_df <- read.csv(here("output", "practical_2", "mersey_watersheds_ea.csv"))

```

If you have any other variables in your R environment, these can be removed using `rm()`.

This data frame should contain the following 10 water quality indicators for each watershed:

-   pH: acidity/alkalinity;
-   SSC: suspended solids concentration (mg l<sup>−1</sup>);
-   Ca: calcium (mg l<sup>−1</sup>);
-   Mg: magnesium (mg l<sup>−1</sup>);
-   NH<sub>4</sub>: ammonium (mg-N l<sup>−1</sup>);
-   NO<sub>3</sub>: nitrate (mg-N l<sup>−1</sup>);
-   NO<sub>2</sub>: nitrite (mg-N l<sup>−1</sup>);
-   TON: total oxidised nitrogen (mg-N l<sup>−1</sup>);
-   PO<sub>4</sub>: phosphate (mg-P l<sup>−1</sup>);
-   Zn: zinc (μg l<sup>−1</sup>).

It should also contain the continuous derivatives (e.g. average elevation) and categorical derivatives (e.g. land cover percentage) for each watershed.

**Note**: some of the calculated percentages may not add up to 100%. During the reclassification of the categorical datasets (`land_cover`, `soils`, `bedrock`), we focused on the macro-classes which make up the majority of the catchment and are known to have the greatest impact of river hydrochemistry (e.g. urban areas, farmland). While other land cover categories are found within each watershed (hence $total$ $percent < 100$), these typically account for only a small percentage of the total area and have a limited effect on the river environment. These categories have been excluded to simplify the analysis. 

### An introduction to linear models in R

It is now time to examine the relationships between river water quality and catchment metrics. The key model outputs that are ultimately required for the assessment are:

1.  Regression equations for each water quality variable (dependent variable; n = 10) and the key explanatory catchment characteristics (independent variables; n = 16).

2.  Associated model values (R<sup>2</sup>, *p* value).

> Remember, you don't have to run every code block shown below, but you can do so if it would help your understanding.

The simplest way to run a linear regression in R is to use the `lm()` function, an example of which is shown below, storing the output in `model` (you can change this name to reflect the input variables):

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Fits a linear model
model <- lm(formula = Zn ~ average_elevation, data = watersheds_df)
```

We have defined the data frame being used (`data = watersheds_df`) and the input variables from that data frame. This is achieved by including their column names, shown here:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
colnames(watersheds_df)
```

Input variables in the **formula** are separated by `~`, where the variable to the left is the dependent variable (`Zn`) and the variable to the right is an independent variable (`average_elevation`). We can, however, include **multiple** independent variables to perform multiple linear regression. This is achieved as follows, where additional independent variables are separated by `+`:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Fits a linear model
model <- lm(formula = Zn ~ average_elevation + Grassland_percent, data = watersheds_df)
```

We can then assess the model output using the `summary` function:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
summary(model)
```

For this set of independent variables, we have an R<sup>2</sup> of 0.24 (`Multiple R-squared: 0.2415`) and a model *p* value of < 0.01 (`p-value: 9.512e-05`).

The model coefficients for the independent variables are described above, where `*` denotes *p* values < 0.05 (95% probability) and `**` denotes *p* values < 0.01 (99% probability). As the coefficients are very small, they are presented in scientific notation. These can be converted to numeric (non-scientific) format using the following code:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
format(-2.096e-04, scientific = FALSE)
```

We can supply multiple values to the `format` function by creating a vector:

```{r, echo = TRUE, eval = FALSE, warning= FALSE, message = FALSE, fig.align = 'center'}
format(c(-2.096e-04, -8.358e-06, ...) , scientific = FALSE)
```

> When you're happy you understanding the formatting of the `lm` function, move on to the next section.

### Training vs. Testing

One limitation of the above approach is that our dataframe (`watersheds_df`) contains observations from all 70 EA monitoring stations.

When performing statistical analysis, it is common practice to split any dataset into:

-   a **training** subset, which is used to create the model(s).
-   a **testing** subset, which is used to evaluate the model(s).

Subsetting our data in this way allows models to be evaluated more rigorously. Many models perform well "in-sample" but poorly "out-of-sample" when evaluated against independent data (i.e. the testing subset). This is commonly referred to as "over-fitting".

Training and testing subsets are usually defined randomly, with an approximate ratio of 70:30 (although this varies). However, and to ensure reproducibility, this step has been completed for you: the `watersheds_df` dataframe contains a `group` variable denoting which monitoring sites belong to the training and testing subsets.

> Run the code above to create `training` and `testing` dataframes:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Extracts training dataset, comprising 50 observations (~70%)
training <- subset(watersheds_df, Group == "Training")

# Extracts training dataset, comprising 20 observations (~30%)
testing <- subset(watersheds_df, Group == "Testing") 

```

> Before you move on to the next section, can you think of any limitations of this approach?

> Hints: How important is the training-testing ratio? How are training-testing subsets created?

### Variable selection strategies

An addition weakness of the above approach is that we have manually defined the independent variables of interest (`average_elevation + Grassland_percent`). For exploratory analysis, however, we may not know which are the most important variables. Perhaps there is a combination of independent variables which produces a better model fit (e.g. R<sup>2</sup> \> 0.24)?

Determining which variables to include/exclude from a model is a very difficult problem, which has resulted in many different variable selection strategies. Common approaches include expert opinion and/or theory, partial least squares (PLS) regression, implemented in `PLS`, Least Absolute Shrinkage and Selection Operator (LASSO), implemented in `glmnet` and `LARS`, as well as elastic net methods and ridge regression, also implemented in `glmnet`. You may want to explore some of these more complex approaches for your dissertation.


Variable selection strategies: https://doi.org/10.1002/bimj.201700067

Elastic net: https://doi.org/10.1111/j.1467-9868.2005.00503.x 

Summary: https://bradleyboehmke.github.io/HOML/regularized-regression.html

**WATCH** ridge regression: https://www.youtube.com/watch?v=Q81RR3yKn30&ab_channel=StatQuestwithJoshStarmer

- Minimises (the sum of the squared residuals) + (lambda [0-Inf] * slope coefficient^2) ... or slope_1^2 + slope_2^2 + slope_3^2
- Lambda can be any value, and this is normally calculated by trying a load of values using cross-validation (e.g., 10-fold), training-testing
- When lambda = 0, this is just standard least squares regression
- As lambda increases, the slope reduces and the model becomes less sensitive to the input (independent variables)
- This introduces bias, but for a drop in variance (poor fit to testing, change in predictions if we fit the model to a different training data set)

**WATCH** LASSO: https://www.youtube.com/watch?v=NGf0voTMlcs&ab_channel=StatQuestwithJoshStarmer

- LASSO the same as regression, except rather than squaring the slope, we take the *absolute* value
- lambda [0-Inf] * |slope coefficient|)
- Again * lambda, determined by cross validation
- Both methods make our model less sensitive to training data
- Big difference: Ridge can reduce slope asymptotically close to 0, whereas LASSO can go all the way to 0
- This is important because in large models with many predictors, even completely unimportant variables (e.g., airspeed of swallow influencing size) will still be > 0 for ridge regression
- In LASSO, they can go to 0 (and those terms go away)
- This will result in a more parsimonious model (-useless variables)
- Ridge regression performs better when most variables in your model are useful (because it will shrink the parameters, but not remove them)

**WATCH** Elastic net: https://www.youtube.com/watch?v=1dKRdX9bfIo&ab_channel=StatQuestwithJoshStarmer

- We can choose ridge (we think they're all important), or we can choose LASSO (some are less so)
- What if there are loads of variables e.g., "large p, small n" case (high-dimensional data with few examples) i.e., number of variables (p) is significantly larger than the number of observations (n)
- Most important for deep learning (e.g., millions of parameters) - some might be useful / useless
- No need to pick, just use elastic net regression, which is a combination of both
= sum of squared residuals + lasso regression penalty + ridge regression penalty
- each penalty get its own lambda
- again, cross validation used on different combinations of lambda1 and lambda2 to find the "best" value
- again, if lambda1-2 are 0 = least squares
- we will get ridge / lasso results, if either of their penalties are 0
- if both > 0, we have a hybrid
- great if we have correlated variables (LASSO tends to pick one, ridge shrinks all correlated variables together)
- elastic groups and shrinks the parameters together... and leaves them in/removes them all at once


For our analysis, we are going to use a relatively simple method known as **Stepwise Regression**, implemented in the `MASS` package. This works by including **all** the relevant independent variables in the analysis and then selecting those with the greatest explanatory power.

However, we don't necessarily want to test *all* model variables. We would probably want to exclude the categorical counts (e.g. `Arable`, `Heath`, ...) as these factors are already represented by the normalised variables (e.g. `Arable_percent`, `Heath_percent`, ...), as well as any IDs or geometry variables (`area`). In general, we are only interested in testing the continuous derivatives (column names starting with `average_`) and the normalised categorical derivatives (column names ending in `_percent`).

Rather than typing out the columns of interest manually, we are going to use the `select` function from the `dplyr` package to do so:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Creates a vector of column names, including only those which contain "average" or "percent"
factors <- colnames(watersheds_df %>% dplyr::select(contains(c("average", "percent"))))

# Prints to console
factors
```

> Run the above code. **Note**, the formatting of `dplyr::select` may be slightly confusing but it is necessary because there is also a `select` function in the `MASS` package. Here, we are telling R to use `select` from `dplyr`.

Using this vector of column names, we are going to create a new data frame (called `variables`) containing only the independent variables of interest. **Crucially**, this is only for the `training` dataset:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
variables <- training[factors]
```

> Run the above code and use `head()` to inspect the results.

**DETAILS**

This randomly assigns each observation to one of 10 "folds".

By using the same foldid across all models, each fold contains the same data in each model.

This controls for randomness — it ensures that any differences you see in performance between alpha = 1, 0.5, or 0 are due to the model, not the random split

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

library(glmnet)
library(vip)

set.seed(2025)

# Independent variables
x <- as.matrix(variables)
y <- training$NO2

# Alphas 0-1
alphas <- seq(0, 1, by = 0.1)
cv_errors <- numeric(length(alphas))

# Fixed folds
foldid <- sample(1:10, size = length(y), replace = TRUE)

for (i in seq_along(alphas)) {
  cv <- cv.glmnet(x, y, alpha = alphas[i], foldid = foldid)
  cv_errors[i] <- min(cv$cvm)
}

plot(alphas, cv_errors, type = "b", xlab = "Alpha", ylab = "CV Error")

#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))

#for (i in seq_along(alphas)) {
  #cv <- cv.glmnet(x, y, alpha = alphas[i])
  #cv_errors[i] <- min(cv$cvm)
#}

# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)

# Ridge regression (alpha = 0)
#ridge <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 0
#)

# Lasso regression (alpha = 1)
#lasso <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = 1
#)

# Elastic (alpha = modelled)
#elastic <- cv.glmnet(
#  x = x,
#  y = y,
#  alpha = best_alpha
#)



# plot results
#par(mfrow = c(1, 3))
#plot(ridge, main = "Ridge penalty\n\n")
#plot(lasso, main = "Lasso penalty\n\n")
#plot(elastic, main = "Elastic penalty\n\n")

#vip(lasso, num_features = 20, geom = "point")

#fit <- glmnet(x, y, alpha = 0.5)
#plot(fit)

#cvfit <- cv.glmnet(x, y, alpha = 0.5)
#plot(cvfit)

#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables

#coef(cvfit, s = "lambda.1se")


#alphas <- seq(0, 1, by = 0.1)
#cv_errors <- numeric(length(alphas))

#for (i in seq_along(alphas)) {
#  cv <- cv.glmnet(x, y, alpha = alphas[i])
#  cv_errors[i] <- min(cv$cvm)
#}

# Find best alpha
#best_alpha <- alphas[which.min(cv_errors)]
#print(best_alpha)

#cvfit <- cv.glmnet(x, y, alpha = best_alpha)
#plot(cvfit)

#cvfit$lambda.min # Best lambda for prediction
#cvfit$lambda.1se # Simpler model with fewer variables

#coef(cvfit, s = "lambda.1se")

```

### Testing for collinearity

Before we use this dataset to investigate the controls on water quality, it is a good idea to test for **collinearity**, which occurs when independent variables in our regression model are highly correlated to each other. This is a common problem in regression analysis. If you are interested in the factors which influence $A$, you might perform a regression involving $B$ and $C$. However, if $B$ and $C$ are highly correlated (as $B$ increases, so too does $C$), how do you know which is important?

To test for this, we can create a **correlation matrix**, using the `cor` function available in the base R `stats` package, and then visualise this using the `corrplot()` function from the `corrplot` package:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Create correlation matrix
cor_matrix <- cor(variables)

# Visualise using corrplot()
corrplot(cor_matrix, method = 'circle', 
         type = 'lower', diag = FALSE)
```

> You can modify the design to show additional information, such as using `method = 'number'` to include the correlation values (1 = perfect positive correlation, 0 = no correlation, -1 = perfect negative correlation). Further information available [here](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)

Overall, we can see evidence of clear collinearity, in particular **between the topographic variables**, with high positive correlations between `average_slope`, `average_elevation` and `average_rainfall`. This reflects the fact that the higher elevation catchments on the periphery of the Mersey Basin tend to be steeper and wetter (i.e., *orographic rainfall*) than their low elevation counterparts. Fundamentally, these different variables are closely related, so including them all in the model means we are double- or triple-counting the effects of topography.

In addition, we can also see evidence of collinearity with **other catchment characteristics**. For example, `average_slope` is negatively correlated with the proportion of arable and urban land, and positively correlated with wetland, heath and peat coverage.

> Can you think why we might see the above pattern?

We can also see evidence of high collinearity between `Coal` and `Sands_and_Muds`, which reflects their spatial distribution across the basin. You could visualise this by plotting the [bedrock raster](https://emm-concepts.com/mersey_four.html#bedrock-geology) you created in Mersey IV. As there are only three bedrock categories, and limestone is rare, high coverage of `Coal` has to be associated with low coverage of `Sands_and_Muds` (and vice versa).

Based on the above reasoning, we can be justified in **removing variables** with evidence of collinearity (`average_slope`, `average_elevation`, `average_rainfall`, `Coal`). We can achieve this using the `select` function from the `dplyr` package, which we used previously, but modified slightly to exclude variables, rather than retain them:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Remove colinear variables
variables <- variables %>% dplyr::select(-contains(c("slope", "rainfall", "elevation", "Coal")))

```

> Run the above to remove collinear variables. You may want to produce a new correlation matrix (`corrplot()`) to investigate the effects.

### Stepwise regression

With our new *filtered* dataset, we can now combine this data frame (`cbind`) with a dependent variable of interest; we will use zinc (Zn) as an example. Our new dataframe will be called `model_df` as it contains all the variables (dependent + independent) required for multiple linear regression. **Note**: by default, `cbind` will (somewhat unhelpfully) rename input column names e.g. `Zn` will become `watersheds_df$Zn`. The code below specifies the new column name as Zn (`Zn =`) for readability:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Column bind the Zn column with the independent variables from the training dataset
model_df <- cbind(Zn = training$Zn, variables)
```

When complete, we can then run a new model, making sure to update the data frame used (`data = model_df`) and updating the formula to `Zn ~ .`. This denotes that *all* other data frame columns will be included as independent variables (a useful time saver!):

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Fits a linear model, including all other columns (~.) as independent variables
zn_model <- lm(formula = Zn ~ ., data = model_df)
```

> When you're happy you understand the `lm` syntax, combine the two dataframes, run the linear model and inspect the output using `summary()`. This should resemble the following:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Fits a linear model
summary(zn_model)
```

Our overall model fit (R<sup>2</sup>) is $0.5143$ which indicates that the independent variables explain ~51% of variability in the dependent variable. Our model is statistically significant, here defined as having a *p* value < 0.05. When values are very small (e.g. *p* < 0.0005), we would typically present these as a discrete value e.g. *p* < 0.05, < 0.01, < 0.001. Generally, we only use models in which we can be 95% confident or higher (i.e. significance level of 0.05 or less).

However, it is important to note that *p* values should be not be considered in isolation and need to be interpreted carefully. For statistical reviews of using and interpreting *p* values, see @goodman_dirty_2008 and @andrade_p_2019. For a broader overview, see the *Nature* commentary by @amrhein_scientists_2019, as well as a summary article by [Vox](https://www.vox.com/latest-news/2019/3/22/18275913/statistical-significance-p-values-explained).

In addition, while the model is significant, it contains **all the independent variables**, some of which probably have no effect on Zinc concentrations. We can filter out unimportant variables using the `step.AIC` function from the `MASS` library:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Stepwise regression model
step.model <- stepAIC(zn_model, # Input linear model
                      direction = "both",
                      trace = FALSE, # Print out intermediate results? 
                      scope = NULL, 
                      k = 2) 
```

Helpfully, this takes the output of the `lm` model (`zn_model`) with no need for any additional data wrangling. The following are important parameters:

-   `direction = "both"`:
    -   Determines the method used, either **forward** or **backward** stepwise regression, or a mixture of **both**.
    -   "Forward" begins with a model with **no** variables and then starts adding the most significant variables, stopping when there are no more significant variables.
    -   "Backward" begins with a model with **all** variables and then starts removing the least significant variables, stopping when only significant variables are remaining.
    -   "Both" includes both of the above, allowing for variables to be added/removed at each step.
-   `scope`:
    -   Defines the range of models examined in the stepwise search (currently set to `NULL`).
-   `k = 2`:
    -   The number of degrees of freedom used for the penalty i.e. for determining whether variables are significant or not. Although any value can be used here, it is typical to use either $k = 2$, which is equivalent to the Akaike information criterion (AIC), or $k = log(n)$, which is equivalent to the Bayesian information criterion (BIC), where `n` is the number of data points in the model ($n = 50$). Both the AIC and BIC can be used to assess the relative quality of statistical models.

> Run the above model (`direction = "both"` and `k = 2`) and print the output using `summary()`:

```{r, echo = FALSE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Prints summary
summary(step.model)
```

As you can see above, we have now produced a more parsimonious model, but there are still many independent variables ($n = 8$), some of which might be less important. We can address this in a few ways:

1.  Use a more strict criteria for determining significance i.e., shifting from AIC (`k = 2`) to BIC (`k = log(n)`)

2.  Updating the model `scope`, which defines the range of models examined in the stepwise search. Rather than, for example, building from an *empty* model and progressively adding variables (i.e., `direction = "forward"`), we can give stepwise a headstart by performing the stepwise search *from an initial model*.

To achieve this, we are going to use the `ols_step_all_possible()` function from the `olsrr` package, which takes a linear model and fits all possible regressions. For a model with three independent variables ($A, B, C$), this would include **six** models: $A$, $B$ and $C$ alone, $A+B$, $A+C$ and $B+C$. As the number of independent variables ($k$) increases, the number of possible combinations increases by a factor of $2^k$. For our reduced dataset, this is equivalent to $2^{12}$ which is $4096$ possible models!

In this case, we are going to use `ols_step_all_possible()` to perform **univariable** linear regression, which is achieved by setting `max_order = 1`, and utilising the **full model** in the calculation:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Perform univariable regression for all independent variables
k <- ols_step_all_possible(zn_model, max_order = 1)

# Print summary
print(k)
```

> Run the above code to produce univariable regressions. You can modify `max_order` to investigate different combinations of variables, although this will increase the code run time. Remember to set `max_order` back to $1$ for subsequent analysis.

Based on the above approach, we can evaluate the relative performance of the different models using R<sup>2</sup>, adjusted R<sup>2</sup>, and other criteria. The best performing univariable model utilises `Urban_percent`, with an R<sup>2</sup> of $~0.36$. As the best explanatory variable, we are going to use this as the starting point for our new stepwise search.

The code below is very similar to our previous use of stepwise, but this time includes the more strict **BIC** threshold for determining significance (`k = log(n)`), and utilises the `scope` argument to define the start point for the search (`lower = ~Urban_percent`) and the potential end point (`upper = ~zn_model`).

**To summarise**, we are starting with a simple univariable model, in which zinc concentrations are solely related to the urban land cover percentage, and then progressively searching through other combinations of models, adding or removing variables (`direction = "both"`), to find the best outcome. The quality of models is assessed by BIC, which evaluates model fit while penalising unnecessary variables. The upper limit is the model containing **all** the variables (`upper = ~zn_model`), and while this would have the highest model fit (R<sup>2</sup>), it is likely to have unnecessary variables and would be excluded based on BIC.

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}
# Updated stepwise regression model, using BIC and scope
step.model <- stepAIC(zn_model, # Input linear model
                      direction = "both",
                      trace = FALSE, 
                      # Utilising the scope argument
                      scope = list(lower = ~Urban_percent, upper = zn_model), 
                      # Utilising the more strict BIC [k = log(n)], rather than AIC [k = 2]
                      k = log(nrow(training)))

# Print summary
summary(step.model)

```

> Run the above code, making sure you understanding the effects of `k` and `scope`.

### Interpreting results

Our original model, based upon 12 independent variables, had an R<sup>2</sup> of 0.514. Our second model, using a simple stepwise regression (`k = 2`, `scope = NULL`), produced a similar output (R<sup>2</sup> = 0.505), with a reduced but still sizeable number of independent variables. Our final model, employing a more strict threshold for significance (`k = log(n)`) and a starting point for the stepwise search, produced an extremely parsimonious model, with just one independent variable, but at the cost of a reduction in model fit (R<sup>2</sup> = 0.364).

These results raise an important question: **how should model parsimony be weighted against model performance?**

This is a question you will have to answer in your own research, as you select a model from a suite of options and [exclude others](https://www.poetryfoundation.org/poems/44272/the-road-not-taken).

In general, we prefer models with the minimum number of parameters (independent variables), so for our subsequent analysis, we will follow the 'strict' approach outlined above. These models require fewer assumptions, less intensive data collection, can be applied more confidently to new data sets/locations, and are often easier to interpret. This principle of model parsimony is based upon **Occam’s Razor**: "other things being equal, simpler explanations are generally better than more complex ones".

Our model coefficients are now as follows:

-   `intercept` = 11.31283, *p* = 9.52e-06 (*p* \< 0.01)
-   `Urban_percent` = 0.43389, *p* = 3.49e-06 (*p* \< 0.01)

Coefficients are important because they are used in **regression equations**, which can then be used to predict values.

The general format for a regression equation is as follows:

$$
y = a + (b_1 \cdot x_1) + (b_2 \cdot x_2) + (b_n \cdot x_n)
$$ where `a` is the constant (intercept) value, and `b` is the coefficient of x.

For our Zn model above, we can define our regression equation (presented using sensible data precision) as:

$$
Zn = 11.31 + (0.43 \cdot Urban \: percent)
$$ **Well done!** You have now calculated a regression which links the dependent variable (Zn) to an independent variable, in this case the percentage of urban land cover.

> For your assessment, we would like you to **explain** the regression results, linking to hydrological processes and literature. Think about specific sources of pollution, transport pathways, types of flow...

## Task 6: Model evaluation

Having created a statistical model, it is necessary to evaluate its performance. Comparison plots of **measured vs. modelled (or predicted) values** are one common way to assess model quality, alongside other metrics such as root-mean-square error (RMSE), normalised root-mean-square-error (nRMSE), Q-Q plots, or histograms of model residuals. You may want to explore some of these for the assessment.

To calculate modelled values, we can use the `predict()` function, taking the model variable (`step.model`) as the input, rather than re-creating the above equation manually in code, and using our `testing` dataframe for the `newdata` argument:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center', cache = TRUE}

# Predict Zn values based upon stepwise model, saving to testing dataframe
testing$predicted_Zn <- predict(step.model, newdata = testing)

```

If`new_data` is not defined, the `predict` function uses the fitted values for prediction i.e. the training data used to construct the model (see [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/predict.lm)).

> Run the above code block to predict Zn concentrations in the testing dataset, based on the regression model produced from the training dataset.

These values could be used to calculate RMSE or other metrics (nRMSE) using your own code or additional packages (e.g. `Metrics`);

$$
 RMSE = \sqrt{mean(measured\:values - modelled\:values)^2}
$$

Plots of measured vs. modelled values (as well as Q-Q plots and histograms) can be created in ggplot2. Here is an example:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# ggplot of measured vs. modelled (predicted) NO2 values
zn_plot <- ggplot(data = testing, aes(x = Zn, y = predicted_Zn)) +
  # Adding a linear regression ("lm"), removing standard error bars (se = FALSE)
  geom_smooth(method = "lm", se = FALSE, colour="#FF953C") +
  # Adds a 1:1 line for comparison
  geom_abline(intercept = 0, slope = 1, lty = "dashed") +
  # Adds the point data, modifying the shape, size, colour and fill
  geom_point(shape = 21, colour = "white", fill = "#5695FF", size = 2.5) +
  # Setting the theme and aspect ratio
  theme_classic() +
  theme(aspect.ratio = 1) +
  # Axis limits
  scale_x_continuous(limits = c(0,60)) +
  scale_y_continuous(limits = c(0,60)) +
  # Add axis labels and a title
  labs(x = Measured~Zn, y = Modelled~Zn, 
       title = Plot~of~measured~vs.~modelled~Zn~values)

zn_plot

```

> Does the regression line match the 1:1 line? Is there any evidence of under- or over-prediction? Are there any outliers? What **types** of errors can you identify?

You could also assess this relationship statistically, using linear regression:

```{r, echo = TRUE, eval = TRUE, warning= FALSE, message = FALSE, fig.align = 'center'}

# Linear regression of measured vs. modelled Zn values
prediction_model <- lm(formula = Zn ~ predicted_Zn, data = testing)

# Print summary statistics
summary(prediction_model)

```

> How well does our Zn model perform on the testing dataset, based on the above graphs/statistics? Is out-of-sample performance comparable to in-sample performance?

<br/>

**To finish the practical** and to prepare for the assessment:

> Replicating the above approaches, calculate regression equations based on stepwise linear regression for all 10 water quality indicators (NO<sub>2</sub>, pH, SSC, Ca, Mg, NH<sub>4</sub>, NO<sub>3</sub>, TON, PO<sub>4</sub>, Zn).

> Save the relevant model coefficients and the R<sup>2</sup> and *p* values for each equation. These should be stored in a single table for the assessment.
